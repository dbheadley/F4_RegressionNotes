[
  {
    "objectID": "Regression2.html",
    "href": "Regression2.html",
    "title": "Regression 2",
    "section": "",
    "text": "For this lecture we will graft additional features onto our linear model to allow it to handle a wider array of situations. To start, we will consider the case where the dependent variable is binary, requiring the use of logistic regression. We will then show how logistic regression is just a special case of a more general class of models called generalized linear models. With these we can model a variety of different types of dependent variables, including count data and ordinal data. Lastly, we will explore how to evaluate the performance of these models and control for overfitting by splitting our data into training and testing sets and incorporating regularization.\nimport os\nimport json\nimport patsy # for creating design matrices\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "Regression2.html#logistic-regression-modeling-probability-of-binary-outcomes",
    "href": "Regression2.html#logistic-regression-modeling-probability-of-binary-outcomes",
    "title": "Regression 2",
    "section": "Logistic regression: modeling probability of binary outcomes",
    "text": "Logistic regression: modeling probability of binary outcomes\nA binary classifier takes a set of measurements, \\boldsymbol{X}, as inputs and returns the probability that they were generated by a specific event, \\hat{\\boldsymbol{y}}. To get from \\boldsymbol{x} to \\hat{\\boldsymbol{y}}, we need a function that describes the probability of the class occurring over a range of measurement values.\n\nA quick probability primer\nProbabiltilies describe how likely events are to occur. They range from 0, for events that never happen, to 1, for events that are guaranteed to happen. When quantifying probabilities we do this for a class of events, with the total probability across all events adding up to 1 (which means that at any time one of them has to occur). For instance, in the case of flipping a coin, there is a 0.5 (1/2 or 50%) chance that the coin will come up Heads, and 0.5 that it will be Tails. These are the only possibilities (this is a Platonic coin, so it has no thickness and thus cannot land on its side). A coin flip is a good example of an unconditional probability, which is the same regardless of the circumstances. For this, we would write:\n\\begin{align}\np(H)&=0.5 \\\\\np(T)&=0.5 \\\\\n\\end{align}\n\nwhich says that the probability of the coin coming up heads, p(H), is 0.5, and the probability of coming up tails, p(T), is 0.5.\nBut probabilities can also depend on the situation, such as the probability that you will buy lunch at a certain time. It is more likely that you will purchase lunch at 11:30 AM than at 10:00 AM. This is a conditional probability. Conditional probabilities are expressed as P(Lunch|Time), which translates as the probability of going for Lunch, (Lunch), is conditional, |, on the time, Time. For a conditional probability we need to know the time to give the probability that we are going to lunch.\n\n\nThe logistic function\nOne equation that is a useful way to express a conditional probability is the logistic function (also known as the sigmoid function). It has the form:\n \\sigma(x) = \\frac{1}{1+e^{-x}} \nLet’s code it up and visualize it:\n\n# create a logistic function\ndef logistic(x):\n    return 1 / (1 + np.exp(-x)) \n\n# plot the logistic function\nx = np.linspace(-10, 10, 100)\nplt.plot(x, logistic(x))\nplt.title('Logistic Function')\nplt.yticks(np.arange(0, 1.1, 0.25))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nThis function has a couple of features that make it useful for modeling probabilities. First, it is bounded between 0 and 1, meaning that it will not go beyond the range of values taken by a measure of probability. Second, it’s value can be directly interpreted as a probability. For instance, if \\sigma(x)=0.5, then the probability of the event occurring is 0.5. If it returns a value greater than 0.5, we can say that the event is more likely to occur than not. If it returns a value less than 0.5, we can say that the event is less likely to occur than not. When we use the logistic function to predict whether an event will occur, 0.5 will be our cutoff. This is often referred to as the decision boundary.\nWe can insert our linear model into the logistic function to get the probability of an event occurring. Recall that: \n\\begin{align}\n\\boldsymbol{y}_i &= \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1\\boldsymbol{x}_{1,i} + \\boldsymbol{\\beta}_2\\boldsymbol{x}_{2,i} + \\boldsymbol{\\beta}_3\\boldsymbol{x}_{3,i} + ... + \\boldsymbol{\\beta}_p\\boldsymbol{x}_{p,i} \\notag \\\\\n\\boldsymbol{y}_i &= \\boldsymbol{\\beta}_0 + \\sum_{j=1}^{p}\\boldsymbol{\\beta}_j\\boldsymbol{x}_{j,i} \\notag \\\\\n\\boldsymbol{y} &= \\boldsymbol{X}\\boldsymbol{\\beta} \\notag \\\\\n\\end{align}\n\nWe can pass this into the logistic function to get the probability the event occurring. This is the logistic regression model.\n \\begin{align}\n\\hat{y}_i &= \\frac{1}{1+e^{-(\\boldsymbol{\\beta}_0 + \\sum_{j=1}^{p}\\boldsymbol{\\beta}_j\\boldsymbol{x}_{j,i})}} \\notag \\\\\n\\hat{\\boldsymbol{y}} &= \\frac{1}{1+e^{-\\boldsymbol{\\beta} \\boldsymbol{x}}} \\notag \\\\\n\\end{align}\n\nwhere \\boldsymbol{\\beta} is a vector of \\beta coefficients and \\boldsymbol{x} is a vector of dependent variables.\nGiven that we have modified the input to the logistic function to now be a linear equation, we can try to understand how the decision boundary is affected by the \\boldsymbol{\\beta} vector. To start, we will consider the simple case where we just have an intercept, \\boldsymbol{\\beta}_0, and one independent variable, \\boldsymbol{\\beta}_1. Since the decision boundary happens at \\sigma(x)=0.5, we can solve for x when \\sigma(x)=0.5.\n \\begin{align}\n        \\notag 0.5&=\\frac{1}{1+e^{-x}} \\\\\n        \\notag 0.5&=\\frac{1}{1+e^{-0}} \\\\\n        \\notag 0.5&=\\frac{1}{1+1} \\\\\n        \\notag 0.5&=\\frac{1}{2}\n    \\end{align}\n\nSince the decision boundary will occur when \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1x=0, we can use some simple algebra to solve for the corresponding value of x.  \\begin{align}\n        \\notag 0&=-(\\boldsymbol{\\beta}_0+\\boldsymbol{\\beta}_1x) \\\\\n        \\notag 0&=-\\boldsymbol{\\beta}_0-\\boldsymbol{\\beta}_1x \\\\\n        \\notag \\boldsymbol{\\beta}_0&=-\\boldsymbol{\\beta}_1x \\\\\n        \\notag -\\frac{\\boldsymbol{\\beta}_0}{\\boldsymbol{\\beta}_1}&=x\n    \\end{align}\n\nThis means that the decision boundary depends on both the intercept and the slope. As you increase the intercept, you move the decision boundary to lower values. Increasing the slope, however, will shift the boundary towards 0. Let’s visualize these relationships.\n\ndef logistic_linear(x, beta):\n    return logistic(np.dot(x, beta))\n\nx = np.hstack([np.ones((100,1)), np.linspace(-10, 10, 100).reshape(-1,1)])\nvar_ints = [-5,-2.5,0,2.5,5]\nvar_slopes = [0.125,0.25,0.5,1,2]\n\nfig, axs = plt.subplots(2,1)\nfor ind, var_int in enumerate(var_ints):\n    y = logistic_linear(x, [var_int, 1])\n    axs[0].plot(x[:,1], y, label=f'intercept={var_int}', color=[0,0,ind/len(var_ints)])\n    axs[0].vlines(-var_int, 0, 1, color=[0,0,ind/len(var_ints)], linestyle=':')\naxs[0].legend()\naxs[0].grid()\naxs[0].set_title('Varying Intercept')\naxs[0].set_ylabel('$\\sigma(x)$')\n\nfor ind, var_slope in enumerate(var_slopes):\n    y = logistic_linear(x, [0, var_slope])\n    axs[1].plot(x[:,1], y, label=f'slope={var_slope}', color=[ind/len(var_slopes),0,0])\naxs[1].legend()\naxs[1].grid()\naxs[1].set_title('Varying Slope')\naxs[1].set_xlabel('x')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nOne thing in common for the logistic curves plotted above is that as we increase x the probability also increases. To make the probability decrease as x increases, we set our slope to be negative. If we do this without also changing the sign of our intercept, then the decision boundary will change too (see the equation above). In addition, as we increase the magnitude of the slope, the decision boundary will move closer to 0.\n\nfig, axs = plt.subplots(2,1)\nfor ind, var_int in enumerate(var_ints):\n    y = logistic_linear(x, [var_int, -1])\n    axs[0].plot(x[:,1], y, label=f'intercept={var_int}', color=[0,0,ind/len(var_ints)])\n    axs[0].vlines(-var_int, 0, 1, color=[0,0,ind/len(var_ints)], linestyle=':')\naxs[0].legend()\naxs[0].grid()\naxs[0].set_title('Varying Intercept with negative slope')\naxs[0].set_ylabel('$\\sigma(x)$')\n\nfor ind, var_int in enumerate(var_ints):\n    y = logistic_linear(x, [var_int, 2.5])\n    axs[1].plot(x[:,1], y, label=f'intercept={var_int}', color=[ind/len(var_ints),0,0])\n    axs[1].vlines(-var_int/2.5, 0, 1, color=[ind/len(var_ints),0,0], linestyle=':')\naxs[1].legend()\naxs[1].grid()\naxs[1].set_title('Varying Intercept with larger slope')\naxs[1].set_ylabel('$\\sigma(x)$')\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\nIn general, we can say that the direction of increasing probability is determined by the slope, \\boldsymbol{\\beta}_1, and the location of the decision boundary is determined by the intercept (\\boldsymbol{\\beta}_0) scaled by the slope. In general, this holds true for multiple dimensions as well. The decision boundary is a hyperplane that is perpendicular to the weight vector, \\boldsymbol{\\beta_{1:p}}. The offset of the decision boundary from the origin is determined by the intercept and the slope of the weight vector. This is fairly easy to see in two dimensions, when you have two independent variables.\n\nx_1, x_2 = np.meshgrid(np.linspace(-10, 10, 100), np.linspace(-10, 10, 100))\nx_1 = x_1.flatten()\nx_2 = x_2.flatten()\n\nbetas = np.array([[0, 1, 0], [0, 1, 1], [-4, 1, 1]])\nfig, axs = plt.subplots(1,3, figsize=(8,4))\ny = logistic_linear(np.hstack([np.ones((10000,1)), np.vstack([x_1, x_2]).T]), betas[0])\naxs[0].imshow(y.reshape(100,100), origin='lower', extent=(-10, 10, -10, 10), cmap='Blues')\naxs[0].plot([0, 0],[10, -10], color='r', linestyle='--')\naxs[0].set_title('Beta = {}'.format(betas[0]))\naxs[0].set_ylabel('$x_2$')\naxs[0].grid()\n\ny = logistic_linear(np.hstack([np.ones((10000,1)), np.vstack([x_1, x_2]).T]), betas[1])\naxs[1].imshow(y.reshape(100,100), origin='lower', extent=(-10, 10, -10, 10), cmap='Blues')\naxs[1].plot([-10, 10],[10, -10], color='r', linestyle='--')\naxs[1].set_title('Beta = {}'.format(betas[1]))\naxs[1].set_xlabel('$x_1$')\naxs[1].grid()\n\ny = logistic_linear(np.hstack([np.ones((10000,1)), np.vstack([x_1, x_2]).T]), betas[2])\naxs[2].imshow(y.reshape(100,100), origin='lower', extent=(-10, 10, -10, 10), cmap='Blues')\naxs[2].plot([-6, 10],[10, -6], color='r', linestyle='--')\naxs[2].set_title('Beta = {}'.format(betas[2]))\naxs[2].grid()\nfig.tight_layout()\n\n\n\n\n\n\n\n\nHere the decision boundary is denoted by the red dashed line, the probability is the color scale. White is 0 and dark blue is 1. If we set the intercept to 0 and \\boldsymbol{\\beta}_1 to 1, then we have the 2D version of the default logistic function. The decision boundary is a vertical line at x_1=0. When both \\boldsymbol{\\beta}_1 and \\boldsymbol{\\beta}_2 are 1, the decision boundary is a diagonal line. The direction of the decision boundary is determined by the weights. If we then set the intercept, \\boldsymbol{\\beta}_0, to a non-zero value (here -4), we shift that decision boundary away from the origin.\n\n\nLogistic regression for binary classification\nLet’s now try to fit a logistic function to some real data. For this example I have compiled a small dataset where the strength of an evoked response potential (ERP) was measured. On some trials, an auditory stimulus was given, driving a strong ERP, while on others no auditory stimulus was presented, resulting in no ERP. The goal is to predict whether an auditory stimulus was given based on the strength of the ERP. We will use logistic regression to do this. First we will load in the data.\n\n# load the data\ndata_dir = './data/'\nlogreg_data = json.load(open(os.path.join(data_dir, 'logregdata.json'), 'r'))\n\n# convert the data to numpy arrays\nX = np.array(logreg_data['X'])\ny = np.array(logreg_data['y'])\n\n# display to make sure it looks right\nprint('X = {}'.format(X))\nprint('y = {}'.format(y))\n\nX = [ 47.8682191   79.21943476  59.77661572  67.42808345 106.59886022\n  45.76555801 103.45205398  56.59460022  60.77195205 111.57862896\n  32.41245229 118.75920507  41.79053316  81.33619878  54.6388707\n  45.60598449  51.5419817   39.32098506  45.9932129   52.26591628\n  27.40391158  26.07979312  46.61066192  25.94308383  64.4758095\n  -1.39409509 -17.08503545  49.70176208 -11.90910869  34.66836456\n   8.93607429  17.14723219 -29.02676859   3.89797864 -17.66074236\n   3.3038969   26.80126476  31.49036756  10.76049118   9.53549015\n   4.30966256  13.1381603  -25.56534353   2.20072814  41.21241201\n  -7.3085461   26.00222903  32.07631057  -2.19790223 -19.46658645\n   1.24571761 -20.11255289   0.51001914 -53.37683445]\ny = [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]\n\n\nWe have two numpy arrays values. X is the ERP strength measured on each trial, while y is a binary variable indicating whether an auditory stimulus was given. To fit a logistic regression model to this data, we will use the Logit class from the statsmodels package. This function behaves similar to the OLS function used in the last lecture. We will use the fit method to fit the model to the data. The summary method will give us a summary of the model fit.\nIt is a good idea to visualize the data before fitting a model to it. This should give you a sense of how well the model will fit the data and whether there are outliers.\n\nfig, ax = plt.subplots()\nax.scatter(X, y, c=y, cmap='bwr')\nax.set_xlabel('ERP strength')\nax.set_ylabel('Auditory stimulus present')\nax.set_title('ERP strength vs. auditory stimulus present')\nax.grid()\n\n\n\n\n\n\n\n\nThis graph separates out the data points corresponding to trials when a auditory stimulus was present (in red) or absent (in blue). Here we can see that on average the ERP strength values are higher when a auditory stimulus is present. Trials where the auditory stimulus were given have a y value of 1, while those without the stimulus are 0.\nNow we will use the Logit class to fit a logistic regression model to the data.\n\n# create a pandas dataframe of the data\ndf = pd.DataFrame({'ERP': X.flatten(), 'stim': y.flatten()})\n\n# create a logistic regression model\nmdl= sm.Logit(df['stim'], sm.add_constant(df['ERP']))\n\n# fit the model\nres = mdl.fit()\n\n# print the summary statistics\nprint(res.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.241408\n         Iterations 8\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                   stim   No. Observations:                   54\nModel:                          Logit   Df Residuals:                       52\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 13 Jan 2025   Pseudo R-squ.:                  0.6503\nTime:                        13:12:20   Log-Likelihood:                -13.036\nconverged:                       True   LL-Null:                       -37.282\nCovariance Type:            nonrobust   LLR p-value:                 3.318e-12\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -4.3803      1.432     -3.060      0.002      -7.186      -1.575\nERP            0.1332      0.040      3.365      0.001       0.056       0.211\n==============================================================================\n\n\nThe fitting was successful. It appears that the ERP strength is a significant predictor of whether an auditory stimulus was given. We can see this from the ‘P&gt;|z|’ column in the summary table. There, the p-value for the ERP term is 0.001. We can also find the decision boundary based on the coefficients that the model has learned. ‘Const’ is the value of the intercept, and ‘ERP’ is the slope. The decision boundary is given by -\\frac{Const}{ERP}=\\frac{-4.38}{0.13}=33.69.\nThe logistic model \\boldsymbol{\\beta} values can be pulled out of the results object. These are found in the params attribute. The first element in the params array is the intercept (\\boldsymbol{\\beta}_0), while each successive element is the beta for the corresponding factor in the model. Passing these to our logistic function, we can generate the curve that the model has learned, and precisely calculate the decision boundary.\n\nfig, ax = plt.subplots()\nax.scatter(X, y, c=y, cmap='bwr')\nax.plot(np.sort(X), logistic(res.params[0] + res.params[1] * np.sort(X)), color='k')\nax.vlines(-res.params[0]/res.params[1], 0, 1, color='k', linestyle=':')\nax.set_xlabel('ERP strength')\nax.set_ylabel('Auditory stimulus present')\nax.set_title('ERP strength vs. auditory stimulus presence')\nax.grid()\n\n/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_11769/1259642582.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  ax.plot(np.sort(X), logistic(res.params[0] + res.params[1] * np.sort(X)), color='k')\n/var/folders/xr/cvz2q4cs7mn5n78kvjyvkb_00000gn/T/ipykernel_11769/1259642582.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  ax.vlines(-res.params[0]/res.params[1], 0, 1, color='k', linestyle=':')\n\n\n\n\n\n\n\n\n\nYou might be wondering why go through all this trouble of creating an entirely new model. Perhaps we could just fit our linear model to this dataset? Let’s try this and see how that turns out.\n\n# create linear regression model\nlin_mdl = sm.OLS(df['stim'], sm.add_constant(df['ERP']))\n\n# fit the model\nlin_res = lin_mdl.fit()\n\n# print the summary statistics\nprint(lin_res.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   stim   R-squared:                       0.566\nModel:                            OLS   Adj. R-squared:                  0.557\nMethod:                 Least Squares   F-statistic:                     67.77\nDate:                Mon, 13 Jan 2025   Prob (F-statistic):           5.48e-11\nTime:                        13:12:20   Log-Likelihood:                -16.518\nNo. Observations:                  54   AIC:                             37.04\nDf Residuals:                      52   BIC:                             41.01\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.1620      0.058      2.772      0.008       0.045       0.279\nERP            0.0101      0.001      8.232      0.000       0.008       0.013\n==============================================================================\nOmnibus:                        7.087   Durbin-Watson:                   1.244\nProb(Omnibus):                  0.029   Jarque-Bera (JB):                2.452\nSkew:                           0.010   Prob(JB):                        0.293\nKurtosis:                       1.956   Cond. No.                         60.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nOk, so at first glance things don’t look too bad. The statistically significant factors are the same as with the logistic regression model. However, let’s plot the residuals to see if there is a pattern in the errors.\n\ny_pred = lin_res.predict(sm.add_constant(df['ERP']))\ny_resid = df['stim'] - y_pred\n\nfig, ax = plt.subplots()\nax.scatter(X, y_resid)\nax.axhline(0, color='r', linestyle='--')\nax.set_xlabel('ERP strength')\nax.set_ylabel('Residual')\nax.set_title('Residuals vs. ERP strength')\nax.grid()\n\n\n\n\n\n\n\n\nEek! This is not good. The residuals are systematically varying with the ERP strength. This arises because the dependent variable can only take on two values, 0 or 1, while the linear model is predicting continuous values ranging from negative to positive infinity. As you keep increasing the ERP strength, you will eventually get values exceeding 1 (or below 0 if you decrease ERP strength), which would not make sense as a value that reflects the probability of an event."
  },
  {
    "objectID": "Regression2.html#generalized-linear-models",
    "href": "Regression2.html#generalized-linear-models",
    "title": "Regression 2",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nThe logistic regression model is a special case of a more general class of models called generalized linear models (GLMs). In the logistic model we took our linear model and passed it through a logistic function to get the probability of an event occurring (an auditory stimulus). This was to ensure that the output was bounded between 0 and 1, just as a probability value should be. GLMs are a generalization of this idea. Instead of just using the logistic function, we can use other functions that capture properties of the dependent variable.\nSo far we have covered two of them. For the linear model, this is just the identity function, f(x)=x. The identity function is used when the dependent variable is continuous and unbounded. For the logistic model, we used, f(x)=\\frac{1}{1+e^{-x}}.\nIn general, a GLM has three components:\n1. A linear model, \\boldsymbol{y}=\\boldsymbol{X}\\boldsymbol{\\beta}.\n2. A link function, g(\\mu), that transforms the conditional mean, \\mu, of the dependent variable (i.e. its mean value given the dependent variables) to a range suitable for the linear model.\n3. Inverse of the link function, f(\\boldsymbol{y}), that maps the linear model to the dependent variable. This is what we have used so far for the logistic equation. It is helpful for predicting the dependent variable given the independent variables.\nThere are many other mean functions beyond the identity and logistic functions. Below is a table of some and there properties:\n\n\n\n\n\n\n\n\n\n\nDistribution\nLink function\nInverse of the link function\nProperties\nNeuroscientific applications\n\n\n\n\nNormal\ng(\\mu)=\\mu\nf(y) = y\nContinuous, Unbounded\nRelating BOLD signal to task variables\n\n\nPoisson\ng(\\mu)=\\log(\\mu)\nf(y) = e^y\nCount data, positive\nSpike counts\n\n\nBinomial\ng(\\mu)=\\log(\\frac{\\mu}{1-\\mu})\nf(y) = \\frac{1}{1+e^{-y}}\nBinary data\nGo/No-go decisions\n\n\nGamma\ng(\\mu)=-\\frac{1}{\\mu}\nf(y) = -\\frac{1}{y}\nContinuous, positive\nReaction times\n\n\n\nIn neuroscience one frequently encounters data that is best described by the Poisson distribution. This distribution is appropriate for count data, which starts at 0 and can go up to positive infinity. To map the output of a linear model onto that distribution (i.e. the inverse link function), one uses the the exponential function. This can be expressed as \\boldsymbol{y}_i=e^{\\boldsymbol{\\beta}\\boldsymbol{X}_i}. The behavior of the exponential function, e^x, is such that it is always positive and increases as x increases.\nLet’s examine how this function behaves.\n\n# inverse link function for poisson GLM\ndef exp_linear(x, beta):\n    return np.exp(np.dot(x, beta))\n\n# Parameters for example exponential functions\nx = np.hstack([np.ones((100,1)), np.linspace(-2, 2, 100).reshape(-1,1)])\nvar_ints = [-1,-0.5,0,0.5,1]\nvar_slopes = [0.1,0.2,0.3,0.4,0.5]\n\n# plot the exponential functions\nfig, axs = plt.subplots(2,1)\nfor ind, var_int in enumerate(var_ints):\n    y = exp_linear(x, [var_int, 1])\n    axs[0].plot(x[:,1], y, label=f'intercept={var_int}', color=[0,0,ind/len(var_ints)])\naxs[0].axhline(1, color='r', linestyle='--')\naxs[0].legend()\naxs[0].grid()\naxs[0].set_title('Varying Intercept')\naxs[0].set_ylabel('$e^x$')\n\nfor ind, var_slope in enumerate(var_slopes):\n    y = exp_linear(x, [0, var_slope])\n    axs[1].plot(x[:,1], y, label=f'slope={var_slope}', color=[ind/len(var_slopes),0,0])\naxs[1].legend()\naxs[1].grid()\naxs[1].set_title('Varying Slope')\naxs[1].set_xlabel('x')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThis is frequently used with spike count data, where the number of spikes is modeled as a Poisson distribution. The Poisson distribution is a discrete distribution that models the number of events that occur in a fixed interval of time or space. It is defined by a single parameter, \\lambda, which is the average number of events in the interval (note, since it is an average that means \\lambda can be a decimal number, it does not have to be an integer like the counts in the Poisson distribution). With a GLM, we can model the expected number of spikes as a function of multiple independent variables. Here \\lambda=e^{\\boldsymbol{X}\\boldsymbol{\\beta}}.\nSimilar to the logistic function, the exponential function will be shifted by the intercept and scaled by the slope. There is no decision boundary in this case, but the slope will determine the rate at which the expected number of spikes increases as the independent variable increases. The intercept will determine the expected number of spikes when the independent variable is 0.\n\nExample with visual coding\nWe can use a Poisson GLM to model the number of spikes a neuron produces in response to visual stimuli. Our dependent variables will be the properties of the visual stimuli, while the independent variable will be the number of spikes.\nSince 1959 it has been known that neurons in the primary visual cortex respond to moving lines in the visual field. It was a chance observation by Hubel and Wiesel, that lead them to identify the space of stimuli that optimally drive neuronal responses in primary visual cortex. They found that when a bar of light moved across a specific area of the visual field, it would elicit single neuron spiking when it occurred at a particular angle. The further it rotated away from that angle, less of a response was elicited.\n\n\n\nHubelWiesel1959\n\n\nWhile bars of a light can be easily generated and their properties modified, modern visual physiologists use drifting gratings, which can be loosely thought of as a series of white bars moving across a black background. To see what they look like, check out this link.\nDrifting gratings are generally characterized by three parameters:\n\nOrientation: This is the direction the bars are moving, and varies from 0-360 degrees. Orientation sensitivity is one of the most well studied features of neural coding for drifting gratings. In general, neurons respond preferentially to two orientations that are opposing, meaning they are 180 degrees apart, with the response fading out when you get more than 30 degrees away from the preferred orientation.\nSpatial frequency: The width of the bars. The thicker the bars are, the lower the spatial frequency. The narrower the bars are, the higher the spatial frequency. Neurons tend to have a preferred spatial frequency.\nTemporal frequency: How fast are the bars moving across the visual field. A higher frequency means the bars are drifting faster. Neurons tend to have a preferred temporal frequency.\nContrast: The degree to which the black and white lines of the grating deviate from gray. Neurons in V1 typically respond with increasing vigor as the contrast is raised from an all gray screen to solid black and white bars.\n\nWe will examine the responses of neurons in mouse primary visual cortex to these stimuli. The Allen Institute has a publicly available dataset of of such data. The data we will work with was collected while mice viewed a wide variety of stimuli, including many drifting gratings. According to their white paper, the drifting gratings all had the same contrast (80%) and spatial frequency (0.04 cycles/degree), but varied in their orientation (0-315 in 45 degree increments), and temporal frequency (1, 2, 4, 8, and 15 Hz).\nI have compiled a small dataset of the spiking responses of neurons to these stimuli from a single mouse.\n\n# load v1 response data\nv1_resp = pd.read_csv('./data/v1_dg_resp.csv')\nv1_resp.head()\n\n\n\n\n\n\n\n\nstimulus_presentation_id\nstart_time\nstop_time\norientation\ntemporal_frequency\n0\n1\n2\n3\n4\n...\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n\n\n\n\n0\n3815\n1636.690362\n1638.692042\n0.0\n1.0\n0\n0\n1\n18\n21\n...\n81\n0\n11\n12\n2\n3\n2\n28\n27\n1\n\n\n1\n3866\n1789.818328\n1791.819968\n0.0\n1.0\n0\n0\n0\n10\n11\n...\n19\n0\n0\n3\n0\n0\n3\n9\n11\n1\n\n\n2\n3884\n1843.863438\n1845.865118\n0.0\n1.0\n0\n2\n1\n11\n13\n...\n51\n0\n14\n8\n1\n1\n9\n42\n19\n1\n\n\n3\n3893\n1870.886012\n1872.887682\n0.0\n1.0\n0\n5\n0\n9\n11\n...\n37\n0\n6\n3\n0\n0\n6\n21\n10\n0\n\n\n4\n3924\n1963.963798\n1965.965448\n0.0\n1.0\n0\n0\n2\n7\n16\n...\n72\n3\n12\n9\n0\n2\n14\n56\n32\n3\n\n\n\n\n5 rows × 67 columns\n\n\n\nThe data has been imported as a pandas dataframe. Each row corresponds to the presentation of a drifting grating stimulus. The fields ‘orientation’ and ‘temporal_frequency’ describe properties of the drifting grating that was presented. Columns that are numbered correspond to different neurons that were recorded from. Each entry in a column contains the number of spikes that neuron fired during the 2 seconds that a stimulus was presented. You can see from the first 5 entries in the table that the same stimulus was presented multiple times. This allows us to get a sense of the variability in the neuron’s response to the same stimulus. If we want to quickly inspect the mean response of each neuron to the different stimuli, we can use the Dataframe’s pivot_table method to calculate the mean response of each neuron to each type of stimulus.\n\nnrn_ids = [str(x) for x in range(62)] # neuron ids, 0-61\nv1_rfs = v1_resp.pivot_table(index=['orientation', 'temporal_frequency'], values=nrn_ids, aggfunc='mean')\n\n\n# plot the receptive field of neuron 11\nexample_rf = v1_rfs['16'].values.reshape(8,5)\nori = np.unique(v1_rfs.index.get_level_values('orientation'))\ntf = np.unique(v1_rfs.index.get_level_values('temporal_frequency'))\n\n# 2D rf plotting function, we will use this a lot in subsequent analyses\ndef rf_plot(rf, title='Receptive Field', ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    imh = ax.imshow(rf.T, origin='lower', aspect='auto')\n    ax.set_xlabel('Orientation (degrees)')\n    ax.set_ylabel('Temporal frequency (Hz)')\n    ax.set_xticks(np.arange(0,8))\n    ax.set_xticklabels(ori)\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(90)\n    ax.set_yticks(np.arange(0,5))\n    ax.set_yticklabels(tf)\n    ax.set_title(title)\n    plt.colorbar(imh, ax=ax)\n\nfig, axs = plt.subplots(1,3, figsize=(8,4))\naxs[0].plot(ori, np.mean(example_rf, axis=1))\naxs[0].set_title('Orientation tuning')\naxs[0].set_xlabel('Orientation (degrees)')\naxs[0].set_ylabel('Spike count (2 sec window)')\naxs[0].grid()\naxs[1].plot(tf, np.mean(example_rf,axis=0))\naxs[1].set_title('Temporal frequency tuning')\naxs[1].set_xlabel('Temporal frequency (Hz)')\naxs[1].grid()\nrf_plot(example_rf, ax=axs[2])\nfig.tight_layout()\n\n\n\n\n\n\n\n\nWe have plotted the response of the neuron in three different ways. The first captures the orientation tuning by plotting the mean response of the neuron to each orientation irrespective of the temporal frequency. The second plot does the same but for temporal frequency. Lastly, the third plot illustrates the response as a two dimensional image with color being the spike count, orientation along the x-axis, and temporal frequency along the y-axis. For this example neuron we can see that it responds vigorously to stimuli with an orientation of 90 degrees, and that response increases with the temporal frequency of the drifting grating.\nThe neurons in this dataset exhibit a wide variety of responses.\n\n# plot a grid of the receptive fields from all neurons\nfig, axs = plt.subplots(8,8)\naxs = axs.flatten()\nfor ind,id in enumerate(nrn_ids):\n    curr_rf = v1_rfs[id].values.reshape(8,5).T\n    axs[ind].imshow(curr_rf, origin='lower')\n    axs[ind].set_xticks([])\n    axs[ind].set_yticks([])\naxs[62].axis('off')\naxs[63].axis('off')\nfig.suptitle('Receptive fields of V1 neurons')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nImagine we want to model the response of a single neuron to these stimuli. We can use a Poisson GLM to do this. We will use the GLM class from the statsmodels package to fit the model. Since the response of a neuron depends on both the orientation and temporal frequency of the drifting grating, along with their interaction, we will have to include all these terms in the model.\nThere is one extra complication here, though. Unlike the linear model we fit using the OLS class, we cannot just pass the dataframe and formula to the GLM function. Instead, we have to use another package to construct the design matrix X and the response vector y. We will use the patsy package to do this. The dmatrices function from the patsy package takes a formula and dataframe, then returns the design matrix and response vector. We can then pass these to the GLM function to fit the model.\n\n\n\n\n\n\nFormula specification\n\n\n\npatsy allows you to specify formulae that can be translated into design matrices. There are many operators and features supported by them, but we will focus on just a few. For a detailed treatment of this, see here. Here y is the dependent variable and x1 and x2 are independent variables.\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n~\nSeparates the dependent variable from the independent variables.\ny ~ x1 + x2\n\n\n+\nAdds a term to the model.\ny ~ x1 + x2\n\n\n*\nIncludes the interaction between variables and their main effects.\ny ~ x1 * x2\n\n\n:\nIncludes only the interaction between variables, without main effects.\ny ~ x1 : x2\n\n\n**\nIncludes polynomial terms up to the specified degree.\ny ~ x1 + x2**2\n\n\nC()\nIndicates that a variable should be treated as a categorical variable.\ny ~ C(x1)\n\n\n\n\n\nAnother formatting detail is that the values in the ‘orientation’ and ‘temporal_frequency’ columns are stored as numeric data types. This means that if we construct a model it will treat these as continuous variables. This would result in the model trying to fit a single slope to the firing rate as a function of either orientation or temporal frequency. However, looking at our neurons, we can see that they respond to orientation and temporal frequency in a non-linear manner. Instead, we should treat these as categorical variables, with a separate \\beta coefficients for each orientation and temporal frequency. We also want our dependent variable to have a meaningful name, instead of just the unit ID number. We will wrap together the renaming and design matrix creation into a single function.\n\n# create a simple function for defining our design matrices\ndef format_tables(df, unit_id, fm='spike_count ~ 1'): # default formula is just the spike count with an intercept\n    df = df.copy()\n    y, X = patsy.dmatrices(fm, df.rename(columns={unit_id: 'spike_count'}))\n    return y, X\n\nTo start using GLMs, let’s create a simple model that just tries to model the response of the neuron as a function of the orientation and temporal frequencies of the drifting gratings, but doesn’t consider their interaction. For those of you that have taken a statistics class, this is just a main effects model.\n\n# calculate the design matrix\ny, X = format_tables(v1_resp, '16', fm='spike_count ~ C(orientation) + C(temporal_frequency)')\n\n# initialize poisson GLM model\nexp_mdl = sm.GLM(y, X, family=sm.families.Poisson())\n\n# fit the model\nres = exp_mdl.fit()\n\n# print the summary statistics\nprint(res.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            spike_count   No. Observations:                  600\nModel:                            GLM   Df Residuals:                      588\nModel Family:                 Poisson   Df Model:                           11\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -1648.5\nDate:                Mon, 13 Jan 2025   Deviance:                       2126.0\nTime:                        13:12:21   Pearson chi2:                 2.56e+03\nNo. Iterations:                     7   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n=================================================================================================\n                                    coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------\nIntercept                        -0.8758      0.158     -5.549      0.000      -1.185      -0.566\nC(orientation)[T.45.0]            0.9719      0.181      5.365      0.000       0.617       1.327\nC(orientation)[T.90.0]            4.2313      0.155     27.225      0.000       3.927       4.536\nC(orientation)[T.135.0]           0.6690      0.190      3.526      0.000       0.297       1.041\nC(orientation)[T.180.0]           0.0465      0.216      0.216      0.829      -0.376       0.469\nC(orientation)[T.225.0]           0.6318      0.191      3.308      0.001       0.257       1.006\nC(orientation)[T.270.0]           3.8008      0.156     24.361      0.000       3.495       4.107\nC(orientation)[T.315.0]           0.8267      0.185      4.468      0.000       0.464       1.189\nC(temporal_frequency)[T.2.0]      0.2260      0.048      4.698      0.000       0.132       0.320\nC(temporal_frequency)[T.4.0]      0.2731      0.048      5.736      0.000       0.180       0.366\nC(temporal_frequency)[T.8.0]      0.2886      0.047      6.083      0.000       0.196       0.382\nC(temporal_frequency)[T.15.0]     0.5987      0.045     13.408      0.000       0.511       0.686\n=================================================================================================\n\n\nExamining the summary table, we can see that the model detected a significant relationship between the response of the neuron and the orientation and temporal frequency of drifting gratings. In particular, notice that the \\beta coefficients for 90 and 270 degrees are the largest, which matches what we saw in the orientation tuning plot above. Note also that the temporal frequency \\beta coefficients increase, which matches what we saw in the temporal frequency tuning plot. To evaluate the model fit, we can plot the predicted response of the neuron against the actual response.\n\n# get observed RF\nrf_true = v1_rfs['16'].values.reshape(8,5)\n\n# calculate the predicted receptive field\n# predicted response\nresp_pred = res.predict(X) \n\n# reshape to 8x5x15, the dimensions of the RF and repetitions of each condition\nresp_pred = resp_pred.reshape(8,5,15) \n\n# take the first repetition of the predicted RF, all other repetitions are the same\nrf_pred = resp_pred[:,:,0]\n\n# calculate the residual\nrf_resid = rf_true - rf_pred\n\nfig, axs = plt.subplots(1,3, figsize=(8,2.5))\nrf_plot(rf_true, title='True RF', ax=axs[0])\nrf_plot(rf_pred, title='Predicted RF', ax=axs[1])\nrf_plot(rf_resid, title='Residual RF', ax=axs[2])\nfig.tight_layout() \n\n\n\n\n\n\n\n\nOur model seems to be doing a good job of capturing the response of the neuron to the drifting gratings. The predicted response is closely tracking the actual response. However, there are some substantial deviations evident in the residual. In particular, the model is overestimating the response for drifting gratings at 90 degrees and 15 Hz, and usually underestimating the response at lower temporal frequencies. To account for this, we can add interaction terms to our model, to capture the response of the neuron to each combination of orientation and temporal frequency.\n\n# rerun model with interaction term\ny, X = format_tables(v1_resp, '16', fm='spike_count ~ C(orientation) * C(temporal_frequency)')\nexp_mdl = sm.GLM(y, X, family=sm.families.Poisson())\nres = exp_mdl.fit()\nprint(res.summary())\n\n# calculate the predicted receptive field\nrf_pred_int = res.predict(X).reshape(8,5,15)[:,:,0]\n\n# calculate the residual\nrf_resid_int = rf_true - rf_pred_int\n\nfig, axs = plt.subplots(1,3, figsize=(8,2.5))\nrf_plot(rf_true, title='True RF', ax=axs[0])\nrf_plot(rf_pred_int, title='Predicted RF', ax=axs[1])\nrf_plot(rf_resid_int, title='Residual RF', ax=axs[2])\nfig.tight_layout()\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            spike_count   No. Observations:                  600\nModel:                            GLM   Df Residuals:                      560\nModel Family:                 Poisson   Df Model:                           39\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -1488.8\nDate:                Mon, 13 Jan 2025   Deviance:                       1806.7\nTime:                        13:12:21   Pearson chi2:                 1.96e+03\nNo. Iterations:                    24   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n=========================================================================================================================\n                                                            coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------------------------------\nIntercept                                                -2.7081      1.000     -2.708      0.007      -4.668      -0.748\nC(orientation)[T.45.0]                                    2.5649      1.038      2.472      0.013       0.531       4.599\nC(orientation)[T.90.0]                                    6.1549      1.001      6.148      0.000       4.193       8.117\nC(orientation)[T.135.0]                                   2.4849      1.041      2.387      0.017       0.445       4.525\nC(orientation)[T.180.0]                                   1.3863      1.118      1.240      0.215      -0.805       3.578\nC(orientation)[T.225.0]                               -1.272e-13      1.414  -8.99e-14      1.000      -2.772       2.772\nC(orientation)[T.270.0]                                   5.5910      1.002      5.581      0.000       3.627       7.555\nC(orientation)[T.315.0]                                   1.9459      1.069      1.820      0.069      -0.149       4.041\nC(temporal_frequency)[T.2.0]                              0.6931      1.225      0.566      0.571      -1.707       3.094\nC(temporal_frequency)[T.4.0]                            -19.8212   1.22e+04     -0.002      0.999    -2.4e+04    2.39e+04\nC(temporal_frequency)[T.8.0]                              1.6094      1.095      1.469      0.142      -0.538       3.756\nC(temporal_frequency)[T.15.0]                             3.5264      1.015      3.476      0.001       1.538       5.515\nC(orientation)[T.45.0]:C(temporal_frequency)[T.2.0]      -1.4663      1.320     -1.110      0.267      -4.054       1.122\nC(orientation)[T.90.0]:C(temporal_frequency)[T.2.0]      -0.5884      1.226     -0.480      0.631      -2.992       1.815\nC(orientation)[T.135.0]:C(temporal_frequency)[T.2.0]     -1.0986      1.307     -0.841      0.401      -3.660       1.463\nC(orientation)[T.180.0]:C(temporal_frequency)[T.2.0]      0.5596      1.350      0.415      0.678      -2.086       3.205\nC(orientation)[T.225.0]:C(temporal_frequency)[T.2.0]      1.2528      1.626      0.771      0.441      -1.934       4.439\nC(orientation)[T.270.0]:C(temporal_frequency)[T.2.0]     -0.2753      1.227     -0.224      0.822      -2.681       2.130\nC(orientation)[T.315.0]:C(temporal_frequency)[T.2.0]     -0.6931      1.336     -0.519      0.604      -3.312       1.926\nC(orientation)[T.45.0]:C(temporal_frequency)[T.4.0]      19.5588   1.22e+04      0.002      0.999   -2.39e+04     2.4e+04\nC(orientation)[T.90.0]:C(temporal_frequency)[T.4.0]      20.0912   1.22e+04      0.002      0.999   -2.39e+04     2.4e+04\nC(orientation)[T.135.0]:C(temporal_frequency)[T.4.0]     19.2822   1.22e+04      0.002      0.999   -2.39e+04     2.4e+04\nC(orientation)[T.180.0]:C(temporal_frequency)[T.4.0]     19.8212   1.22e+04      0.002      0.999   -2.39e+04     2.4e+04\nC(orientation)[T.225.0]:C(temporal_frequency)[T.4.0]     21.2075   1.22e+04      0.002      0.999   -2.39e+04     2.4e+04\nC(orientation)[T.270.0]:C(temporal_frequency)[T.4.0]     20.1410   1.22e+04      0.002      0.999   -2.39e+04     2.4e+04\nC(orientation)[T.315.0]:C(temporal_frequency)[T.4.0]     20.1779   1.22e+04      0.002      0.999   -2.39e+04     2.4e+04\nC(orientation)[T.45.0]:C(temporal_frequency)[T.8.0]      -1.3412      1.156     -1.160      0.246      -3.606       0.924\nC(orientation)[T.90.0]:C(temporal_frequency)[T.8.0]      -1.3443      1.097     -1.225      0.220      -3.495       0.806\nC(orientation)[T.135.0]:C(temporal_frequency)[T.8.0]     -1.1499      1.156     -0.995      0.320      -3.415       1.115\nC(orientation)[T.180.0]:C(temporal_frequency)[T.8.0]     -1.0498      1.262     -0.832      0.406      -3.523       1.424\nC(orientation)[T.225.0]:C(temporal_frequency)[T.8.0]      0.3365      1.531      0.220      0.826      -2.664       3.336\nC(orientation)[T.270.0]:C(temporal_frequency)[T.8.0]     -1.3311      1.098     -1.212      0.226      -3.484       0.822\nC(orientation)[T.315.0]:C(temporal_frequency)[T.8.0]     -0.9163      1.189     -0.770      0.441      -3.247       1.415\nC(orientation)[T.45.0]:C(temporal_frequency)[T.15.0]     -1.9169      1.059     -1.810      0.070      -3.993       0.159\nC(orientation)[T.90.0]:C(temporal_frequency)[T.15.0]     -3.1814      1.016     -3.130      0.002      -5.174      -1.189\nC(orientation)[T.135.0]:C(temporal_frequency)[T.15.0]    -2.4277      1.068     -2.273      0.023      -4.521      -0.335\nC(orientation)[T.180.0]:C(temporal_frequency)[T.15.0]    -2.2046      1.160     -1.900      0.057      -4.479       0.069\nC(orientation)[T.225.0]:C(temporal_frequency)[T.15.0]     0.5680      1.430      0.397      0.691      -2.236       3.372\nC(orientation)[T.270.0]:C(temporal_frequency)[T.15.0]    -2.9415      1.017     -2.891      0.004      -4.936      -0.947\nC(orientation)[T.315.0]:C(temporal_frequency)[T.15.0]    -1.4118      1.091     -1.294      0.195      -3.549       0.726\n=========================================================================================================================\n\n\n\n\n\n\n\n\n\nBy including the interaction term, the model now perfectly captures the response of the neuron to the drifting gratings. In fact, it is too perfect. The residuals are on the order of 10^-10 (which is essentially zero). This is a sign that we have overfit the model. We can test this by training on the model on just a subset of data and comparing it with an RF estimated from another subset it has not seen. This procedure is generally referred to as cross-validation. The data we train or fit the model to will be called the training set, while the data it has not seen where we evaluate its performance will be called the test set. The residual calculated on the test set will give us a sense of how well the model generalizes to new data.\n\n# separate data into stratified training and test sets\ntrain_inds = np.mod(np.arange(len(v1_resp)), 15)&lt;12\nv1_resp_train = v1_resp.iloc[train_inds]\nv1_resp_test = v1_resp.iloc[~train_inds]\n\nWe split the data into two sets. The training set will be 80% of the data, while the test set will be 20%. We use more data for training because the more data the model sees, the better it will be able to learn the underlying relationship between the independent and dependent variables. A 80/20%, also known as a 5-fold cross validation, is a common split ratio.\n\n# train model\ny_train, X_train = format_tables(v1_resp_train, '16', fm='spike_count ~ C(orientation) * C(temporal_frequency)')\nexp_mdl = sm.GLM(y_train, X_train, family=sm.families.Poisson())\nres = exp_mdl.fit()\n\n# test model\ny_test, X_test = format_tables(v1_resp_test, '16', fm='spike_count ~ C(orientation) * C(temporal_frequency)')\nrf_test_pred = res.predict(X_test).reshape(8,5,3)[:,:,0]\n\n# true RF from test set\nv1_rfs_test = v1_resp_test.pivot_table(index=['orientation', 'temporal_frequency'], values=nrn_ids, aggfunc='mean')\nrf_test_true = v1_rfs_test['16'].values.reshape(8,5)\n\n# calculate the residual\nrf_test_resid = rf_test_true - rf_test_pred\n\nfig, axs = plt.subplots(1,3, figsize=(8,2.5))\nrf_plot(rf_test_true, title='True RF', ax=axs[0])\nrf_plot(rf_test_pred, title='Predicted RF', ax=axs[1])\nrf_plot(rf_test_resid, title='Residual RF', ax=axs[2])\nfig.tight_layout()\n\n\n\n\n\n\n\n\nComparing the prediction with the held out test data, we can see that the model performs far worse. For instance, for drifting gratings at 90 degrees and 8 Hz our estimate is off by almost 25 spikes! We could examine the error for each stimulus to get a sense of how poorly the model performs, but it would be more convenient to have a measure that summarizes the error across all stimuli. For GLMs, the most common measure of error is the deviance. The deviance reflects how well the model fits the data by comparing the predicted response to the actual response. The lower the deviance, the better the model fits the data. The deviance is calculated as:\n D = 2\\sum_{i=1}^{n}y_i\\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) - y_i + \\hat{y}_i \nRecall from our linear regression lesson, y_i is the actual response of the neuron, while \\hat{y}_i is the predicted response. The deviance is calculated for each data point, and then summed across all data points. Looking at the equation, you can get some intuition for how it gives a lower value for a better fitting model. If y_i and \\hat{y}_i are the same, then the log term will be 0 and the sum of y_i and \\hat{y}_i will also be zero. Adding them together gives zero.\n\n# function for calculating deviance\ndef deviance(y_true, y_pred):\n    y_pred[y_pred==0] = 1e-10 # to avoid log(0), which is -inf\n    y_true[y_true==0] = 1e-10\n    return 2*np.sum(y_true*np.log(y_true/y_pred) - y_true + y_pred)\n\n# calculate the deviance of the model\ndev_main = deviance(rf_true, rf_pred)\ndev_int = deviance(rf_true, rf_pred_int)\ndev_test = deviance(rf_test_true, rf_test_pred)\n\nprint('Deviance of main model: {}'.format(dev_main))\nprint('Deviance of interaction model: {}'.format(dev_int))\nprint('Deviance of interaction model with train/test: {}'.format(dev_test))\n\nDeviance of main model: 21.281874845218248\nDeviance of interaction model: 2.931960808858664e-11\nDeviance of interaction model with train/test: 104.20409893816736\n\n\nThere is a story in these deviance scores! The deviance of the interaction model with train/test is higher than both the main effects and interaction models trained and tested on all the data. This is because training and testing on the entire dataset allows the model to fit to most of the variability, whether it is due to a systematic relationship or noise. Fitting the interaction model on just the training data and testing it on the test data caused a massive drop in performance (a higher deviance score). This is because the noise it fit to in the training set was not in the test set. Since the interaction terms seem especially noise sensitive, perhaps the model would perform better if we got rid of the interaction terms. To test this, let’s use a main effects model with the train/test data split.\n\n# train model\ny_train, X_train = format_tables(v1_resp_train, '16', fm='spike_count ~ C(orientation) + C(temporal_frequency)')\nexp_mdl = sm.GLM(y_train, X_train, family=sm.families.Poisson())\nres = exp_mdl.fit()\n\n# test model\ny_test, X_test = format_tables(v1_resp_test, '16', fm='spike_count ~ C(orientation) + C(temporal_frequency)')\nrf_test_main_pred = res.predict(X_test).reshape(8,5,3)[:,:,0]\n\ndev_test_main = deviance(rf_test_true, rf_test_main_pred)\n\nprint('Deviance of interaction vs main model on test set: {} vs {}'.format(dev_test, dev_test_main))\n\nDeviance of interaction vs main model on test set: 104.20409893816736 vs 123.80230520651624\n\n\nWhen using the train/test approach, the main effects model performed worse than the interaction model. This suggests that the interaction model is capturing some of the underlying relationships in the data that are missed by only considering orientation and temporal frequency as independently contributing to the response of the neuron. However, we saw that the interaction model has a tendency to overfit, so it may be performing worse than is apparent. Is there a way to get the benefits of including all the interaction terms in the model while controlling for overfitting? Yes, if use regularization as part of the fitting process."
  },
  {
    "objectID": "Regression2.html#regularization",
    "href": "Regression2.html#regularization",
    "title": "Regression 2",
    "section": "Regularization",
    "text": "Regularization\nThe problem has been that we have so many factors accessible to the interaction model that it can now overfit the data. By overfitting, we mean that the model tries to capture all the variability in the training data. A good example of this phenomenon is fitting a curve with as many parameters as there are data points.\nImagine we have two variables, x and y, that are related to each other by the equation y=x+\\epsilon. Here \\epsilon is some noise that makes x and y only partially correlated. We want to predict the value of y given the value of x. There are a host of models that can be used. One is our old friend the line equation, where y=b+wx.\n\n# create two partially correlated variables\nnp.random.seed(47)\nx = np.random.randn(10)\ny = x + np.random.randn(10)/2 # y is partially correlated with x\n\ndef plot_w_fit(x, y, mdl_order=1, ax=None):\n    \"\"\"\n    Function to plot data and fit a linear model.\n\n    Parameters\n    ----------\n    x : array\n        Array of x values.\n    y : array\n        Array of y values.\n    mdl_order : int\n        Order of polynomial to fit to data.\n    ax : matplotlib.axes.Axes\n        Axis to plot on.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    min_x = np.min(x)-0.5\n    max_x = np.max(x)+0.5\n    x_samples = np.linspace(min_x,max_x,100)\n    # plot x vs y\n    ax.plot(x, y, 'k.', label='Train data')\n    y_lim = ax.get_ylim()\n    # plot polynomial fit\n    ax.plot(x_samples, np.poly1d(np.polyfit(x, y, mdl_order))(x_samples), 'r', alpha=0.5, label='Fit curve')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_ylim(y_lim)\n    ax.set_xlim([min_x, max_x])\n    ax.set_title('x vs y with order {} fit'.format(mdl_order))\n\nplot_w_fit(x,y)\nplt.legend()\n\n\n\n\n\n\n\n\nA line captures the trend, larger values of x lead to larger values of y, but almost none of the data points lie along the best fit line. The line is a polynomial with order=1. What happens if we increase the order? To do that, we can add additional terms, y = b + m_1x + m_2x^2 (a quadratic equation), y= b + m_1x + m_2x^2 + m_3x^3 (cubic equation), and so on.\n\n# plot x vs y with fits of different orders\nfig, ax = plt.subplots(1, 3, figsize=(8,2.5))\nplot_w_fit(x, y, 2, ax=ax[0])\nax[0].legend()\nplot_w_fit(x, y, 5, ax=ax[1])\nplot_w_fit(x, y, 9, ax=ax[2])\nfig.tight_layout()\n\n\n\n\n\n\n\n\nAs we increase the order, the fitted curve gets closer to the data points. If our order is equal to the number of data points, then it passes through each point. This is analogous to our interaction model that was trained and tested on the entire dataset. It could perfectly predict the response to each of the stimuli. However, with the curve fitting case here we can see that between points the curve deviates dramatically. If we generate additional data points using the same rule that generated the original data, y=x+\\epsilon, we will find that higher order fits are even worse then the lower order ones.\n\n# generate new data using the same process\nx_new = np.random.randn(10)\ny_new = x_new + np.random.randn(10)/2\n\n# plot x vs y with fits of different orders\nfig, ax = plt.subplots(1, 3, figsize=(8,2.5))\nplot_w_fit(x, y, 2, ax=ax[0])\nax[0].plot(x_new, y_new, 'g+', label='Test data')\nax[0].legend()\nplot_w_fit(x, y, 5, ax=ax[1])\nax[1].plot(x_new, y_new, 'g+')\nplot_w_fit(x, y, 9, ax=ax[2])\nax[2].plot(x_new, y_new, 'g+')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe higher order models better fit the data points they are trained on (the training data set), but poorly on those data points they are not (green crosses, our test data set). In fact, increasing model order seems to worsen the fit to test data. We can see this by examining how the mean squared error on the test data varies for models of different order.\n\nord_list = np.arange(0, 9)\ntrain_err = np.zeros(ord_list.size)\ntest_err = np.zeros(ord_list.size)\n\n# fit models of different orders and get train and test errors\nfor i in ord_list:\n    mdl = np.poly1d(np.polyfit(x, y, i))\n    train_err[i] = np.mean((y - mdl(x))**2)\n    test_err[i] = np.mean((y_new - mdl(x_new))**2)\n\n# plot train and test errors\nplt.plot(ord_list, train_err, color='k', label='Train')\nplt.plot(ord_list, test_err, color='g', label='Test')\nplt.yscale('log')\nplt.xlabel('Model order')\nplt.ylabel('Mean squared error')\nplt.legend()\nplt.title('Train and test errors for models of different orders')\n\nText(0.5, 1.0, 'Train and test errors for models of different orders')\n\n\n\n\n\n\n\n\n\nAs we increase the model order, the error on the training data keeps decreasing. If we get to a model order of 9 (not plotted), the error drops to 0, which reflects the fact that the curve can now pass through each data point. On the other hand, the error on the test data decreases, then increases. It reaches its minimum at a model of order 1, which reflects the fact that the data was generated by a linear model, y=x. As the order increases past 1, the error increases, because the model is now making predictions between the training data points that are deviating from the underlying linear function that generates the data. This is the problem with overfitting, and why it is crucial to use separate training and test data sets to evaluate the performance of decoders.\nWith the polynomial fit, it is straightforward to set the model order to minimize error on the test data set. For our interaction model with 40 \\beta coefficients, and that we do not a priori know which are important, this is not so easy. Regularization is a technique that can help us with this. Regularization works by adding a penalty term to the error function (which we will now call a loss function to fit with the terminology in machine learning) that the model is trying to minimize. This penalty term is a function of the model \\beta coefficients, and it is designed to discourage the model from increasing their aggregate value. There are two common types of regularization, L1 and L2.\n\nL2 and L1 penalties\nRegularization adds a term to the loss function that penalizes it for using non-zero \\beta coefficients. This is referred to as the penalty term. The degree to which we allow this penalty term to influence the loss is set by the \\lambda parameter. There are different functions, called norms, for measuring the magnitude of the \\boldsymbol{\\beta} vector. If we take the sum of the squared value for each \\beta, that is referred to as the L2 norm, or ridge regression. This is written as:\n loss = loss_{fit} + \\lambda\\sum_{i=0}^{n}\\boldsymbol{\\beta}_i^2 \nThe consequence of this is that large \\betas shrink to smaller values while still minimizing the loss_{fit}. The degree to which one emphasizes minimizing the \\boldsymbol{\\beta} is set by the \\lambda parameter. If \\lambda is set to 0, then the equation is the same loss function we have been using normally without regularization. As \\lambda increases, the penalty becomes more stringent, pushing the individual \\beta coefficients towards 0.\nHow does this affect the loss landscape? We can imagine it as being the summation of two surfaces, one for loss_{fit} (which we were calling the error surface or landscape in the last lecture) and another for \\lambda\\sum_{i=0}^{n}\\boldsymbol{\\beta}_i^2. We can contrive a convex loss_{fit}, and then vary the strength of \\lambda, to see how it distorts it.\n\n# create loss surface as quadratic function of two variables\nb0 = np.linspace(-10, 10, 100)\nb1 = np.linspace(-10, 10, 100)\nb00, b11 = np.meshgrid(b0, b1)\nloss_fit = (b00-8)**2 + (b11-2)**2\n\n# create L2 penalty surface\npp = b00**2 + b11**2\n\n# lambda list\nlambdas = [0, 0.5, 1, 10]\n\n# plot loss surface\nfig, ax = plt.subplots(2,2)\nax = ax.flatten()\nfor ind, lam in enumerate(lambdas):\n    curr_loss = loss_fit + lam*pp\n    ax[ind].contourf(b00, b11, curr_loss, levels=10)\n    CS = ax[ind].contour(b00, b11, curr_loss, levels=10, colors='white')\n    ax[ind].plot(8, 2, 'ro', markersize=10)\n    ax[ind].clabel(CS, fontsize=10)\n    min_ind = np.argmin(curr_loss)\n    ax[ind].plot(b00.flatten()[min_ind], b11.flatten()[min_ind], 'wo', markersize=10)\n    ax[ind].set_title('$\\lambda$ = ' + str(lam))\n    ax[ind].grid()\n    ax[ind].set_aspect('equal')\nfig.suptitle('Loss surface for different $\\lambda$ values with L2 penalty')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nOur contrived loss function has a minimum when b0=8 and b1=2. When the \\lambda parameter is zero, the overall loss, loss (white dot), is equal to loss_{fit} (red dot). Increasing \\lambda starts to shift the minimum of loss towards the origin, where b0 and b1 are equal to 0. The \\beta coefficient that is stronger is less affected by this, with b1 approaching 0 sooner than b0. Note that this approach is gradual. Generally, the coefficients will not precisely reach 0. Thus, the L2 norm shrinks the weights.\nAnother norm is to take the sum of the absolute values of the \\beta coefficients. This is known as the L1 norm, or Least Absolute Shrinkage and Selection Operator (LASSO). This is expressed mathematically as:\n loss = loss_{fit} + \\lambda\\sum_{i=0}^{n}|\\boldsymbol{\\beta}_i| \nBecause we are not squaring the coefficients, the shape of the penalty is different. Instead of a smooth quadratic curve centered on zero, it will be an upside down pyramid centered on zero.\n\n# create L2 penalty surface\npp = np.abs(b00) + np.abs(b11)\n\n# lambda list\nlambdas = [0, 1, 10, 100]\n\n# plot loss surface\nfig, ax = plt.subplots(2,2)\nax = ax.flatten()\nfor ind, lam in enumerate(lambdas):\n    curr_loss = loss_fit + lam*pp\n    ax[ind].contourf(b00, b11, curr_loss, levels=10)\n    CS = ax[ind].contour(b00, b11, curr_loss, levels=10, colors='white')\n    ax[ind].plot(8, 2, 'ro', markersize=10)\n    ax[ind].clabel(CS, fontsize=10)\n    min_ind = np.argmin(curr_loss)\n    ax[ind].plot(b00.flatten()[min_ind], b11.flatten()[min_ind], 'wo', markersize=10)\n    ax[ind].set_title('$\\lambda$ = ' + str(lam))\n    ax[ind].grid()\n    ax[ind].set_aspect('equal')\nfig.suptitle('Loss surface for different $\\lambda$ values with L1 penalty')\nfig.tight_layout()\n\n\n\n\n\n\n\n\nThe loss landscape is quite different now. As \\lambda increases, the surface develops edges centered on the major axes of the graph. These drive each coefficient onto the corresponding axis where its value is clamped at zero. It does this first for the smallest coefficient, in this case b1. What this means is that the LASSO is not just shrinking the weights, but also selecting them, by forcing some to 0.\nThese are the two principal ways to penalize weights. Since they act on all the weights in a model, we do not have to preselect the features we want the model to train on. At this point you might be wondering, if we know that b0 and b1 are really contributing to the model, why would we want to shrink them? Aren’t we just making the model worse by regularization? In this case, we know they matter, but the implicit assumption when using regularization with variables of unknown relevance is that most will not contribute and should be at or near zero. Moreover, as covered in the next section, the degree to which we penalize the coefficients will be set in a principled way that allows the data to tell us whether they are important or not. For that, we can fit the model multiple times to different values of \\lambda, and then check how the model performs on the test data set. The \\lambda producing the lowest test error would be the one to use.\n\n\nRegularization with statsmodels\nThe GLM class in the statsmodels package has a fit method that incorporates a regularization term. The fit_regularized method allows you to specify the type of regularization, L1 or L2, and the strength of the regularization, \\lambda. The lambda term is specified by the alpha input argument. Whether you want to use L1 or L2 regularization is specified by the L1_wt input argument. If L1_wt is set to 0, then L2 regularization is used. If it is set to 1, then L1 regularization is used. You can also set it to a value between 0 and 1 to use a combination of L1 and L2 regularization, which is known as elastic net regularization.\nFor the example here, we will go with L1 regularization, but feel free to experiment with L2 regularization as well. We will fit the model to a range of \\lambda values, and then evaluate the model deviance on the test data set. The model with the lowest deviance on the test data set will be the one we use.\n\n# specify training model\ny_train, X_train = format_tables(v1_resp_train, '16', fm='spike_count ~ C(orientation) * C(temporal_frequency)')\nexp_mdl = sm.GLM(y_train, X_train, family=sm.families.Poisson())\n\n# test data\ny_test, X_test = format_tables(v1_resp_test, '16', fm='spike_count ~ C(orientation) * C(temporal_frequency)')\n\n# fit model with different lambdas\nparams = []\ndev_test = []\nlambdas = np.insert(np.geomspace(1e-5, 1e4, 20),0,0)\nfor alpha in lambdas:\n    exp_res = exp_mdl.fit_regularized(alpha=alpha, L1_wt=1)\n    params.append(exp_res.params)\n    dev_test.append(deviance(rf_test_true, exp_res.predict(X_test).reshape(8,5,3)[:,:,0]))\n\n/Users/drewheadley/anaconda3/envs/regress/lib/python3.10/site-packages/statsmodels/genmod/generalized_linear_model.py:1464: UserWarning: Elastic net fitting did not converge\n  warnings.warn(\"Elastic net fitting did not converge\")\n\n\nAbove we first format a training dataset of the response of our V1 neuron and pass it to a GLM object that will implement Poisson regression. Next we format a test dataset that will be used to evaluate the model upon repeated fittings. Last we fit the model to a range of \\lambda values and evaluate the model on the test data set. For each fitting we save the beta coefficients used by the model (params), and the deviance on the test data set.\nNext we will plot the deviance on the test data set as a function of \\lambda. This will allow us to see how the model performs as we increase the strength of the regularization penalty.\n\n# find lambda with minimum deviance\nbest_fit = np.argmin(dev_test)\n\n# plot deviance vs lambda\nfig, ax = plt.subplots()\nax.plot(lambdas, dev_test)\nax.scatter(lambdas[best_fit], dev_test[best_fit], color='r')\nax.set_yscale('log')\nax.set_xscale('log')\nax.set_xlabel('$\\lambda$')\nax.set_ylabel('Deviance')\nax.grid(which='both')\nax.set_title('Deviance vs $\\lambda$ for L1 regularized model')\n\nText(0.5, 1.0, 'Deviance vs $\\\\lambda$ for L1 regularized model')\n\n\n\n\n\n\n\n\n\nAs \\lambda increases, the deviance on the test data first decreases modestly, and then increases dramatically. The smallest deviance was near the middle of the \\lambda values we tested. This is a common pattern with regularization. The reason for why the deviance behaved this way is best illustrated using a plot of the \\beta coefficients as a function of \\lambda.\n\n# setup the colors of the coefficient lines so that they reflect the type of factor they are\ncolors = np.array(['r', 'g', 'b'])\ndef col_type(X):\n    col_names = X.design_info.column_names\n    col_types = np.zeros(len(col_names))\n    for i, col in enumerate(X.design_info.column_names):\n        if col == 'Intercept':\n            col_types[i] = 0\n        elif col.count(':') == 0: # main effect\n            col_types[i] = 1\n        else: # interaction effect\n            col_types[i] = 2\n    return col_types\n\n\n# format for plotting\ncol_type = col_type(X)\nparams = np.stack(params)\n\n# plot each line in params colored based on the type specified by col_type\nfig, ax = plt.subplots()\nfor i in range(params.shape[1]):\n    ax.plot(lambdas, params[:,i], color=colors[col_type[i].astype(int)], alpha=0.5)\n\nax.set_xscale('log')\nax.axvline(lambdas[best_fit], color='k', linestyle='--')\nax.set_xlabel('Lambda')\nax.set_ylabel('Coefficient value')\nax.set_title('Coefficient values vs. lambda')\nax.grid()\n\n\n\n\n\n\n\n\nIn the above graph we have plotted the fitted value for each \\beta coefficient as a function of \\lambda. The intercept is the red line, main effects are green, interaction terms are blue, and the dotted black line is the \\lambda with the best performance. Starting from the left side with the lowest \\lambda value, we see that initially all \\beta coefficients are non-zero. Turning up the \\lambda knob, the coefficients start to collapse. First several of the interaction effects come shooting down to zero. Turning it further the main effects start to shrink. By the time we reach the \\lambda with the best deviance on the test data set, most of the terms have either been zeroed or shrunk, with the exception of two terms that correspond to the orientation of 90 and 270 degrees (where the neuron responds most vigorously). Increasing \\lambda further past this point, all the other coefficients latch on to zero. To compensate for their loss, the two remaining ones at the preferred orientations ramp up their values. However, further increases in lambda drive those down too. In response, the intercept started to increase to compensate for the loss of those terms. Ultimately, it too collapses to zero.\nThese changes explain the wobbles and cliff in the deviance score as \\lambda increased. The initial decrease in deviance is probably due to the model being able to ignore the noise in the data. The subsequent waves in it reflect different strategies for capturing the neurons receptive field. Finally, the dramatic upswing in deviance at high \\lambda is due to the model ignoring the data entirely."
  },
  {
    "objectID": "Regression1.html",
    "href": "Regression1.html",
    "title": "Regression 1",
    "section": "",
    "text": "Today we are going to cover regression with linear models. We will start with the simplest case, just two data points, and work our way up to the general linear model that handles multiple data points and multiple features. Derivations will be shown at each stage to emphasize the mathematical underpinings of these models and their analytical symmetry. To complement this, whenever possible visualizations will be provided to help build intuition about the models and their behavior.\n# import python packages we will work with here\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# seed our random number generator to ensure reproducibility\nfrom numpy.random import MT19937\nfrom numpy.random import RandomState, SeedSequence\nrs = RandomState(MT19937(SeedSequence(1)))"
  },
  {
    "objectID": "Regression1.html#single-regression-fitting-a-straight-line",
    "href": "Regression1.html#single-regression-fitting-a-straight-line",
    "title": "Regression 1",
    "section": "Single regression: fitting a straight line",
    "text": "Single regression: fitting a straight line\nOften you will collect data by taking repeated measurements of a thing, which we will call \\boldsymbol{y}. With each measurement, you also measure another thing, or take notes about the situation occurring during the measurement, which we will call \\boldsymbol{x}. Here \\boldsymbol{x} and \\boldsymbol{y} are vectors each of length N, the number of measurements. We will think of \\boldsymbol{x} as the independent variable, which is affecting the state of \\boldsymbol{y}. \\boldsymbol{x} can be under our control or not. \\boldsymbol{y} is referred to as the dependent variable, because its state depends on \\boldsymbol{x}.\nNote: We will be doing some linear algebra in these lectures, so to keep things clear about the dimensionality of the variables we are working with, we will adopt the following convention: 1. Scalars will be represented by unbolded letters (e.g. a, b, N) 2. Vectors will be bolded lowercase letters (e.g. \\boldsymbol{a}, \\boldsymbol{b}, \\boldsymbol{n}) 3. Matrices will be bolded uppercase letters (e.g. \\boldsymbol{X}, \\boldsymbol{Y}, \\boldsymbol{N})\nIf we want to refer to a specific item in a vector, we will use a single subscript, e.g. \\boldsymbol{x}_1, \\boldsymbol{x}_2, … \\boldsymbol{x}_i. For a matrix, we use two subscripts, with the first referring to the row index, and the second to the column index, e.g. \\boldsymbol{X}_{1,1}, \\boldsymbol{X}_{2,3}, \\boldsymbol{X}_{i,j}.\n\nFitting a line exactly to two observations\nGiven these observations, \\boldsymbol{x} and \\boldsymbol{y}, we want to understand how they are related. Their relationship can be visualized by plotting each pair of observations, (\\boldsymbol{x}_i, \\boldsymbol{y}_i), which we will call a sample, on a graph. Let’s create two fake samples and plot them.\n\n# generate some random observations\nx = rs.randn(2,1)\ny = rs.randn(2,1)\n\n# plot those observations\nfig, ax = plt.subplots()\nax.scatter(x,y)\nax.grid()\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_xlim([-5, 5])\nax.set_ylim([-5, 5])\nax.set_title('Two data points')\n\nText(0.5, 1.0, 'Two data points')\n\n\n\n\n\n\n\n\n\nGiven just these two data points, what kind of relationship can be inferred? One could infer a straight line running through both of them. This is formulated as \\boldsymbol{y}_i = \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1\\boldsymbol{x_i}. This relationship can also be referred to as single regression. Here \\boldsymbol{\\beta} will be a vector of coefficients that we solve for to describe the relationship between \\boldsymbol{x} and \\boldsymbol{y}. With just two observations, we want to solve for the line that exactly passes through each of those observations. Using a bit of algebra, one can solve for each of these.\n\n\\begin{align}\ny &=  \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1 x \\notag \\\\\ny-\\boldsymbol{\\beta}_1\\ x &=  \\boldsymbol{\\beta}_0 \\notag \\\\\n\\end{align}\n\nSo, when x=0, we see that y=\\boldsymbol{\\beta}_0. This is known as the intercept of the line, since that is where the line intercepts or crosses the y-axis. To exactly solve for it given our observations, we need to know the value of \\boldsymbol{\\beta}_1. This is the slope of the line, which tells us how much y changes for a one unit change in x. This can be solved for by starting with the equation that solves for intercept and just using our two observations with the equation:\n\n\\begin{align}\n\\boldsymbol{y}_1 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_1 &= \\boldsymbol{\\beta}_0 = \\boldsymbol{y}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_0 \\notag \\\\\n\\boldsymbol{y}_1 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_1 &= \\boldsymbol{y}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_0 \\notag \\\\\n\\boldsymbol{y}_1 - \\boldsymbol{y}_0 &= \\boldsymbol{\\beta}_1\\boldsymbol{x}_1 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_0 \\notag \\\\\n\\boldsymbol{y}_1 - \\boldsymbol{y}_0 &= \\boldsymbol{\\beta}_1(\\boldsymbol{x}_1 - \\boldsymbol{x}_0) \\notag \\\\\n\\frac{\\boldsymbol{y}_1 - \\boldsymbol{y}_0}{\\boldsymbol{x}_1 - \\boldsymbol{x}_0} &= \\boldsymbol{\\beta}_1 \\notag \\\\\n\\end{align}\n\nGiven the slope, we can then solve for the intercept. Let’s do this for our two observations.\n\n# solve for the slope of the line\nslope = (y[1]-y[0])/(x[1]-x[0])\n\n# solve for the intercept of the line\nintercept = y[0] - slope*x[0]\n\n# create a vector for the beta coefficients\nbeta = np.array([intercept, slope])\n\nprint('The beta coefficients are: ', beta)\n\nThe beta coefficients are:  [[ 1.8568385 ]\n [-0.82212133]]\n\n\n\n# plot the fitted line, along with marks for the observations, intercept, and slope\nfig, ax = plt.subplots()\nx_line = np.array([-5, 5])\ny_line = intercept + slope*x_line\nax.plot(x_line, y_line, color='red', alpha=0.5, label='Fitted line')\nax.scatter(x, y, label='Observations')\nax.scatter(0, intercept, marker='x', color='green', label='Intercept')\nax.plot(x, [y[1], y[1]], color='purple', linestyle='--', label='dx')\nax.plot([x[0], x[0]], y, color='orange', linestyle='--', label='dy')\nax.legend()\nax.grid()\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_xlim([-5, 5])\nax.set_ylim([-5, 5])\nax.set_title('Two data points with fitted line')\n\nText(0.5, 1.0, 'Two data points with fitted line')\n\n\n\n\n\n\n\n\n\n\n\nFitting a line to more than two observations\nHopefully, you will be working with more than just two data points. So now we have to think about solving for the best fit line that passes through many data points. With multiple data points we cannot use the same algebraic approach as above. Instead, we create an equation that captures the difference between the observed data points and a proposed line, then try to derive an equation that tells us what to set the \\boldsymbol{\\beta} coefficients to so that its value is minimized. To start, let’s first create some fake data and plot it.\n\n# create fake set of observations with known slope and intercept\nx = rs.randn(100,1)\ny = 1 + 2*x + rs.randn(100,1)\n\n# plot those observations\nfig, ax = plt.subplots()\nax.scatter(x,y)\nax.grid()\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_xlim([-5, 5])\nax.set_ylim([-7, 7])\nax.set_title('100 data points')\n\nText(0.5, 1.0, '100 data points')\n\n\n\n\n\n\n\n\n\nTo create this data, we first specified a line with a known intercept and slope (\\boldsymbol{\\beta}_0=1, \\boldsymbol{\\beta}_1=2). We then added some random noise to the data to capture variability in the process we are observing or error in our measurements. Because of this error, we will not be able to perfectly fit a line to the data. Instead, we will try to find a line that best fits the observed data. Put another way, one that minimizes the error between it and the observed data. This error is known as the residual and is calculated as the difference between the observed data and the predicted data from the line. The residual for observation i is calculated as \\boldsymbol{y}_i - (\\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1\\boldsymbol{x}_i).\nMoving forward, since the line will not perfectly match each data point, we will adjust our notation slightly. We will denote the observed data as \\boldsymbol{y} and the predicted data as \\boldsymbol{\\hat{y}} (read as ‘y-hat’). The predicted data is calculated as \\boldsymbol{\\hat{y}} = \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1\\boldsymbol{x}.\n\n\nMean squared error\nHow should we measure the error between the observed data and the line? Whatever measure we choose, it should be a single number that increases as the observations fall farther away from the line. Since an observation can be both above and below the line, we can take the difference between the observed value of \\boldsymbol{y} and its predicted value, \\hat{\\boldsymbol{y}}, and square it. This will ensure that the error is always positive. We can then take the mean of the squared errors across all observations to get a single overall error. We will call this the mean squared error (MSE), which is expressed mathematically as:\n\n\\begin{align}\nMSE &= \\frac{1}{N}\\sum_{i=1}^{N}(\\boldsymbol{y}_i - (\\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1\\boldsymbol{x}_i))^2 \\notag \\\\\n&= \\frac{1}{N}\\sum_{i=1}^{N}(\\boldsymbol{y}_i - \\boldsymbol{\\hat{y}}_i)^2 \\notag \\\\\n\\end{align}\n\nLet’s create a function that calculates the mean squared error between the predicted line and data.\n\n# define MSE function\ndef MSE(obs, pred):\n    return np.mean((obs - pred)**2)\n\ndef pred_line(x, beta):\n    return beta[0] + beta[1]*x\n\nNow let’s try a few different prospective lines and see how well they fit the data.\n\n# beta coefficients for three different prospective lines\nbeta1 = np.array([0, 1])\nbeta2 = np.array([1, -2])\nbeta3 = np.array([-3, 1])\n\n# plot each of the lines, along with the observations, and indicate the MSE\nfig, ax = plt.subplots(1,3, figsize=(9,3))\nfor i, beta in enumerate([beta1, beta2, beta3]):\n    y_pred = pred_line(x, beta)\n    y_line = pred_line(np.array([-5, 5]), beta)\n    ax[i].plot([-7, 7], y_line, color='red', alpha=0.5, label='Proposed line')\n    ax[i].scatter(x, y, label='Observations')\n    ax[i].grid()\n    ax[i].set_xlabel('x')\n    ax[i].set_ylabel('y')\n    ax[i].set_xlim([-5, 5])\n    ax[i].set_ylim([-7, 7])\n    ax[i].set_title('MSE: {:.2f}'.format(MSE(y, y_pred)))\nplt.legend()\n\n\n\n\n\n\n\n\nNone of the lines we randomly created are good fits to the data. The first one passes through the data and is the most closely aligned to its slope, but it is still not a good fit. The second line passes through but its slope is going in the opposite direction. And the third line is even worse, and hardly passes through our observations.\nPerhaps we should try to be methodical about finding the best fit line. One way to do that is to systematically vary the intercept and slope of the best fit line and calculate the mean squared error for each combination. We can then find the combination of intercept and slope that minimizes the mean squared error. If we do this as a grid search, stepping through each combination, we can create a plot of how the MSE varies with each combination. This allows us to visualize the error surface and identify its minimum. Let’s do this.\n\n# create a grid of intercept and slope values\nintercepts = np.linspace(-5, 5, 100)\nslopes = np.linspace(-5, 5, 100)\nintercepts, slopes = np.meshgrid(intercepts, slopes)\n\n# calculate the MSE for each combination of intercept and slope\nMSEs = np.zeros(intercepts.shape)\nfor i in range(intercepts.shape[0]):\n    for j in range(intercepts.shape[1]):\n        beta = np.array([intercepts[i,j], slopes[i,j]])\n        MSEs[i,j] = MSE(y, pred_line(x, beta))\n\n# plot the MSE surface\nfig,ax = plt.subplots()\nax.contourf(intercepts, slopes, MSEs, levels=10)\nCS = ax.contour(intercepts, slopes, MSEs, levels=10, colors='white')\nax.scatter(1, 2, color='red')\nax.clabel(CS, fontsize=10)\nax.set_xlabel('Intercept')\nax.set_ylabel('Slope')\nax.set_title('MSE surface')\nax.set_aspect('equal')\n\n\n\n\n\n\n\n\nThe above contour plot shows us the error surface for the mean squared error. The x-axis represents the intercept and the y-axis represents the slope. The color of the surface represents the error, with blue being lowest error and yellow the highest. The red dot is the intercept and slope we used to create the data. As you can see, the error surface has a valley centered on the true intercept and slope. This is where the error is minimized, which is why we want to find the minimum of this surface.\nNote that the error surface has a bowl-like shape. This is because the error is a quadratic function of the intercept and slope. This is a nice property because it means that the error surface has a single minimum (a global minimum). This is not always the case with other models, which can have multiple minima that are higher in elevation (local minima). This is one reason why linear models are so popular, because they are easy to optimize. Error surfaces with such a shape are known as convex. Convexity is a desirable property in optimization because it means that there is a single minimum that can be found no matter where start, simply by following the slope of the surface downhill. For most machine learning model this is often done iteratively, by taking small steps in the direction of the slope, which is known as gradient descent. Since our curve is analytically simple, we can use a more direct method, and precisely solve for the minimum.\n\n\n\n\n\n\nHelpful mathematical rules\n\n\n\nFor derivatives:\n1. Constant rule: \\frac{d}{dx}c = 0\n2. Addition rule: \\frac{d}{dx}(f(x) + g(x)) = \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x)\n3. Power rule: \\frac{d}{dx}x^n = nx^{n-1}\n4. Chain rule: \\frac{d}{dx}f(g(x)) = f'(g(x))g'(x)\nSums:\n1. Mean: \\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\n2. Multiplication by N: N\\bar{x} = \\sum_{i=1}^{N}x_i\n3. Associativity: \\sum_{i=1}^{N}(a_i + b_i) = \\sum_{i=1}^{N}a_i + \\sum_{i=1}^{N}b_i\n4. Distributivity: \\sum_{i=1}^{N}a(b_i) = a\\sum_{i=1}^{N}b_i\n\n\n\n\nError minimization with ordinary least squares\nAbove we found the minimum using a grid search, stepping through many combinations of intercepts and slopes. This is an inefficient way to find the minimum. Moreover, if our minimum is outside the range of the grid, when we will not stumble upon it. Alternatively, we could use gradient descent and choose a random point on the error landscape from which to coast down to the minimum. To do this we have to the gradient of the surface at our starting point, move a little bit downhill along that gradient, and repeating the process at each new point until we reach the bottom. This too is computationally expensive. Instead, we should try to find the minimum using a method that directly calculates it. One such method is ordinary least squares (OLS). The starting point of this approach is recognizing that the error surface almost always has only one minimum and at that point the gradient is zero. This is because the error surface of the MSE when fitting a line is almost always a parabola, with a single minimum point at the location where the intercept and slope create a best fit line for the data. All other combinations of intercept and slope will have higher error. We can solve for where that point is by taking the derivative of the MSE with respect to the intercept and the slope, and finding where those derivatives are equal to zero.\nWe can use some calculus and algebra to solve for the intercept and slope that minimize the mean squared error. Let’s start with the intercept:\n\n\\begin{align}\n\\frac{\\partial MSE}{\\partial \\boldsymbol{\\beta}_0} &= \\frac{1}{N}\\sum_{i=1}^{N}(\\boldsymbol{y}_i - \\boldsymbol{\\hat{y}_i})^2 \\frac{d}{d\\boldsymbol{\\beta}_0}\\notag \\\\\n&= \\frac{1}{N}\\sum(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i)^2 \\frac{d}{d\\boldsymbol{\\beta}_0}\\notag \\\\\n&= \\frac{1}{N}\\sum 2(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i)(-1) \\notag \\\\\n&= -2\\frac{1}{N}\\sum(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i) \\notag \\\\\n\\end{align}\n\nNow we set the derivative equal to zero and solve for \\boldsymbol{\\beta}_0:\n\n\\begin{align}\n0 &= -2\\frac{1}{N}\\sum(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i) \\notag \\\\\n&= \\sum(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i) \\notag \\\\\n&= \\sum\\boldsymbol{y}_i - \\sum\\boldsymbol{\\beta}_0 - \\sum\\boldsymbol{\\beta}_1\\boldsymbol{x}_i \\notag \\\\\n&= \\sum\\boldsymbol{y}_i - N\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\sum\\boldsymbol{x}_i \\notag \\\\\nN\\boldsymbol{\\beta}_0 &= \\sum\\boldsymbol{y}_i - \\boldsymbol{\\beta}_1\\sum\\boldsymbol{x}_i \\notag \\\\\n\\boldsymbol{\\beta}_0 &= \\frac{1}{N}\\sum\\boldsymbol{y}_i - \\boldsymbol{\\beta}_1\\frac{1}{N}\\sum\\boldsymbol{x}_i \\notag \\\\\n\\boldsymbol{\\beta}_0 &= \\bar{\\boldsymbol{y}} - \\boldsymbol{\\beta}_1\\bar{\\boldsymbol{x}} \\notag \\\\\n\\end{align}\n\nNotice how the equation for the intercept resembles the one we found for just two observations (\\boldsymbol{\\beta}_0 = \\boldsymbol{y} - \\boldsymbol{\\beta}_1\\boldsymbol{x}). The only difference is that we are now using the mean of the observations, \\bar{\\boldsymbol{y}} and \\bar{\\boldsymbol{x}} (read as ‘x-bar’), instead of the variables representing x and y. Thought of another way, what this equation is saying is that if you start at the intercept and move in the direction of the slope towards the mean of the \\boldsymbol{x} values, you will end up at the mean of the \\boldsymbol{y} values. You can visualize this as so:\n\n# plot the observations with lines for the mean of X and mean of Y\nfig, ax = plt.subplots()\nax.scatter(x, y)\nax.scatter(0, 1, marker='x', color='red', label='Intercept')\nax.axvline(x.mean(), color='green', linestyle='--', label='$\\\\boldsymbol{\\\\bar{x}}$')\nax.axhline(y.mean(), color='purple', linestyle='--', label='$\\\\boldsymbol{\\\\bar{y}}$')\nax.arrow(0, 1, x.mean(), x.mean()*2, head_width=0.15, head_length=0.15, fc='black', ec='black', label='Moving towards mean of Y' )\nax.grid()\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_xlim([-2, 2])\nax.set_ylim([-2, 2])\nax.set_title('Geometric interpretation of $\\\\boldsymbol{\\\\beta}_0=\\\\boldsymbol{\\\\bar{y}} - \\\\boldsymbol{\\\\beta}_1\\\\boldsymbol{\\\\bar{x}}$')\nax.legend()\n\n\n\n\n\n\n\n\nNotice how the line emerging from the intercept and moving in the direction of the slope passes close to where the mean of the y value is. Note also that it does not exactly line up. This is because we used the actual slope and intercept to generate this plot. Since noise was added to the data, the best fit line will be slightly different from the \\beta coefficients we used to generate the data. To get those values, we now need to solve for the slope, \\boldsymbol{\\beta}_1.\n\n\\begin{align}\n\\frac{\\partial MSE}{\\partial \\boldsymbol{\\beta}_1} &= \\frac{1}{N}\\sum_{i=1}^{N}(\\boldsymbol{y}_i - \\boldsymbol{\\hat{y}_i})^2 \\frac{d}{d\\boldsymbol{\\beta}_1}\\notag \\\\\n&= \\frac{1}{N}\\sum(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i)^2 \\frac{d}{d\\boldsymbol{\\beta}_1}\\notag \\\\\n&= \\frac{1}{N}\\sum 2(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i)(-\\boldsymbol{x}_i) \\notag \\\\\n&= -2\\frac{1}{N}\\sum(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i)\\boldsymbol{x}_i \\notag \\\\\n\\end{align}\n\nNow we set the derivative equal to zero and solve for \\boldsymbol{\\beta}_1:\n\n\\begin{align}\n0 &= -2\\frac{1}{N}\\sum(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i)\\boldsymbol{x}_i \\notag \\\\\n&= \\sum(\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_1\\boldsymbol{x}_i)\\boldsymbol{x}_i \\notag \\\\\n&= \\sum\\boldsymbol{x}_i\\boldsymbol{y}_i - \\boldsymbol{\\beta}_0\\boldsymbol{x}_i-\\boldsymbol{\\beta}_1\\boldsymbol{x}_i^2 \\notag \\\\\n&= \\sum\\boldsymbol{x}_i\\boldsymbol{y}_i - (\\bar{\\boldsymbol{y}}-\\boldsymbol{\\beta}_1\\bar{\\boldsymbol{x}})\\boldsymbol{x}_i-\\boldsymbol{\\beta}_1\\boldsymbol{x}_i^2 \\notag \\\\\n&= \\sum\\boldsymbol{x}_i\\boldsymbol{y}_i - \\bar{\\boldsymbol{y}}\\boldsymbol{x}_i+\\boldsymbol{\\beta}_1\\bar{\\boldsymbol{x}}\\boldsymbol{x}_i-\\boldsymbol{\\beta}_1\\boldsymbol{x}_i^2 \\notag \\\\\n&=  \\sum\\boldsymbol{x}_i\\boldsymbol{y}_i - \\sum\\bar{\\boldsymbol{y}}\\boldsymbol{x}_i+ \\sum\\boldsymbol{\\beta}_1\\bar{\\boldsymbol{x}}\\boldsymbol{x}_i -\\sum\\boldsymbol{\\beta}_1\\boldsymbol{x}_i^2 \\notag \\\\\n&= \\sum\\boldsymbol{x}_i\\boldsymbol{y}_i - \\bar{\\boldsymbol{y}}\\sum\\boldsymbol{x}_i + \\boldsymbol{\\beta}_1\\bar{\\boldsymbol{x}}\\sum\\boldsymbol{x}_i - \\boldsymbol{\\beta}_1\\sum\\boldsymbol{x}_i^2 \\notag \\\\\n&= \\sum\\boldsymbol{x}_i\\boldsymbol{y}_i - N\\bar{\\boldsymbol{y}}\\bar{\\boldsymbol{x}} + \\boldsymbol{\\beta}_1N\\bar{\\boldsymbol{x}}^2 - \\boldsymbol{\\beta}_1\\sum\\boldsymbol{x}_i^2 \\notag \\\\\n&= \\sum\\boldsymbol{x}_i\\boldsymbol{y}_i - N\\bar{\\boldsymbol{y}}\\bar{\\boldsymbol{x}} + \\boldsymbol{\\beta}_1(N\\bar{\\boldsymbol{x}}^2-\\sum\\boldsymbol{x}_i^2) \\notag \\\\\n\\boldsymbol{\\beta}_1(-N\\bar{\\boldsymbol{x}}^2+\\sum\\boldsymbol{x}_i^2) &= \\sum\\boldsymbol{x}_i\\boldsymbol{y}_i - N\\bar{\\boldsymbol{y}}\\bar{\\boldsymbol{x}} \\notag \\\\\n\\boldsymbol{\\beta}_1 &= \\frac{\\sum\\boldsymbol{x}_i\\boldsymbol{y}_i - N\\bar{\\boldsymbol{y}}\\bar{\\boldsymbol{x}}}{\\sum\\boldsymbol{x}_i^2-N\\bar{\\boldsymbol{x}}^2} \\notag \\\\\n\\end{align}\n\nIf you do a lot more tricky algebra, we can simplify this equation further to:\n\n\\begin{align}\n\\boldsymbol{\\beta}_1 &= \\frac{\\sum(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})(\\boldsymbol{y}_i-\\bar{\\boldsymbol{y}})}{\\sum(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^2} \\notag \\\\\n\\end{align}\n\nHow do we interpret this equation? The numerator captures the covariance between \\boldsymbol{x} and \\boldsymbol{y}. The denominator is the variance of \\boldsymbol{x}. (Note that because you are dividing the covariance by the variance, the 1/N term in each of those is canceled out). This means that the slope of the best fit line is the covariance between \\boldsymbol{x} and \\boldsymbol{y} divided by the variance of \\boldsymbol{x}. Put another way, the slope of the best fit line is a measure of how much \\boldsymbol{y} changes with respect to \\boldsymbol{x} for a one unit change in \\boldsymbol{x}. Again, notice the symmetry with our calculation for just two data points, where the slope was the change in the value of y over the change in the value of x.\nNow that we have equations for the intercept and slope, we can create a function that calculates the beta coefficients of the best fit line.\n\n# solves for best fit line with OLS, verbose version\ndef OLS_single_verbose(x, y):\n    beta1 = np.sum((x - x.mean())*(y - y.mean()))/np.sum((x - x.mean())**2)\n    beta0 = y.mean() - beta1*x.mean()\n    return beta0, beta1\n\n# solves for best fit line with OLS, concise version\ndef OLS_single(x,y):\n    beta1 = np.cov(x.T, y.T, bias=True)[0,1]/np.var(x) # note bias=True, which divides by n instead of n-1\n    beta0 = y.mean() - beta1*x.mean()\n    return beta0, beta1\n\n# solve for the best fit line\nprint('The beta coefficients are: intercept=1, slope=2')\nintercept, slope = OLS_single_verbose(x, y)\nprint('Verbose version: intercept={}, slope={}'.format(intercept, slope))\nintercept, slope = OLS_single(x, y)\nprint('Concise version: intercept={}, slope={}'.format(intercept, slope))\n\nThe beta coefficients are: intercept=1, slope=2\nVerbose version: intercept=1.0325148699057354, slope=1.9071403792001222\nConcise version: intercept=1.0325148699057354, slope=1.9071403792001227\n\n\nBoth versions came very close to the true values of intercept and slope. Surpisingly, they differ only slightly in the intercept despite receiving the exact same data. This likely arises from numerical precision issues in the calculations. To check if our hand coded functions are correct, we can use the built-in Numpy function np.polyfit to calculate the beta coefficients. np.polyfit is a Numpy function that takes two variables data and returns the coefficients of a best fit line. It is especially useful because it supports lines with with additional terms, such as quadratic (deg=2) or cubic (deg=3). Here we will just fit a straight line, so we will set deg=1.\n\n# solves for best fit line with OLS, builtin function version\ndef OLS_single_builtin(x, y):\n   coefs = np.polyfit(x.flatten(), y.flatten(), 1)\n   return coefs[1], coefs[0]\n# solve for the best fit line\nintercept, slope = OLS_single_builtin(x, y)\nprint('Numpy built-in function: intercept={}, slope={}'.format(intercept, slope))\n\nNumpy built-in function: intercept=1.0325148699057347, slope=1.9071403792001218\n\n\nThe results are the same down to the 15th decimal place. This is a good sign that our functions are working correctly."
  },
  {
    "objectID": "Regression1.html#multiple-regression-fitting-a-flat-plane",
    "href": "Regression1.html#multiple-regression-fitting-a-flat-plane",
    "title": "Regression 1",
    "section": "Multiple regression: fitting a flat plane",
    "text": "Multiple regression: fitting a flat plane\nSo far we have dealt with fitting a line to describe the relationship between y and a single factor x, known as single regression. Here two measurements were made for each observation (y and x). This is sufficient if we have one factor we are measuring or manipulating to affect the state of another variable. Often, however, there is more than one factor that affects the state of the dependent variable. In that case, known as multiple regression, we need to estimate how each factor affects the dependent variable. If we have two factors that are linearly affecting the dependent variable, we can fit a plane to the data. This is formulated as \\boldsymbol{y}_i = \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1\\boldsymbol{x}_{1,i} + \\boldsymbol{\\beta}_2\\boldsymbol{x}_{2,i}. Here \\boldsymbol{x}_1 and \\boldsymbol{x}_2 are vectors of length N that represent the two factors affecting the dependent variable. \\boldsymbol{\\beta} will be a vector of coefficients that we solve for to describe the relationship between \\boldsymbol{x}_1, \\boldsymbol{x}_2, and \\boldsymbol{y}. This could be visualized as a plane in three dimensions, with the two factors on the x and y axes and the dependent variable on the z axis. For instance:\n\n# create simulated surface\nx1 = np.linspace(-2, 2, 100)\nx2 = np.linspace(-2, 2, 100)\nX1, X2 = np.meshgrid(x1, x2)\nY = 1 + 2*X1 + 3*X2\n\n# plot the surface y as a function of x1 and x2\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.plot_surface(X1, X2, Y, alpha=0.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_zlabel('y')\nax.set_title('Surface where $y = 1 + 2x_1 + 3x_2$')\n\nText(0.5, 0.92, 'Surface where $y = 1 + 2x_1 + 3x_2$')\n\n\n\n\n\n\n\n\n\nBeyond two factors, you could have 3, or 4, or many many more. In this case, you are fitting a hyperplane to the data. This is expressed mathematically as:\n\n\\begin{align}\n\\boldsymbol{y}_i &= \\boldsymbol{\\beta}_0 + \\boldsymbol{\\beta}_1\\boldsymbol{x}_{1,i} + \\boldsymbol{\\beta}_2\\boldsymbol{x}_{2,i} + \\boldsymbol{\\beta}_3\\boldsymbol{x}_{3,i} + ... + \\boldsymbol{\\beta}_p\\boldsymbol{x}_{p,i} \\notag \\\\\n\\boldsymbol{y}_i &= \\boldsymbol{\\beta}_0 + \\sum_{j=1}^{p}\\boldsymbol{\\beta}_j\\boldsymbol{x}_{j,i} \\notag \\\\\n\\end{align}\n\nThis is referred to as a General Linear Model or just Linear Model. Many software packages abbreviate this as LM (the G is usually reserved for Generalized Linear models, which we will cover next lecture).\n\nThe matrix version of OLS\nThe least squares equation we developed for the fitting a line (single regression) can be generalized to this hyperplane case (multiple regression). To expedite this, we will switch to matrix notation. For this, the equation \\boldsymbol{y}_i = \\boldsymbol{\\beta}_0 + \\sum_{j=1}^{p}\\boldsymbol{\\beta}_j\\boldsymbol{x}_{j,i} can be rewritten as \\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta}, where \\boldsymbol{y} is a vector of length N representing the dependent variable, \\boldsymbol{X} is a matrix of size N \\times (p+1) representing the factors affecting the dependent variable (with the +1 for the intercept), and \\boldsymbol{\\beta} is a vector of length p+1 representing the coefficients, one for the intercept and each factor. The matrix \\boldsymbol{X} is constructed by stacking the factor vectors \\boldsymbol{x}_1, \\boldsymbol{x}_2, …, \\boldsymbol{x}_p as columns. The vector \\boldsymbol{\\beta} is solved for by minimizing the mean squared error between the observed data and the predicted data from the hyperplane. This gives us the familiar equation we seek to minimize:\n\n\\begin{align}\nMSE &= \\frac{1}{N}\\sum_{i=1}^N(\\boldsymbol{y}_i - \\boldsymbol{X}_{i,}\\boldsymbol{\\beta})^2 \\notag \\\\\n\\end{align}\n\nJust as before, we need to find the \\boldsymbol{\\beta} that minimizes the mean squared error. This can be done by taking the derivative of the MSE with respect to \\boldsymbol{\\beta} and setting it equal to zero. This will give us the equation for the best fit coefficients. The first step is to realize that \\sum\\boldsymbol{a}^2 = \\boldsymbol{a}^T\\boldsymbol{a}, where \\boldsymbol{a} is a vector. This allows us to rewrite the MSE as:\n\n\\begin{align}\nMSE &= \\frac{1}{N}(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta})^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) \\notag \\\\\n&= \\frac{1}{N}(\\boldsymbol{y}^T - \\boldsymbol{\\beta}^T\\boldsymbol{X}^T)(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}) \\notag \\\\\n&= \\frac{1}{N}(\\boldsymbol{y}^T\\boldsymbol{y} - \\boldsymbol{y}^T\\boldsymbol{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{y} + \\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta}) \\notag \\\\\n&= \\frac{1}{N}(\\boldsymbol{y}^T\\boldsymbol{y} - 2\\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{y} + \\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta}) \\notag \\\\\n\\end{align}\n\nFrom here, we can solve for the derivative of the MSE with respect to \\boldsymbol{\\beta}.\n\n\\frac{\\partial{MSE}}{\\partial{\\boldsymbol{\\beta}}} = \\frac{1}{N}(-2\\boldsymbol{X}^T\\boldsymbol{y} +2\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta}) \\notag \\\\\n\nAnd just as the case with single regression, we can set this derivative equal to zero and solve for \\boldsymbol{\\beta}.\n\n\\begin{align}\n0&= \\frac{1}{N}(-2\\boldsymbol{X}^T\\boldsymbol{y} +2\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta}) \\notag \\\\\n0&=-\\boldsymbol{X}^T\\boldsymbol{y} +\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta} \\notag \\\\\n-\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta} &= -\\boldsymbol{X}^T\\boldsymbol{y}  \\notag \\\\\n\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta} &= \\boldsymbol{X}^T\\boldsymbol{y} \\notag \\\\\n\\boldsymbol{\\beta} &= \\frac{\\boldsymbol{X}^T\\boldsymbol{y}}{(\\boldsymbol{X}^T\\boldsymbol{X})} \\notag \\\\\n\\end{align}\n\nViola, there it is. Notice how it is similar to the single regression case. The denominator is a matrix of the sum of squares for each factor (along the diagonal) and the cross sum of squares between factors (off diagonal elements). This captures the scale of each independent variable and how they are covarying with one another. The numerator is a vector that captures the covariation between each independent variable and the dependent variable. This is a generalization of the slope in the single regression case. The form I have written it in above underscores the symmetry between these two equations, but it is more commonly written as: \\boldsymbol{\\beta} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\nYou might be wondering why we are not also solving for the intercept. This is because the intercept is already included in the matrix \\boldsymbol{X} as a column of ones. This is why the matrix \\boldsymbol{X} is of size N \\times (p+1), with the +1 for the intercept. This allows us to solve for the intercept and the coefficients of the factors in a single step. Thus, when solving for \\boldsymbol{\\beta}, you should make sure to include the intercept in the matrix \\boldsymbol{X} as a column of 1s.\nWe can now write our own function to calculate the beta coefficients of a multiple regression problem. Let’s give that a try.\n\n# solves for best fit hyperplane with OLS\ndef OLS_multiple_verbose(X, y):\n    # preallocate space for xtx and xty\n    dep_num = X.shape[1]\n    xtx = np.zeros((dep_num, dep_num))\n    xty = np.zeros((dep_num, 1))\n\n    # calculate the denominator, xtx\n    for i in range(dep_num):\n        for j in range(dep_num):\n            xtx[i,j] = np.sum(X[:,i]*X[:,j])\n    \n    # calculate the numerator, xty\n    for i in range(dep_num):\n        xty[i] = np.sum(X[:,i]*y)\n\n    # solve for beta\n    beta = np.linalg.inv(xtx) @ xty \n    return beta\n\n# solves for best fit hyperplane with OLS, concise version\ndef OLS_multiple(X, y):\n    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n    return beta\n\n# create fake set of observations with known coefficients\nX = np.concat((np.ones((100,1)), rs.randn(100,3)), axis=1)\nbeta_true = np.array([1, 2, 3, 4])\ny = X @ beta_true + rs.randn(100)\n\n\n# solve for the best fit hyperplane\nbeta = OLS_multiple_verbose(X, y)\nprint('The true beta coefficients are: ', beta_true)\nprint('Verbose version: ', beta.flatten())\n\nbeta = OLS_multiple(X, y)\nprint('Concise version: ', beta)\n\nThe true beta coefficients are:  [1 2 3 4]\nVerbose version:  [1.02546624 2.07215648 2.87082363 3.93774308]\nConcise version:  [1.02546624 2.07215648 2.87082363 3.93774308]\n\n\nNumpy also has a built-in function for solving for the beta coefficients of a multiple regression problem. This function is np.linalg.lstsq. It takes the matrix of factors and the dependent variable and returns the beta coefficients. Let’s use this function to check if our hand coded function is correct.\n\nbeta = np.linalg.lstsq(X, y)[0]\nprint('Numpy built-in function: ', beta)\n\nNumpy built-in function:  [1.02546624 2.07215648 2.87082363 3.93774308]\n\n\nExactly the same! Sometimes (but not always!) math is math no matter how you code it.\n\n\nMultiple regression with the statsmodels package\nSo far we have used Numpy to solve our regression problems. This is fine if we are just solving for the beta coefficients. However, if we want to do more advanced statistical analysis, such as hypothesis testing, confidence intervals, or model comparison, we need to use a more advanced package. One such package is statsmodels. This package is built on top of Numpy and Scipy and provides a more comprehensive set of tools for statistical analysis. Let’s use this package to solve for the beta coefficients of a multiple regression problem.\nIn the statsmodels package there is a function, OLS, that takes the dependent variable (\\boldsymbol{y}) and the matrix of independent variables/factors (\\boldsymbol{X}) and returns a RegressionResults model object. In the lingo of the statsmodels package, the dependent variable is referred to as the ‘endogenous’ variable, while the independent variable is the ‘exogenous’ variable. Once the OLS model object is created, you fit the beta coefficients by calling its fit method. This returns a results object. The results object contains a lot of information about the model. The beta coefficients are stored in the params attribute.\n\n# create an OLS model using statsmodels\nols_mdl = sm.OLS(y, X)\n\n# fit the beta coefficients\nresults = ols_mdl.fit()\n\n# return the beta coefficients\nprint('Statsmodels beta coefficients: ', results.params)\n\nStatsmodels beta coefficients:  [1.02546624 2.07215648 2.87082363 3.93774308]\n\n\nAgain, we find that the fitted beta coefficients match those we obtained with our home brew versions."
  },
  {
    "objectID": "Regression1.html#constructing-a-design-matrix",
    "href": "Regression1.html#constructing-a-design-matrix",
    "title": "Regression 1",
    "section": "Constructing a design matrix",
    "text": "Constructing a design matrix\n\nContinuous factor\nNow that we can fit a model with a simple function, we will explore how to design our \\boldsymbol{X} matrix to model different kinds of experimental situations. Recall that the \\boldsymbol{X} matrix is a matrix of size N \\times (p+1), where N is the number of observations and p is the number of factors. The first column of the matrix is a column of ones, which represents the intercept. The intercept can be thought of as capturing the mean value of the dependent variable when all the factors are zero. The remaining columns are the factors that affect the dependent variable. In the example data we generated above, the factors were continuous variables. That means they were real numbers that could take on any value. For single regression there was only one factor, while for multiple regression we included three factors. Something I find helpful when thinking about regression problems is to visualize the design matrix. This can help you understand how the factors are affecting the dependent variable. Let’s visualize the design matrix for some of the data we generated above.\n\ndef plot_dmat(df, num_rows=10, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    X = df.values\n    varnames = df.columns\n    max_abs = np.max(np.abs(X[:num_rows]))\n    axh = ax.imshow(X[:num_rows], cmap='bwr', norm=plt.Normalize(vmin=-max_abs, vmax=max_abs))\n    ax.set_xlabel('Dependent variable')\n    ax.set_ylabel('Observations')\n    ax.set_title('Design matrix for the first 10 observations')\n    if varnames is not None:\n        ax.set_xticks(np.arange(X.shape[1]))\n        ax.set_xticklabels(varnames)\n        ax.tick_params(axis='x', rotation=90)\n    ax.figure.colorbar(axh)\n    return ax\n\n# turn our design matrix into a pandas dataframe, which will allow us to label the columns\ndmat = pd.DataFrame(X, columns=['Intercept', 'F1', 'F2', 'F3'])\nplot_dmat(dmat)\n\n\n\n\n\n\n\n\nEach row is an observation and each column a factor. The first column is the intercept and all the entries in it have the value of 1. The subsequent columns are the continuously valued independent variables we generated randomly. You can see the range of blue to red is continuous in the last three columns. I find that a color map that sets 0 to white and had all positive values as red and negative values as blue helps see the relationships between the factors.\n\n\nOrdinal categorical\nBut your independent variables do not need to be continuous. Instead they could be categorical. Categorical variables take on a limited set of values, denoting either states or values (often integers). For instance, a categorical variable could be the amount of reward payout given in a task, with values ranging from 0 to 3. When a categorical variable is composed of numerically ordered items, such as amount of reward, we refer to it as ordinal. We could simulate a categorical variable for reward payout by adding another column to our current design matrix that randomly chooses from those values for each observation.\n\n# add reward categorical column representing reward payout\nreward_cat = rs.choice([0,1,2,3], size=(100,1)) # create a categorical variable\ndmat['Rew_Cat'] = reward_cat\n\nplot_dmat(dmat)\n\n\n\n\n\n\n\n\nWith this plot it is obvious that column 4 is a categorical variable. The values are discrete and there is no continuous gradient between them. Moreover, all of its values are positive, since no blue is visible.\n\n\nBinary/dummy categorical\nSpecifying a categorical variable as an integer is fine if we think that the relationship between the dependent variable and the categorical independent variable will be monotonic (they increase or decrease together). But this is not always the case. We could imagine the case where two symbols are displayed during reward payout. Either, neither, or both could be displayed on any trial. For that, we need to create dummy variables. A dummy variable is a categorical binary variable that represents the presence or absence of an event. We create a dummy variable for each symbol that is 1 if the symbol is present and 0 if absent. This allows us to capture the effect of each symbol on the dependent variable. Let’s add a dummy variable to our design matrix.\n\n# add two dummy variables to the design matrix\ndummies = rs.choice([0,1], size=(100,2))\ndmat[['Dummy1', 'Dummy2']] = dummies\n\nplot_dmat(dmat)\n\n\n\n\n\n\n\n\nThe two new columns on the right side are the dummy variables, whose values are restricted to 0 or 1. Note that on any given trial either the first or second can have the value 1. They are independent of each other.\n\n\nOne-hot categorical\nWhat if we wanted to encode the reward payout as a dummy variable? This would allows us to model it as a categorical variable that has a nonlinear relationship with the dependent variable. We would want a dummy variable for each possible value of the reward payout. For that we would use one-hot encoding. One-hot encoding is a method of converting a categorical variable into a binary matrix. Each column of the matrix represents a possible value of the categorical variable. The column corresponding to the value of the categorical variable is 1, while all other columns are 0. Note that the number of columns we create is one less than the number of conditions. This is to avoid the problem of multicollinearity between the intercept column and our one-hot encoding columns. We will explain this further later. For now, let’s add a one-hot encoding of the reward payout to our design matrix.\n\n# one hot encode the reward payout variable\npayout_data = dmat['Rew_Cat'].values\npayout_onehot = np.zeros((100, 3))\nfor i in range(3):\n    payout_onehot[:,i] = payout_data == i+1\n\ndmat[['Rew_OH1', 'Rew_OH2', 'Rew_OH3']] = payout_onehot\n\nplot_dmat(dmat)\n\n\n\n\n\n\n\n\nHere we have added three columns to the design matrix, one for each possible payout (1, 2, and 3), excluding the 0 payout column. No trial can have more than one of these columns with a value of 1. This is because the reward payout can only take on one value at a time. If there is a non-monotonic relationship between the dependent variable and the reward payout, this design matrix will allow us to capture that relationship by giving a unique \\beta coefficient to each payout value. If the payout is zero, none of the dummy columns for payout will be 1. Instead, the intercept column will reflect that condition, or more precisely the condition where the payout was not 1, 2, or 3. You can think of each of the one-hot encoded dummy variables are representing the change in the dependent variable by presence of that payout value relative to no payout. It is important to keep in mind that you do not have to choose the 0 payout condition. We could have chosen the 1 payout condition as the reference and then the intercept would represent the condition where the payout was not 1. This is a choice you have to make when designing your model. It is often just conceptually easier to go with the 0 condition.\n\n\nInteraction term\nThe last kind of variable we consider is an interaction term. This occurs when the effect of one factor on the dependent variable depends on the value of another factor. For instance, the effect of Factor 1 on the dependent variable could depend on the symbols presented. We could model this by adding a columns to the design matrix that are the product of the reward payout and each of the symbols. This would allow us to capture the interaction between Factor 1 and the symbols. Let’s add an interaction term to our design matrix.\n\n# create the interaction terms between the categorical reward and the dummy coded symbols\nint_terms = dmat['F1'].values[:,None]*dmat[['Dummy1', 'Dummy2']].values\ndmat[['Int1', 'Int2']] = int_terms\n\nplot_dmat(dmat)\n\n\n\n\n\n\n\n\nThe interaction terms here capture how the dependence of Factor 1 changes with the presence of each symbol. For instance, the \\beta coefficient associated with ‘Int1’ tells the model how much to change the slope of the relationship between Factor 1 and the dependent variable, given that Dummy 1 was true."
  },
  {
    "objectID": "Regression1.html#evaluating-a-linear-model",
    "href": "Regression1.html#evaluating-a-linear-model",
    "title": "Regression 1",
    "section": "Evaluating a linear model",
    "text": "Evaluating a linear model\n\nDesign matrix issues\nBefore we run our linear model, we have to ensure that it meets certain criteria. Perhaps most important of these is that the columns of the design matrix are linearly independent. This means that no column can be expressed as a linear combination of the other columns. By that, I mean that no column can be expressed as a sum of the other columns multiplied by some scalar. If the columns are linearly dependent, the matrix \\boldsymbol{X}^T\\boldsymbol{X} will not be invertible and the \\beta coefficients cannot be solved for. One can imagine a situation where the design matrix has a column that is the sum of two other columns. This would mean that the effect of the two factors would be confounded and the model would not be able to distinguish between them. This is known as multicollinearity.\nThe design matrix we have created above has a multicollinearity issue. We have specified the reward payout as a categorical variable and as a one-hot encoding. This means that the sum of the one-hot encoding columns is equal to the categorical variable column (minus 1). If reward payout affects the dependent variable, it is ambiguous which of the columns, the categorical or the one-hot encoding, is responsible. To fix this we need to remove either set of columns. If we assume that the relationship between the dependent variable and the reward payout is nonlinear, we should remove the categorical variable column.\nThe issue of multicollinearity is also why when creating the one-hot encoding columns we drop one of the columns. This is to avoid the problem of multicollinearity between the intercept column and the one-hot encoding columns. If we did not drop one of the columns, the model would not be able to distinguish between the intercept and the one-hot encoding columns. This is because the sum of the one-hot encoding columns is equal to the intercept column.\n\n\n\n\n\n\nEffect of multicollinearity on the error landscape\n\n\n\nThe issue of multicollinearity can seem a bit abstract if you don’t understand how matrix inverses are calculated (e.g. gaussian elimination). To get a geometric or visual intuition for the problem, consider how the error landscape is affected if two factors are highly correlated. In this case, the error landscape will be stretched along the direction of the correlation. This means that the error surface will be very flat in the direction of the correlation and very steep in the orthogonal direction. This can make it difficult for the optimization algorithm to find the minimum of the error surface. This is why multicollinearity can cause the optimization algorithm to fail to converge. We can visualize this by plotting the error surface for a design matrix with two highly correlated factors.\nFirst we will create a couple functions to help us calculate and visualize the error surface.\n\n# calculate the loss landscape for a two factor linear model\ndef mse_landscape(X, y):\n    # perform a grid search of the parameters for the regression, just for the two factors\n    intercepts = np.linspace(-10, 10, 101)\n    factors = np.linspace(-10, 10, 101)\n    intercept, factor1, factor2 = np.meshgrid(intercepts, factors, factors)\n    MSEs = np.zeros(intercept.shape)\n    for i in range(intercept.shape[0]):\n        for j in range(intercept.shape[1]):\n            for k in range(intercept.shape[2]):\n                beta = np.array([intercept[i,j,k], factor1[i,j,k], factor2[i,j,k]])\n                MSEs[i,j,k] = MSE(y, X @ beta)\n    return MSEs, intercept, factors\n\n# plot the error surface for the \ndef plot_mse_surface(factors, MSEs, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.contourf(factors, factors, MSEs[:,50,:], levels=10)\n    CS = ax.contour(factors, factors, MSEs[:,50,:], levels=10, colors='white')\n    ax.clabel(CS, fontsize=10)\n    ax.set_xlabel('Factor 2')\n    ax.set_ylabel('Factor 1')\n    ax.set_aspect('equal')\n    ax.axhline(0, color='white', linestyle='--', alpha=0.5)\n    ax.axvline(0, color='white', linestyle='--', alpha=0.5)\n    return ax\n\n# \n\nNext we will create three design matrices. One will have two factors that are strongly positively correlated, the second with those factors lack any correlation, and the third with those factors are strongly negatively correlated.\n\n# beta values for our models to examine multicollinearity (intercept, factor1, factor2)\nbeta_mc = np.array([0, 2, 3])\n\n# create data generated by a linear model positively correlated factors\nXp = np.random.randn(100, 3)\nXp[:,2]= Xp[:,1] + np.random.randn(100)/8\nyp = Xp @ beta_mc + np.random.randn(100)\nprint('The correlation between the positively correlated factors is {:0.2f}'.format(np.corrcoef(Xp.T)[1,2]))\n\n# create data generated by a linear model uncorrelated factors\nXu = np.random.randn(100, 3)\nXu[:,2]= Xu[:,2] + np.random.randn(100)/8\nXu[:,0] = 1\nyu = Xu @ beta_mc + np.random.randn(100)\nprint('The correlation between the uncorrelated factors is {:0.2f}'.format(np.corrcoef(Xu.T)[1,2]))\n\n# create data generated by a linear model negatively correlated factors\nXn = np.random.randn(100, 3)\nXn[:,2]= -Xn[:,1] + np.random.randn(100)/8\nXn[:,0] = 1\nyn = Xn @ beta_mc + np.random.randn(100)\nprint('The correlation between the negatively correlated factors is {:0.2f}'.format(np.corrcoef(Xn.T)[1,2]))\n\nThe correlation between the positively correlated factors is 0.99\nThe correlation between the uncorrelated factors is -0.01\nThe correlation between the negatively correlated factors is -0.99\n\n\n/Users/drewheadley/anaconda3/envs/regress/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/Users/drewheadley/anaconda3/envs/regress/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n\n\nNow that we have our data, we will calculate the error landscape. In addition, to demonstrate how mulitcolinearity can cause instability in estimation of the beta coefficients, we will recalculate the betas for each design matrix multiple times, leaving out one observation each time.\n\n# get the MSE landscapes for all data\nMSEsp, intercepts, factors = mse_landscape(Xp, yp)\nMSEsu, _, _ = mse_landscape(Xu, yu)\nMSEsn, _, _ = mse_landscape(Xn, yn)\n\n# get the estimates of the beta coefficients, leaving one observation out each time\nbetasp = np.stack([np.linalg.lstsq(Xp, yp+rs.randn(100))[0] for i in range(100)])\nbetasu = np.stack([np.linalg.lstsq(Xu, yu+rs.randn(100))[0] for i in range(100)])\nbetasn = np.stack([np.linalg.lstsq(Xn, yn+rs.randn(100))[0] for i in range(100)])\n\n# plot the MSEs\nfig, ax = plt.subplots(1,3, figsize=(8,3))\nplot_mse_surface(factors, MSEsp, ax=ax[0])\nplot_mse_surface(factors, MSEsu, ax=ax[1])\nplot_mse_surface(factors, MSEsn, ax=ax[2])\n\n# plot the estimates of the beta coefficients\nax[0].scatter(betasp[:,2], betasp[:,1], color='tab:orange', marker='+')\nax[1].scatter(betasu[:,2], betasu[:,1], color='tab:orange', marker='+')\nax[2].scatter(betasn[:,2], betasn[:,1], color='tab:orange', marker='+')\nax[0].scatter(beta_mc[2], beta_mc[1], color='red')\nax[1].scatter(beta_mc[2], beta_mc[1], color='red')\nax[2].scatter(beta_mc[2], beta_mc[1], color='red')\nax[0].set_title('Positively correlated factors')\nax[1].set_title('Uncorrelated factors')\nax[2].set_title('Negatively correlated factors')\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\nFor the above graphs, the orange plus signs are the estimated \\beta coefficients and the red dot is the true \\beta coefficient.\nNotice how the valley of the error surface is streched along a diagonal for the positive and negatively correlated cases. Both are offset from the origin of the graph by the true \\beta coefficients. In the positive correlation case, the diagonal has a negative slope. As the value of one of the \\beta for one of the factors increases, the value for the other decreases. This is because as you increase the \\beta for one factor, the other must be decreased since it has the same values. On the other hand, the negative case has a positive slope. As the value of one of the \\beta for one of the factors increases, the value for the other also increases. This is because as you increase the beta for one factor, the other must also be increased since they have opposite values. This is why the error surface is stretched along the diagonal.\nYou can also see that the variety of beta estimates when we added noise to \\boldsymbol{y} fall along the diagonal. This is due to the error surface being stretched along the diagonal. By comparison, when the factors are uncorrelated the scatter of \\beta estimates is tightly clustered around the true \\beta values.\n\n\nLet’s now simulate some data using our design matrix.\n\nmdl_spec = ['Intercept', 'F1', 'F2', 'F3', \n          'Dummy1', 'Dummy2', \n          'Rew_OH1', 'Rew_OH2', 'Rew_OH3', \n          'Int1', 'Int2']\nbeta_true = np.array([1, 0, 1, 2, \n                      -1, 1,\n                      1, 3, -1,\n                      0, 1])\nX = dmat[mdl_spec].values\ny = X @ beta_true + rs.randn(100)\n\nNow that we have our design matrix and simulated \\boldsymbol{y}, we can fit a linear model to the data. We will use the OLS function in the statsmodels package to do this.\n\n# create linear model and specify the exogenous variable names\nlm_mdl = sm.OLS(y, X)\nlm_mdl.exog_names[:] = mdl_spec\n\n# fit the model\nlm_results = lm_mdl.fit()\n\nAs we did above, let’s compare the true \\beta coefficients with the \\beta coefficients we estimated with our model.\n\n# print true beta coefficiengs to 2 decimal places\nprint('Factor name: True beta, Estimated beta')\nfor i in range(beta_true.size):\n    print('{}: {:4.2f}, {:4.2f}'.format(mdl_spec[i], beta_true[i], lm_results.params[i]))\n\nFactor name: True beta, Estimated beta\nIntercept: 1.00, 1.14\nF1: 0.00, -0.43\nF2: 1.00, 1.09\nF3: 2.00, 1.89\nDummy1: -1.00, -0.89\nDummy2: 1.00, 1.18\nRew_OH1: 1.00, 0.88\nRew_OH2: 3.00, 2.57\nRew_OH3: -1.00, -1.07\nInt1: 0.00, 0.29\nInt2: 1.00, 1.24\n\n\nNote that they are all close to the true \\beta values. Since we had a 100 data points, and the noise we added was relatively small, we were able to recover the true \\beta coefficients with a high degree of accuracy. This is not always the case. If the noise is large or the number of observations is small, the \\beta coefficients will be estimated less accurately. For instance, if we reduce the number of observations to 15:\n\n# restrict model fitting to just 15 observations\nlm_mdl_sub = sm.OLS(y[:15], X[:15])\nlm_results_sub = lm_mdl_sub.fit()\n\nprint('Factor name: True beta, Estimated beta')\nfor i in range(beta_true.size):\n    print('{}: {:4.2f}, {:4.2f}'.format(mdl_spec[i], beta_true[i], lm_results_sub.params[i]))\n\nFactor name: True beta, Estimated beta\nIntercept: 1.00, 0.43\nF1: 0.00, 1.00\nF2: 1.00, 0.70\nF3: 2.00, 2.42\nDummy1: -1.00, 0.17\nDummy2: 1.00, 1.20\nRew_OH1: 1.00, 2.20\nRew_OH2: 3.00, 1.58\nRew_OH3: -1.00, -0.31\nInt1: 0.00, -0.21\nInt2: 1.00, -0.35\n\n\nNow we are off by quite a bit more. To get a more concrete sense of how well our model is fitting the data, and the reliability of our \\beta coefficients, we should examine the summary table returned by the results object.\n\n\nSummary table of the model\nAfter fitting the linear model, we get a results object that has a summary method, which provides a comprehensive summary of the model. This includes the \\beta coefficients, the standard errors of the coefficients, the t-statistics, the p-values, and the confidence intervals. Here we will return it for the model fitted to all the data and examine each of its major fields in turn.\n\nprint(lm_results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.875\nModel:                            OLS   Adj. R-squared:                  0.861\nMethod:                 Least Squares   F-statistic:                     62.35\nDate:                Mon, 13 Jan 2025   Prob (F-statistic):           7.58e-36\nTime:                        13:12:15   Log-Likelihood:                -142.24\nNo. Observations:                 100   AIC:                             306.5\nDf Residuals:                      89   BIC:                             335.1\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.1426      0.293      3.901      0.000       0.561       1.725\nF1            -0.4338      0.215     -2.017      0.047      -0.861      -0.006\nF2             1.0883      0.117      9.307      0.000       0.856       1.321\nF3             1.8867      0.124     15.159      0.000       1.639       2.134\nDummy1        -0.8930      0.248     -3.602      0.001      -1.386      -0.400\nDummy2         1.1790      0.236      4.985      0.000       0.709       1.649\nRew_OH1        0.8772      0.341      2.571      0.012       0.199       1.555\nRew_OH2        2.5657      0.320      8.006      0.000       1.929       3.202\nRew_OH3       -1.0690      0.305     -3.504      0.001      -1.675      -0.463\nInt1           0.2889      0.226      1.279      0.204      -0.160       0.738\nInt2           1.2386      0.220      5.618      0.000       0.801       1.677\n==============================================================================\nOmnibus:                        2.239   Durbin-Watson:                   2.220\nProb(Omnibus):                  0.326   Jarque-Bera (JB):                2.137\nSkew:                           0.280   Prob(JB):                        0.344\nKurtosis:                       2.553   Cond. No.                         6.87\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nModel parameters and overall fit\nThe top banner of the summary table tells us the model we fitted and its overall fit quality. You can see for ‘Model’ we have ‘OLS’ and the ‘Method’ is ‘Least Squares’. Also present in that column is the number of observations (100) and degrees of freedom of the model (10, for 10 factors that were not the intercept). Most important when evaluating the overall fit of the model is the ‘R-squared’ value. This is a measure of how much of the variance in the dependent variable is explained by the model. The R-squared values from 0, meaning none of the variance is explained, to 1, meaning all the variance is explained. In this case, the R-squared value is 0.883, meaning that 88.3% of the variance in the dependent variable is explained by the model. This is a good fit. Other more specialized versions of fit quality are returned below, such as Aikake Information Criterion (AIC) and Bayesian Information Criterion (BIC). These are used to compare the fit of different models to the data and control for the number of factors in the model.\nBelow that area is the table of coefficients. This table shows the \\beta coefficients (‘coef’), their standard errors of the coefficients (‘std err’), t-statistics (t), p-values (‘P&gt;|t|’), and confidence intervals (‘[0.025 0.975]’). The \\beta coefficients are the same as we calculated above. The standard errors are a measure of how much the \\beta coefficients vary from the true value. The t-statistics are a measure of how significant the \\beta coefficients are. The p-values are a measure of the probability that the \\beta coefficients are different from zero. The confidence intervals are a range of values that the \\beta coefficients are likely to fall within. Notice that the two factors whose true \\beta coefficients we set to 0, F1 and Int1, have very small \\beta coefficients that are not significantly different from 0. This is reflected in the p-values, which are very large. The confidence intervals also include 0. This is a good sign that the model is correctly identifying factors that do not affect the dependent variable.\nThe bottom field contains statistics that evaluate the statistical properties of the model. The ‘Omnibus’ test is a test of the normality of the residuals. The ‘Durbin-Watson’ test is a test of the independence of the residuals between observations 1 sample apart (also known as autocorrelation). Generally, a value close to 2 suggest the residuals are independent. Ours is 1.759, which makes sense since we generated each observation randomly, so there shouldn’t be any correlations between adjacent observations. The ‘Jarque-Bera’ test is a test of the skewness and kurtosis of the residuals to determine if they fit what would be expected for a normal distribution. Here it’s p-value (‘Prob(JB)’) was 0.167, which is not significant. The ‘Cond. No.’ (Condition Number test) is a test of multicollinearity. Explaining its calculation goes a bit beyond this lecture (it is the ratio between the largest and smallest singular values of the \\boldsymbol{X}^T\\boldsymbol{X}). A value greater than 30 is indicative of multicollinearity. In our case, the value is 6.87, which is good.\n\n\n\nVisualizing the model fit\nAnother way to evaluate the model is to visualize the fit of the model to the data. This can be done by plotting the predicted values of the model against the observed values. If the model fit is good, the predicted values should be close to the observed values. Moreover, the variability in the predicted values should not depend on the observed values. This is known as homoscedasticity. If the variability in the predicted values depends on the observed values, this is called heteroscedasticity. Let’s plot the predicted values of the model against the observed values.\n\n# get predicted values from the model\ny_pred = lm_results.predict()\n\n# plot the predicted values against the true values\nfig, ax = plt.subplots()\nax.scatter(y, y_pred)\nax.plot([-8, 8], [-8, 8], color='red', linestyle='--')\nax.set_xlabel('True values')\nax.set_ylabel('Predicted values')\nax.set_title('Predicted vs. true values')\nax.grid()\n\n\n\n\n\n\n\n\nOur predicted values fall along the equality line (dotted red), indicating that the predicted and observed values tend to be close to each other. This is a good sign that the model is fitting the data well. Indeed, the ‘R-squared’ field in the summary table already told us this. It can be calculated by squaring the correlation coefficient of the the observed y (\\boldsymbol{y}) with the predicted y_pred (\\boldsymbol{\\hat{y}}).\n\nr_squared = np.corrcoef(y, y_pred)**2\nprint('R-squared: ', r_squared[0,1])\n\nR-squared:  0.8750901027173\n\n\nAs you can see, same exact value as reported in the summary table.\nMoreover, the variability in the predicted values does not seem to depend on the observed values, indicating the model residuals are homoscedastic. To better visualize this, we can plot the residuals of the model against the predicted values.\n\nresid = y - y_pred\n\n# plot the residuals against the predicted values\nfig, ax = plt.subplots()\nax.scatter(y_pred, resid)\nax.axhline(0, color='red', linestyle='--')\nax.set_ylim([-4,4])\nax.set_xlabel('Predicted values')\nax.set_ylabel('Residuals')\nax.set_title('Residuals vs. predicted values')\nax.grid()\n\n\n\n\n\n\n\n\nThe residuals do not seem to vary systematically with the predicted values, reinforcing the idea that the residuals are homoscedastic. What might a similar plot look like if the residuals were not homoscedastic? We can simulate this by tweaking the model that generates the data so that one of our variables has a nonlinear relationship with the dependent variable. For this, we will make Factor 2 and 3 have a cubic (x^3) relationship with the dependent variable.\n\nX_nonlin = X.copy()\nX_nonlin[:,2:4] = X_nonlin[:,2:4]**3 # create a non-linear relationship that will be hard to fit\ny_nonlin = X_nonlin @ beta_true + rs.randn(100)\nlm_mdl_nonlin = sm.OLS(y_nonlin, X) # note use use the original X here\nlm_results_nonlin = lm_mdl_nonlin.fit()\ny_pred_nonlin = lm_results_nonlin.predict()\ny_resid_nonlin = y_nonlin - y_pred_nonlin\n\nfig, ax = plt.subplots()\nax.scatter(y_pred_nonlin, y_resid_nonlin)\nax.axhline(0, color='red', linestyle='--')\n#ax.set_ylim([-4,4])\nax.set_xlabel('Predicted values')\nax.set_ylabel('Residuals')\nax.set_title('Residuals vs. predicted values')\nax.grid()\n\n\n\n\n\n\n\n\nYou can see the cubic shape of the residuals. Negative predicted values tend to have large negative residuals, while large positive predicted values have large positive residuals. It is also apparent that the residuals systematically shift from being positive to negative as you move from a predicted value of ~-7 to ~7. Both of these phenomena are evidence for heteroscedasticity and a sign that the dependent variable has a nonlinear relationship with the independent variables."
  }
]