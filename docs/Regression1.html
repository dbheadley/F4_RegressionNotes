<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Regression 1 â€“ Regression Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Regression Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.qmd"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./Regression1.html" aria-current="page"> 
<span class="menu-text">Regression 1</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Regression2.html"> 
<span class="menu-text">Regression 2</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="./about.qmd"> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#single-regression-fitting-a-straight-line" id="toc-single-regression-fitting-a-straight-line" class="nav-link active" data-scroll-target="#single-regression-fitting-a-straight-line">Single regression: fitting a straight line</a>
  <ul class="collapse">
  <li><a href="#fitting-a-line-exactly-to-two-observations" id="toc-fitting-a-line-exactly-to-two-observations" class="nav-link" data-scroll-target="#fitting-a-line-exactly-to-two-observations">Fitting a line exactly to two observations</a></li>
  <li><a href="#fitting-a-line-to-more-than-two-observations" id="toc-fitting-a-line-to-more-than-two-observations" class="nav-link" data-scroll-target="#fitting-a-line-to-more-than-two-observations">Fitting a line to more than two observations</a></li>
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error">Mean squared error</a></li>
  <li><a href="#error-minimization-with-ordinary-least-squares" id="toc-error-minimization-with-ordinary-least-squares" class="nav-link" data-scroll-target="#error-minimization-with-ordinary-least-squares">Error minimization with ordinary least squares</a></li>
  </ul></li>
  <li><a href="#multiple-regression-fitting-a-flat-plane" id="toc-multiple-regression-fitting-a-flat-plane" class="nav-link" data-scroll-target="#multiple-regression-fitting-a-flat-plane">Multiple regression: fitting a flat plane</a>
  <ul class="collapse">
  <li><a href="#the-matrix-version-of-ols" id="toc-the-matrix-version-of-ols" class="nav-link" data-scroll-target="#the-matrix-version-of-ols">The matrix version of OLS</a></li>
  <li><a href="#multiple-regression-with-the-statsmodels-package" id="toc-multiple-regression-with-the-statsmodels-package" class="nav-link" data-scroll-target="#multiple-regression-with-the-statsmodels-package">Multiple regression with the <code>statsmodels</code> package</a></li>
  </ul></li>
  <li><a href="#constructing-a-design-matrix" id="toc-constructing-a-design-matrix" class="nav-link" data-scroll-target="#constructing-a-design-matrix">Constructing a design matrix</a>
  <ul class="collapse">
  <li><a href="#continuous-factor" id="toc-continuous-factor" class="nav-link" data-scroll-target="#continuous-factor">Continuous factor</a></li>
  <li><a href="#ordinal-categorical" id="toc-ordinal-categorical" class="nav-link" data-scroll-target="#ordinal-categorical">Ordinal categorical</a></li>
  <li><a href="#binarydummy-categorical" id="toc-binarydummy-categorical" class="nav-link" data-scroll-target="#binarydummy-categorical">Binary/dummy categorical</a></li>
  <li><a href="#one-hot-categorical" id="toc-one-hot-categorical" class="nav-link" data-scroll-target="#one-hot-categorical">One-hot categorical</a></li>
  <li><a href="#interaction-term" id="toc-interaction-term" class="nav-link" data-scroll-target="#interaction-term">Interaction term</a></li>
  </ul></li>
  <li><a href="#evaluating-a-linear-model" id="toc-evaluating-a-linear-model" class="nav-link" data-scroll-target="#evaluating-a-linear-model">Evaluating a linear model</a>
  <ul class="collapse">
  <li><a href="#design-matrix-issues" id="toc-design-matrix-issues" class="nav-link" data-scroll-target="#design-matrix-issues">Design matrix issues</a></li>
  <li><a href="#summary-table-of-the-model" id="toc-summary-table-of-the-model" class="nav-link" data-scroll-target="#summary-table-of-the-model">Summary table of the model</a></li>
  <li><a href="#visualizing-the-model-fit" id="toc-visualizing-the-model-fit" class="nav-link" data-scroll-target="#visualizing-the-model-fit">Visualizing the model fit</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Regression 1</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Today we are going to cover regression with linear models. We will start with the simplest case, just two data points, and work our way up to the general linear model that handles multiple data points and multiple features. Derivations will be shown at each stage to emphasize the mathematical underpinnings of these models and their analytical symmetry. To complement this, whenever possible visualizations will be provided to help build intuition about the models and their behavior.</p>
<div id="3f38397e" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import python packages we will work with here</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># seed our random number generator to ensure reproducibility</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> MT19937</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> RandomState, SeedSequence</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>rs <span class="op">=</span> RandomState(MT19937(SeedSequence(<span class="dv">1</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="single-regression-fitting-a-straight-line" class="level2">
<h2 class="anchored" data-anchor-id="single-regression-fitting-a-straight-line">Single regression: fitting a straight line</h2>
<p>Often you will collect data by taking repeated measurements of a thing, which we will call <span class="math inline">\boldsymbol{y}</span>. With each measurement, you also measure another thing, or take notes about the situation occurring during the measurement, which we will call <span class="math inline">\boldsymbol{x}</span>. Here <span class="math inline">\boldsymbol{x}</span> and <span class="math inline">\boldsymbol{y}</span> are vectors each of length <span class="math inline">N</span>, the number of measurements. We will think of <span class="math inline">\boldsymbol{x}</span> as the independent variable, which is affecting the state of <span class="math inline">\boldsymbol{y}</span>. <span class="math inline">\boldsymbol{x}</span> can be under our control or not. <span class="math inline">\boldsymbol{y}</span> is referred to as the dependent variable, because its state depends on <span class="math inline">\boldsymbol{x}</span>.</p>
<p>Note: We will be doing some linear algebra in these lectures, so to keep things clear about the dimensionality of the variables we are working with, we will adopt the following convention:</p>
<ol type="1">
<li>Scalars will be represented by unbolded letters (e.g.&nbsp;<span class="math inline">a</span>, <span class="math inline">b</span>, <span class="math inline">N</span>)</li>
<li>Vectors will be bolded lowercase letters (e.g.&nbsp;<span class="math inline">\boldsymbol{a}</span>, <span class="math inline">\boldsymbol{b}</span>, <span class="math inline">\boldsymbol{n}</span>)</li>
<li>Matrices will be bolded uppercase letters (e.g.&nbsp;<span class="math inline">\boldsymbol{X}</span>, <span class="math inline">\boldsymbol{Y}</span>, <span class="math inline">\boldsymbol{N}</span>)</li>
</ol>
<p>If we want to refer to a specific item in a vector, we will use a single subscript, e.g.&nbsp;<span class="math inline">\boldsymbol{x}_1</span>, <span class="math inline">\boldsymbol{x}_2</span>, â€¦ <span class="math inline">\boldsymbol{x}_i</span>. For a matrix, we use two subscripts, with the first referring to the row index, and the second to the column index, e.g.&nbsp;<span class="math inline">\boldsymbol{X}_{1,1}</span>, <span class="math inline">\boldsymbol{X}_{2,3}</span>, <span class="math inline">\boldsymbol{X}_{i,j}</span>.</p>
<section id="fitting-a-line-exactly-to-two-observations" class="level3">
<h3 class="anchored" data-anchor-id="fitting-a-line-exactly-to-two-observations">Fitting a line exactly to two observations</h3>
<p>Given these observations, <span class="math inline">\boldsymbol{x}</span> and <span class="math inline">\boldsymbol{y}</span>, we want to understand how they are related. Their relationship can be visualized by plotting each pair of observations, <span class="math inline">(\boldsymbol{x}_i, \boldsymbol{y}_i)</span>, which we will call a <em>sample</em>, on a graph. Letâ€™s create two fake samples and plot them.</p>
<div id="b176894d" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate some random observations</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rs.randn(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> rs.randn(<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot those observations</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>ax.scatter(x,y)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>ax.grid()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Two data points'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>Text(0.5, 1.0, 'Two data points')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-3-output-2.png" width="587" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Given just these two data points, what kind of relationship can be inferred? One could infer a straight line running through both of them. This is formulated as <span class="math inline">\boldsymbol{y}_i = \boldsymbol{\beta}_0 + \boldsymbol{\beta}_1\boldsymbol{x_i}</span>. This relationship can also be referred to as <em>single regression</em>. Here <span class="math inline">\boldsymbol{\beta}</span> will be a vector of <em>coefficients</em> that we solve for to describe the relationship between <span class="math inline">\boldsymbol{x}</span> and <span class="math inline">\boldsymbol{y}</span>. With just two observations, we want to solve for the line that exactly passes through each of those observations. Using a bit of algebra, one can solve for each of these.</p>
<p><span class="math display">
\begin{align}
y &amp;=  \boldsymbol{\beta}_0 + \boldsymbol{\beta}_1 x \notag \\
y-\boldsymbol{\beta}_1\ x &amp;=  \boldsymbol{\beta}_0 \notag \\
\end{align}
</span></p>
<p>So, when <span class="math inline">x=0</span>, we see that <span class="math inline">y=\boldsymbol{\beta}_0</span>. This is known as the <em>intercept</em> of the line, since that is where the line intercepts or crosses the y-axis. To exactly solve for it given our observations, we need to know the value of <span class="math inline">\boldsymbol{\beta}_1</span>. This is the <em>slope</em> of the line, which tells us how much <span class="math inline">y</span> changes for a one unit change in <span class="math inline">x</span>. This can be solved for by starting with the equation that solves for intercept and just using our two observations with the equation:</p>
<p><span class="math display">
\begin{align}
\boldsymbol{y}_1 - \boldsymbol{\beta}_1\boldsymbol{x}_1 &amp;= \boldsymbol{\beta}_0 = \boldsymbol{y}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_0 \notag \\
\boldsymbol{y}_1 - \boldsymbol{\beta}_1\boldsymbol{x}_1 &amp;= \boldsymbol{y}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_0 \notag \\
\boldsymbol{y}_1 - \boldsymbol{y}_0 &amp;= \boldsymbol{\beta}_1\boldsymbol{x}_1 - \boldsymbol{\beta}_1\boldsymbol{x}_0 \notag \\
\boldsymbol{y}_1 - \boldsymbol{y}_0 &amp;= \boldsymbol{\beta}_1(\boldsymbol{x}_1 - \boldsymbol{x}_0) \notag \\
\frac{\boldsymbol{y}_1 - \boldsymbol{y}_0}{\boldsymbol{x}_1 - \boldsymbol{x}_0} &amp;= \boldsymbol{\beta}_1 \notag \\
\end{align}
</span></p>
<p>Given the slope, we can then solve for the intercept. Letâ€™s do this for our two observations.</p>
<div id="5054f08f" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># solve for the slope of the line</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>slope <span class="op">=</span> (y[<span class="dv">1</span>]<span class="op">-</span>y[<span class="dv">0</span>])<span class="op">/</span>(x[<span class="dv">1</span>]<span class="op">-</span>x[<span class="dv">0</span>])</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># solve for the intercept of the line</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>intercept <span class="op">=</span> y[<span class="dv">0</span>] <span class="op">-</span> slope<span class="op">*</span>x[<span class="dv">0</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># create a vector for the beta coefficients</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.array([intercept, slope])</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The beta coefficients are: '</span>, beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The beta coefficients are:  [[ 1.8568385 ]
 [-0.82212133]]</code></pre>
</div>
</div>
<div id="037380c6" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the fitted line, along with marks for the observations, intercept, and slope</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>x_line <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>y_line <span class="op">=</span> intercept <span class="op">+</span> slope<span class="op">*</span>x_line</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>ax.plot(x_line, y_line, color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Fitted line'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>ax.scatter(x, y, label<span class="op">=</span><span class="st">'Observations'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>ax.scatter(<span class="dv">0</span>, intercept, marker<span class="op">=</span><span class="st">'x'</span>, color<span class="op">=</span><span class="st">'green'</span>, label<span class="op">=</span><span class="st">'Intercept'</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>ax.plot(x, [y[<span class="dv">1</span>], y[<span class="dv">1</span>]], color<span class="op">=</span><span class="st">'purple'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'dx'</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>ax.plot([x[<span class="dv">0</span>], x[<span class="dv">0</span>]], y, color<span class="op">=</span><span class="st">'orange'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'dy'</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>ax.grid()</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Two data points with fitted line'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>Text(0.5, 1.0, 'Two data points with fitted line')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-5-output-2.png" width="587" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="fitting-a-line-to-more-than-two-observations" class="level3">
<h3 class="anchored" data-anchor-id="fitting-a-line-to-more-than-two-observations">Fitting a line to more than two observations</h3>
<p>Hopefully, you will be working with more than just two data points. So now we have to think about solving for the best fit line that passes through many data points. With multiple data points we cannot use the same algebraic approach as above. Instead, we create an equation that captures the difference between the observed data points and a proposed line, then try to derive an equation that tells us what to set the <span class="math inline">\boldsymbol{\beta}</span> coefficients to so that its value is minimized. To start, letâ€™s first create some fake data and plot it.</p>
<div id="7be60ac7" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create fake set of observations with known slope and intercept</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> rs.randn(<span class="dv">100</span>,<span class="dv">1</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>x <span class="op">+</span> rs.randn(<span class="dv">100</span>,<span class="dv">1</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot those observations</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>ax.scatter(x,y)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>ax.grid()</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">7</span>, <span class="dv">7</span>])</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'100 data points'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>Text(0.5, 1.0, '100 data points')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-6-output-2.png" width="587" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>To create this data, we first specified a line with a known intercept and slope (<span class="math inline">\boldsymbol{\beta}_0=1</span>, <span class="math inline">\boldsymbol{\beta}_1=2</span>). We then added some random noise to the data to capture variability in the process we are observing or error in our measurements. Because of this error, we will not be able to perfectly fit a line to the data. Instead, we will try to find a line that best fits the observed data. Put another way, one that minimizes the error between it and the observed data. This error is known as the <em>residual</em> and is calculated as the difference between the observed data and the predicted data from the line. The residual for observation <span class="math inline">i</span> is calculated as <span class="math inline">\boldsymbol{y}_i - (\boldsymbol{\beta}_0 + \boldsymbol{\beta}_1\boldsymbol{x}_i)</span>.</p>
<p>Moving forward, since the line will not perfectly match each data point, we will adjust our notation slightly. We will denote the observed data as <span class="math inline">\boldsymbol{y}</span> and the predicted data as <span class="math inline">\boldsymbol{\hat{y}}</span> (read as â€˜y-hatâ€™). The predicted data is calculated as <span class="math inline">\boldsymbol{\hat{y}} = \boldsymbol{\beta}_0 + \boldsymbol{\beta}_1\boldsymbol{x}</span>.</p>
</section>
<section id="mean-squared-error" class="level3">
<h3 class="anchored" data-anchor-id="mean-squared-error">Mean squared error</h3>
<p>How should we measure the error between the observed data and the line? Whatever measure we choose, it should be a single number that increases as the observations fall farther away from the line. Since an observation can be both <em>above</em> and <em>below</em> the line, we can take the difference between the observed value of <span class="math inline">\boldsymbol{y}</span> and its predicted value, <span class="math inline">\hat{\boldsymbol{y}}</span>, and square it. This will ensure that the error is always positive. We can then take the mean of the squared errors across all observations to get a single overall error. We will call this the <em>mean squared error</em> (MSE), which is expressed mathematically as:</p>
<p><span class="math display">
\begin{align}
MSE &amp;= \frac{1}{N}\sum_{i=1}^{N}(\boldsymbol{y}_i - (\boldsymbol{\beta}_0 + \boldsymbol{\beta}_1\boldsymbol{x}_i))^2 \notag \\
&amp;= \frac{1}{N}\sum_{i=1}^{N}(\boldsymbol{y}_i - \boldsymbol{\hat{y}}_i)^2 \notag \\
\end{align}
</span></p>
<p>Letâ€™s create a function that calculates the mean squared error between the predicted line and data.</p>
<div id="c9302f8a" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define MSE function</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> MSE(obs, pred):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((obs <span class="op">-</span> pred)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pred_line(x, beta):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta[<span class="dv">0</span>] <span class="op">+</span> beta[<span class="dv">1</span>]<span class="op">*</span>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now letâ€™s try a few different prospective lines and see how well they fit the data.</p>
<div id="6be987dc" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># beta coefficients for three different prospective lines</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>beta1 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>beta2 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>beta3 <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">3</span>, <span class="dv">1</span>])</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot each of the lines, along with the observations, and indicate the MSE</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">9</span>,<span class="dv">3</span>))</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, beta <span class="kw">in</span> <span class="bu">enumerate</span>([beta1, beta2, beta3]):</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> pred_line(x, beta)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    y_line <span class="op">=</span> pred_line(np.array([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>]), beta)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    ax[i].plot([<span class="op">-</span><span class="dv">7</span>, <span class="dv">7</span>], y_line, color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Proposed line'</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    ax[i].scatter(x, y, label<span class="op">=</span><span class="st">'Observations'</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    ax[i].grid()</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    ax[i].set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    ax[i].set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    ax[i].set_xlim([<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>])</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    ax[i].set_ylim([<span class="op">-</span><span class="dv">7</span>, <span class="dv">7</span>])</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    ax[i].set_title(<span class="st">'MSE: </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(MSE(y, y_pred)))</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-8-output-1.png" width="747" height="302" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>None of the lines we randomly created are good fits to the data. The first one passes through the data and is the most closely aligned to its slope, but it is still not a good fit. The second line passes through but its slope is going in the opposite direction. And the third line is even worse, and hardly passes through our observations.</p>
<p>Perhaps we should try to be methodical about finding the best fit line. One way to do that is to systematically vary the intercept and slope of the best fit line and calculate the mean squared error for each combination. We can then find the combination of intercept and slope that minimizes the mean squared error. If we do this as a <em>grid search</em>, stepping through each combination, we can create a plot of how the MSE varies with each combination. This allows us to visualize the error surface and identify its minimum. Letâ€™s do this.</p>
<div id="e274f049" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a grid of intercept and slope values</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>intercepts <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>slopes <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>intercepts, slopes <span class="op">=</span> np.meshgrid(intercepts, slopes)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the MSE for each combination of intercept and slope</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>MSEs <span class="op">=</span> np.zeros(intercepts.shape)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(intercepts.shape[<span class="dv">0</span>]):</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(intercepts.shape[<span class="dv">1</span>]):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        beta <span class="op">=</span> np.array([intercepts[i,j], slopes[i,j]])</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        MSEs[i,j] <span class="op">=</span> MSE(y, pred_line(x, beta))</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the MSE surface</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>fig,ax <span class="op">=</span> plt.subplots()</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>ax.contourf(intercepts, slopes, MSEs, levels<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>CS <span class="op">=</span> ax.contour(intercepts, slopes, MSEs, levels<span class="op">=</span><span class="dv">10</span>, colors<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>ax.scatter(<span class="dv">1</span>, <span class="dv">2</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>ax.clabel(CS, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Intercept'</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Slope'</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'MSE surface'</span>)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="st">'equal'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-9-output-1.png" width="436" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The above contour plot shows us the error surface for the mean squared error. The x-axis represents the intercept and the y-axis represents the slope. The color of the surface represents the error, with blue being lowest error and yellow the highest. The red dot is the intercept and slope we used to create the data. As you can see, the error surface has a valley centered on the true intercept and slope. This is where the error is minimized, which is why we want to find the minimum of this surface.</p>
<p>Note that the error surface has a bowl-like shape. This is because the error is a quadratic function of the intercept and slope. This is a nice property because it means that the error surface has a single minimum (a <em>global minimum</em>). This is not always the case with other models, which can have multiple minima that are higher in elevation (<em>local minima</em>). This is one reason why linear models are so popular, because they are easy to optimize. Error surfaces with such a shape are known as <em>convex</em>. Convexity is a desirable property in optimization because it means that there is a single minimum that can be found no matter where start, simply by following the slope of the surface downhill. For most machine learning model this is often done iteratively, by taking small steps in the direction of the slope, which is known as <em>gradient descent</em>. Since our curve is analytically simple, we can use a more direct method, and precisely solve for the minimum.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Helpful mathematical rules
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>For derivatives:</strong></p>
<p>1. Constant rule: <span class="math inline">\frac{d}{dx}c = 0</span></p>
<p>2. Addition rule: <span class="math inline">\frac{d}{dx}(f(x) + g(x)) = \frac{d}{dx}f(x) + \frac{d}{dx}g(x)</span></p>
<p>3. Power rule: <span class="math inline">\frac{d}{dx}x^n = nx^{n-1}</span></p>
<p>4. Chain rule: <span class="math inline">\frac{d}{dx}f(g(x)) = f'(g(x))g'(x)</span></p>
<p><strong>Sums:</strong></p>
<p>1. Mean: <span class="math inline">\bar{x} = \frac{1}{N}\sum_{i=1}^{N}x_i</span></p>
<p>2. Multiplication by N: <span class="math inline">N\bar{x} = \sum_{i=1}^{N}x_i</span></p>
<p>3. Associativity: <span class="math inline">\sum_{i=1}^{N}(a_i + b_i) = \sum_{i=1}^{N}a_i + \sum_{i=1}^{N}b_i</span></p>
<p>4. Distributivity: <span class="math inline">\sum_{i=1}^{N}a(b_i) = a\sum_{i=1}^{N}b_i</span></p>
</div>
</div>
</section>
<section id="error-minimization-with-ordinary-least-squares" class="level3">
<h3 class="anchored" data-anchor-id="error-minimization-with-ordinary-least-squares">Error minimization with ordinary least squares</h3>
<p>Above we found the minimum using a grid search, stepping through many combinations of intercepts and slopes. This is an inefficient way to find the minimum. Moreover, if our minimum is outside the range of the grid, when we will not stumble upon it. Alternatively, we could use gradient descent and choose a random point on the error landscape from which to coast down to the minimum. To do this we have to the gradient of the surface at our starting point, move a little bit downhill along that gradient, and repeating the process at each new point until we reach the bottom. This too is computationally expensive. Instead, we should try to find the minimum using a method that directly calculates it. One such method is <em>ordinary least squares</em> (OLS). The starting point of this approach is recognizing that the error surface almost always has only one minimum and at that point the gradient is zero. This is because the error surface of the MSE when fitting a line is almost always a parabola, with a single minimum point at the location where the intercept and slope create a best fit line for the data. All other combinations of intercept and slope will have higher error. We can solve for where that point is by taking the derivative of the MSE with respect to the intercept and the slope, and finding where those derivatives are equal to zero.</p>
<p>We can use some calculus and algebra to solve for the intercept and slope that minimize the mean squared error. Letâ€™s start with the intercept:</p>
<p><span class="math display">
\begin{align}
\frac{\partial MSE}{\partial \boldsymbol{\beta}_0} &amp;= \frac{1}{N}\sum_{i=1}^{N}(\boldsymbol{y}_i - \boldsymbol{\hat{y}_i})^2 \frac{d}{d\boldsymbol{\beta}_0}\notag \\
&amp;= \frac{1}{N}\sum(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i)^2 \frac{d}{d\boldsymbol{\beta}_0}\notag \\
&amp;= \frac{1}{N}\sum 2(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i)(-1) \notag \\
&amp;= -2\frac{1}{N}\sum(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i) \notag \\
\end{align}
</span></p>
<p>Now we set the derivative equal to zero and solve for <span class="math inline">\boldsymbol{\beta}_0</span>:</p>
<p><span class="math display">
\begin{align}
0 &amp;= -2\frac{1}{N}\sum(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i) \notag \\
&amp;= \sum(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i) \notag \\
&amp;= \sum\boldsymbol{y}_i - \sum\boldsymbol{\beta}_0 - \sum\boldsymbol{\beta}_1\boldsymbol{x}_i \notag \\
&amp;= \sum\boldsymbol{y}_i - N\boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\sum\boldsymbol{x}_i \notag \\
N\boldsymbol{\beta}_0 &amp;= \sum\boldsymbol{y}_i - \boldsymbol{\beta}_1\sum\boldsymbol{x}_i \notag \\
\boldsymbol{\beta}_0 &amp;= \frac{1}{N}\sum\boldsymbol{y}_i - \boldsymbol{\beta}_1\frac{1}{N}\sum\boldsymbol{x}_i \notag \\
\boldsymbol{\beta}_0 &amp;= \bar{\boldsymbol{y}} - \boldsymbol{\beta}_1\bar{\boldsymbol{x}} \notag \\
\end{align}
</span></p>
<p>Notice how the equation for the intercept resembles the one we found for just two observations (<span class="math inline">\boldsymbol{\beta}_0 = \boldsymbol{y} - \boldsymbol{\beta}_1\boldsymbol{x}</span>). The only difference is that we are now using the mean of the observations, <span class="math inline">\bar{\boldsymbol{y}}</span> and <span class="math inline">\bar{\boldsymbol{x}}</span> (read as â€˜x-barâ€™), instead of the variables representing <span class="math inline">x</span> and <span class="math inline">y</span>. Thought of another way, what this equation is saying is that if you start at the intercept and move in the direction of the slope towards the mean of the <span class="math inline">\boldsymbol{x}</span> values, you will end up at the mean of the <span class="math inline">\boldsymbol{y}</span> values. You can visualize this as so:</p>
<div id="951f37f4" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the observations with lines for the mean of X and mean of Y</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>ax.scatter(x, y)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>ax.scatter(<span class="dv">0</span>, <span class="dv">1</span>, marker<span class="op">=</span><span class="st">'x'</span>, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Intercept'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>ax.axvline(x.mean(), color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'$</span><span class="ch">\\</span><span class="st">boldsymbol{</span><span class="ch">\\</span><span class="st">bar</span><span class="sc">{x}</span><span class="st">}$'</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>ax.axhline(y.mean(), color<span class="op">=</span><span class="st">'purple'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="st">'$</span><span class="ch">\\</span><span class="st">boldsymbol{</span><span class="ch">\\</span><span class="st">bar</span><span class="sc">{y}</span><span class="st">}$'</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>ax.arrow(<span class="dv">0</span>, <span class="dv">1</span>, x.mean(), x.mean()<span class="op">*</span><span class="dv">2</span>, head_width<span class="op">=</span><span class="fl">0.15</span>, head_length<span class="op">=</span><span class="fl">0.15</span>, fc<span class="op">=</span><span class="st">'black'</span>, ec<span class="op">=</span><span class="st">'black'</span>, label<span class="op">=</span><span class="st">'Moving towards mean of Y'</span> )</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>ax.grid()</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>ax.set_xlim([<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>])</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>])</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Geometric interpretation of $</span><span class="ch">\\</span><span class="st">boldsymbol{</span><span class="ch">\\</span><span class="st">beta}_0=</span><span class="ch">\\</span><span class="st">boldsymbol{</span><span class="ch">\\</span><span class="st">bar</span><span class="sc">{y}</span><span class="st">} - </span><span class="ch">\\</span><span class="st">boldsymbol{</span><span class="ch">\\</span><span class="st">beta}_1</span><span class="ch">\\</span><span class="st">boldsymbol{</span><span class="ch">\\</span><span class="st">bar</span><span class="sc">{x}</span><span class="st">}$'</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-10-output-1.png" width="611" height="451" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice how the line emerging from the intercept and moving in the direction of the slope passes close to where the mean of the <span class="math inline">y</span> value is. Note also that it does not exactly line up. This is because we used the actual slope and intercept to generate this plot. Since noise was added to the data, the best fit line will be slightly different from the <span class="math inline">\beta</span> coefficients we used to generate the data. To get those values, we now need to solve for the slope, <span class="math inline">\boldsymbol{\beta}_1</span>.</p>
<p><span class="math display">
\begin{align}
\frac{\partial MSE}{\partial \boldsymbol{\beta}_1} &amp;= \frac{1}{N}\sum_{i=1}^{N}(\boldsymbol{y}_i - \boldsymbol{\hat{y}_i})^2 \frac{d}{d\boldsymbol{\beta}_1}\notag \\
&amp;= \frac{1}{N}\sum(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i)^2 \frac{d}{d\boldsymbol{\beta}_1}\notag \\
&amp;= \frac{1}{N}\sum 2(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i)(-\boldsymbol{x}_i) \notag \\
&amp;= -2\frac{1}{N}\sum(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i)\boldsymbol{x}_i \notag \\
\end{align}
</span></p>
<p>Now we set the derivative equal to zero and solve for <span class="math inline">\boldsymbol{\beta}_1</span>:</p>
<p><span class="math display">
\begin{align}
0 &amp;= -2\frac{1}{N}\sum(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i)\boldsymbol{x}_i \notag \\
&amp;= \sum(\boldsymbol{y}_i - \boldsymbol{\beta}_0 - \boldsymbol{\beta}_1\boldsymbol{x}_i)\boldsymbol{x}_i \notag \\
&amp;= \sum\boldsymbol{x}_i\boldsymbol{y}_i - \boldsymbol{\beta}_0\boldsymbol{x}_i-\boldsymbol{\beta}_1\boldsymbol{x}_i^2 \notag \\
&amp;= \sum\boldsymbol{x}_i\boldsymbol{y}_i - (\bar{\boldsymbol{y}}-\boldsymbol{\beta}_1\bar{\boldsymbol{x}})\boldsymbol{x}_i-\boldsymbol{\beta}_1\boldsymbol{x}_i^2 \notag \\
&amp;= \sum\boldsymbol{x}_i\boldsymbol{y}_i - \bar{\boldsymbol{y}}\boldsymbol{x}_i+\boldsymbol{\beta}_1\bar{\boldsymbol{x}}\boldsymbol{x}_i-\boldsymbol{\beta}_1\boldsymbol{x}_i^2 \notag \\
&amp;=  \sum\boldsymbol{x}_i\boldsymbol{y}_i - \sum\bar{\boldsymbol{y}}\boldsymbol{x}_i+ \sum\boldsymbol{\beta}_1\bar{\boldsymbol{x}}\boldsymbol{x}_i -\sum\boldsymbol{\beta}_1\boldsymbol{x}_i^2 \notag \\
&amp;= \sum\boldsymbol{x}_i\boldsymbol{y}_i - \bar{\boldsymbol{y}}\sum\boldsymbol{x}_i + \boldsymbol{\beta}_1\bar{\boldsymbol{x}}\sum\boldsymbol{x}_i - \boldsymbol{\beta}_1\sum\boldsymbol{x}_i^2 \notag \\
&amp;= \sum\boldsymbol{x}_i\boldsymbol{y}_i - N\bar{\boldsymbol{y}}\bar{\boldsymbol{x}} + \boldsymbol{\beta}_1N\bar{\boldsymbol{x}}^2 - \boldsymbol{\beta}_1\sum\boldsymbol{x}_i^2 \notag \\
&amp;= \sum\boldsymbol{x}_i\boldsymbol{y}_i - N\bar{\boldsymbol{y}}\bar{\boldsymbol{x}} + \boldsymbol{\beta}_1(N\bar{\boldsymbol{x}}^2-\sum\boldsymbol{x}_i^2) \notag \\
\boldsymbol{\beta}_1(-N\bar{\boldsymbol{x}}^2+\sum\boldsymbol{x}_i^2) &amp;= \sum\boldsymbol{x}_i\boldsymbol{y}_i - N\bar{\boldsymbol{y}}\bar{\boldsymbol{x}} \notag \\
\boldsymbol{\beta}_1 &amp;= \frac{\sum\boldsymbol{x}_i\boldsymbol{y}_i - N\bar{\boldsymbol{y}}\bar{\boldsymbol{x}}}{\sum\boldsymbol{x}_i^2-N\bar{\boldsymbol{x}}^2} \notag \\
\end{align}
</span></p>
<p>If you do a lot more tricky algebra, we can simplify this equation further to:</p>
<p><span class="math display">
\begin{align}
\boldsymbol{\beta}_1 &amp;= \frac{\sum(\boldsymbol{x}_i-\bar{\boldsymbol{x}})(\boldsymbol{y}_i-\bar{\boldsymbol{y}})}{\sum(\boldsymbol{x}_i-\bar{\boldsymbol{x}})^2} \notag \\
\end{align}
</span></p>
<p>How do we interpret this equation? The numerator captures the covariance between <span class="math inline">\boldsymbol{x}</span> and <span class="math inline">\boldsymbol{y}</span>. The denominator is the variance of <span class="math inline">\boldsymbol{x}</span>. (Note that because you are dividing the covariance by the variance, the <span class="math inline">1/N</span> term in each of those is canceled out). This means that the slope of the best fit line is the covariance between <span class="math inline">\boldsymbol{x}</span> and <span class="math inline">\boldsymbol{y}</span> divided by the variance of <span class="math inline">\boldsymbol{x}</span>. Put another way, the slope of the best fit line is a measure of how much <span class="math inline">\boldsymbol{y}</span> changes with respect to <span class="math inline">\boldsymbol{x}</span> for a one unit change in <span class="math inline">\boldsymbol{x}</span>. Again, notice the symmetry with our calculation for just two data points, where the slope was the change in the value of <span class="math inline">y</span> over the change in the value of <span class="math inline">x</span>.</p>
<p>Now that we have equations for the intercept and slope, we can create a function that calculates the beta coefficients of the best fit line.</p>
<div id="87842592" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># solves for best fit line with OLS, verbose version</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> OLS_single_verbose(x, y):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    beta1 <span class="op">=</span> np.<span class="bu">sum</span>((x <span class="op">-</span> x.mean())<span class="op">*</span>(y <span class="op">-</span> y.mean()))<span class="op">/</span>np.<span class="bu">sum</span>((x <span class="op">-</span> x.mean())<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    beta0 <span class="op">=</span> y.mean() <span class="op">-</span> beta1<span class="op">*</span>x.mean()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta0, beta1</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># solves for best fit line with OLS, concise version</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> OLS_single(x,y):</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    beta1 <span class="op">=</span> np.cov(x.T, y.T, bias<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">/</span>np.var(x) <span class="co"># note bias=True, which divides by n instead of n-1</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    beta0 <span class="op">=</span> y.mean() <span class="op">-</span> beta1<span class="op">*</span>x.mean()</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta0, beta1</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># solve for the best fit line</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The beta coefficients are: intercept=1, slope=2'</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>intercept, slope <span class="op">=</span> OLS_single_verbose(x, y)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Verbose version: intercept=</span><span class="sc">{}</span><span class="st">, slope=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(intercept, slope))</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>intercept, slope <span class="op">=</span> OLS_single(x, y)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Concise version: intercept=</span><span class="sc">{}</span><span class="st">, slope=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(intercept, slope))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The beta coefficients are: intercept=1, slope=2
Verbose version: intercept=1.0325148699057354, slope=1.9071403792001222
Concise version: intercept=1.0325148699057354, slope=1.9071403792001227</code></pre>
</div>
</div>
<p>Both versions came very close to the true values of intercept and slope. Surpisingly, they differ only slightly in the intercept despite receiving the exact same data. This likely arises from numerical precision issues in the calculations. To check if our hand coded functions are correct, we can use the built-in Numpy function <code>np.polyfit</code> to calculate the beta coefficients. <code>np.polyfit</code> is a Numpy function that takes two variables data and returns the coefficients of a best fit line. It is especially useful because it supports lines with with additional terms, such as quadratic (<code>deg=2</code>) or cubic (<code>deg=3</code>). Here we will just fit a straight line, so we will set <code>deg=1</code>.</p>
<div id="6b502313" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># solves for best fit line with OLS, builtin function version</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> OLS_single_builtin(x, y):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>   coefs <span class="op">=</span> np.polyfit(x.flatten(), y.flatten(), <span class="dv">1</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span> coefs[<span class="dv">1</span>], coefs[<span class="dv">0</span>]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># solve for the best fit line</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>intercept, slope <span class="op">=</span> OLS_single_builtin(x, y)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Numpy built-in function: intercept=</span><span class="sc">{}</span><span class="st">, slope=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(intercept, slope))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Numpy built-in function: intercept=1.0325148699057347, slope=1.9071403792001218</code></pre>
</div>
</div>
<p>The results are the same down to the 15th decimal place. This is a good sign that our functions are working correctly.</p>
</section>
</section>
<section id="multiple-regression-fitting-a-flat-plane" class="level2">
<h2 class="anchored" data-anchor-id="multiple-regression-fitting-a-flat-plane">Multiple regression: fitting a flat plane</h2>
<p>So far we have dealt with fitting a line to describe the relationship between <span class="math inline">y</span> and a single factor <span class="math inline">x</span>, known as single regression. Here two measurements were made for each observation (<span class="math inline">y</span> and <span class="math inline">x</span>). This is sufficient if we have one factor we are measuring or manipulating to affect the state of another variable. Often, however, there is more than one factor that affects the state of the dependent variable. In that case, known as <em>multiple regression</em>, we need to estimate how each factor affects the dependent variable. If we have two factors that are linearly affecting the dependent variable, we can fit a plane to the data. This is formulated as <span class="math inline">\boldsymbol{y}_i = \boldsymbol{\beta}_0 + \boldsymbol{\beta}_1\boldsymbol{x}_{1,i} + \boldsymbol{\beta}_2\boldsymbol{x}_{2,i}</span>. Here <span class="math inline">\boldsymbol{x}_1</span> and <span class="math inline">\boldsymbol{x}_2</span> are vectors of length <span class="math inline">N</span> that represent the two factors affecting the dependent variable. <span class="math inline">\boldsymbol{\beta}</span> will be a vector of coefficients that we solve for to describe the relationship between <span class="math inline">\boldsymbol{x}_1</span>, <span class="math inline">\boldsymbol{x}_2</span>, and <span class="math inline">\boldsymbol{y}</span>. This could be visualized as a plane in three dimensions, with the two factors on the x and y axes and the dependent variable on the z axis. For instance:</p>
<div id="402801ab" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create simulated surface</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>X1, X2 <span class="op">=</span> np.meshgrid(x1, x2)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>X1 <span class="op">+</span> <span class="dv">3</span><span class="op">*</span>X2</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the surface y as a function of x1 and x2</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>ax.plot_surface(X1, X2, Y, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'$x_1$'</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'$x_2$'</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>ax.set_zlabel(<span class="st">'y'</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Surface where $y = 1 + 2x_1 + 3x_2$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>Text(0.5, 0.92, 'Surface where $y = 1 + 2x_1 + 3x_2$')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-13-output-2.png" width="412" height="417" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Beyond two factors, you could have 3, or 4, or many many more. In this case, you are fitting a <em>hyperplane</em> to the data. This is expressed mathematically as:</p>
<p><span class="math display">
\begin{align}
\boldsymbol{y}_i &amp;= \boldsymbol{\beta}_0 + \boldsymbol{\beta}_1\boldsymbol{x}_{1,i} + \boldsymbol{\beta}_2\boldsymbol{x}_{2,i} + \boldsymbol{\beta}_3\boldsymbol{x}_{3,i} + ... + \boldsymbol{\beta}_p\boldsymbol{x}_{p,i} \notag \\
\boldsymbol{y}_i &amp;= \boldsymbol{\beta}_0 + \sum_{j=1}^{p}\boldsymbol{\beta}_j\boldsymbol{x}_{j,i} \notag \\
\end{align}
</span></p>
<p>This is referred to as a <em>General Linear Model</em> or just <em>Linear Model</em>. Many software packages abbreviate this as LM (the G is usually reserved for <strong>Generalized</strong> Linear models, which we will cover next lecture).</p>
<section id="the-matrix-version-of-ols" class="level3">
<h3 class="anchored" data-anchor-id="the-matrix-version-of-ols">The matrix version of OLS</h3>
<p>The least squares equation we developed for the fitting a line (single regression) can be generalized to this hyperplane case (multiple regression). To expedite this, we will switch to matrix notation. For this, the equation <span class="math inline">\boldsymbol{y}_i = \boldsymbol{\beta}_0 + \sum_{j=1}^{p}\boldsymbol{\beta}_j\boldsymbol{x}_{j,i}</span> can be rewritten as <span class="math inline">\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}</span>, where <span class="math inline">\boldsymbol{y}</span> is a vector of length <span class="math inline">N</span> representing the dependent variable, <span class="math inline">\boldsymbol{X}</span> is a matrix of size <span class="math inline">N \times (p+1)</span> representing the factors affecting the dependent variable (with the <span class="math inline">+1</span> for the intercept), and <span class="math inline">\boldsymbol{\beta}</span> is a vector of length <span class="math inline">p+1</span> representing the coefficients, one for the intercept and each factor. The matrix <span class="math inline">\boldsymbol{X}</span> is constructed by stacking the factor vectors <span class="math inline">\boldsymbol{x}_1</span>, <span class="math inline">\boldsymbol{x}_2</span>, â€¦, <span class="math inline">\boldsymbol{x}_p</span> as columns. The vector <span class="math inline">\boldsymbol{\beta}</span> is solved for by minimizing the mean squared error between the observed data and the predicted data from the hyperplane. This gives us the familiar equation we seek to minimize:</p>
<p><span class="math display">
\begin{align}
MSE &amp;= \frac{1}{N}\sum_{i=1}^N(\boldsymbol{y}_i - \boldsymbol{X}_{i,}\boldsymbol{\beta})^2 \notag \\
\end{align}
</span></p>
<p>Just as before, we need to find the <span class="math inline">\boldsymbol{\beta}</span> that minimizes the mean squared error. This can be done by taking the derivative of the MSE with respect to <span class="math inline">\boldsymbol{\beta}</span> and setting it equal to zero. This will give us the equation for the best fit coefficients. The first step is to realize that <span class="math inline">\sum\boldsymbol{a}^2 = \boldsymbol{a}^T\boldsymbol{a}</span>, where <span class="math inline">\boldsymbol{a}</span> is a vector. This allows us to rewrite the MSE as:</p>
<p><span class="math display">
\begin{align}
MSE &amp;= \frac{1}{N}(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}) \notag \\
&amp;= \frac{1}{N}(\boldsymbol{y}^T - \boldsymbol{\beta}^T\boldsymbol{X}^T)(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}) \notag \\
&amp;= \frac{1}{N}(\boldsymbol{y}^T\boldsymbol{y} - \boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{y} + \boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}) \notag \\
&amp;= \frac{1}{N}(\boldsymbol{y}^T\boldsymbol{y} - 2\boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{y} + \boldsymbol{\beta}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}) \notag \\
\end{align}
</span></p>
<p>From here, we can solve for the derivative of the MSE with respect to <span class="math inline">\boldsymbol{\beta}</span>.</p>
<p><span class="math display">
\frac{\partial{MSE}}{\partial{\boldsymbol{\beta}}} = \frac{1}{N}(-2\boldsymbol{X}^T\boldsymbol{y} +2\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}) \notag \\
</span></p>
<p>And just as the case with single regression, we can set this derivative equal to zero and solve for <span class="math inline">\boldsymbol{\beta}</span>.</p>
<p><span class="math display">
\begin{align}
0&amp;= \frac{1}{N}(-2\boldsymbol{X}^T\boldsymbol{y} +2\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta}) \notag \\
0&amp;=-\boldsymbol{X}^T\boldsymbol{y} +\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta} \notag \\
-\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta} &amp;= -\boldsymbol{X}^T\boldsymbol{y}  \notag \\
\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\beta} &amp;= \boldsymbol{X}^T\boldsymbol{y} \notag \\
\boldsymbol{\beta} &amp;= \frac{\boldsymbol{X}^T\boldsymbol{y}}{(\boldsymbol{X}^T\boldsymbol{X})} \notag \\
\end{align}
</span></p>
<p>Viola, there it is. Notice how it is similar to the single regression case. The denominator is a matrix of the sum of squares for each factor (along the diagonal) and the cross sum of squares between factors (off diagonal elements). This captures the scale of each independent variable and how they are covarying with one another. The numerator is a vector that captures the covariation between each independent variable and the dependent variable. This is a generalization of the slope in the single regression case. The form I have written it in above underscores the symmetry between these two equations, but it is more commonly written as: <span class="math inline">\boldsymbol{\beta} = (\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}</span>.</p>
<p>You might be wondering why we are not also solving for the intercept. This is because the intercept is already included in the matrix <span class="math inline">\boldsymbol{X}</span> as a column of ones. This is why the matrix <span class="math inline">\boldsymbol{X}</span> is of size <span class="math inline">N \times (p+1)</span>, with the <span class="math inline">+1</span> for the intercept. This allows us to solve for the intercept and the coefficients of the factors in a single step. Thus, when solving for <span class="math inline">\boldsymbol{\beta}</span>, you should make sure to include the intercept in the matrix <span class="math inline">\boldsymbol{X}</span> as a column of 1s.</p>
<p>We can now write our own function to calculate the beta coefficients of a multiple regression problem. Letâ€™s give that a try.</p>
<div id="8aa5a756" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># solves for best fit hyperplane with OLS</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> OLS_multiple_verbose(X, y):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># preallocate space for xtx and xty</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    dep_num <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    xtx <span class="op">=</span> np.zeros((dep_num, dep_num))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    xty <span class="op">=</span> np.zeros((dep_num, <span class="dv">1</span>))</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the denominator, xtx</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dep_num):</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(dep_num):</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            xtx[i,j] <span class="op">=</span> np.<span class="bu">sum</span>(X[:,i]<span class="op">*</span>X[:,j])</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calculate the numerator, xty</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(dep_num):</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        xty[i] <span class="op">=</span> np.<span class="bu">sum</span>(X[:,i]<span class="op">*</span>y)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># solve for beta</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.linalg.inv(xtx) <span class="op">@</span> xty </span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co"># solves for best fit hyperplane with OLS, concise version</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> OLS_multiple(X, y):</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">@</span> X.T <span class="op">@</span> y</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> beta</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co"># create fake set of observations with known coefficients</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.concat((np.ones((<span class="dv">100</span>,<span class="dv">1</span>)), rs.randn(<span class="dv">100</span>,<span class="dv">3</span>)), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>beta_true <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X <span class="op">@</span> beta_true <span class="op">+</span> rs.randn(<span class="dv">100</span>)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="co"># solve for the best fit hyperplane</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> OLS_multiple_verbose(X, y)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The true beta coefficients are: '</span>, beta_true)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Verbose version: '</span>, beta.flatten())</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> OLS_multiple(X, y)</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Concise version: '</span>, beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The true beta coefficients are:  [1 2 3 4]
Verbose version:  [1.02546624 2.07215648 2.87082363 3.93774308]
Concise version:  [1.02546624 2.07215648 2.87082363 3.93774308]</code></pre>
</div>
</div>
<p>Numpy also has a built-in function for solving for the beta coefficients of a multiple regression problem. This function is <code>np.linalg.lstsq</code>. It takes the matrix of factors and the dependent variable and returns the beta coefficients. Letâ€™s use this function to check if our hand coded function is correct.</p>
<div id="b87be5cd" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> np.linalg.lstsq(X, y)[<span class="dv">0</span>]</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Numpy built-in function: '</span>, beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Numpy built-in function:  [1.02546624 2.07215648 2.87082363 3.93774308]</code></pre>
</div>
</div>
<p>Exactly the same! Sometimes (but not always!) math is math no matter how you code it.</p>
</section>
<section id="multiple-regression-with-the-statsmodels-package" class="level3">
<h3 class="anchored" data-anchor-id="multiple-regression-with-the-statsmodels-package">Multiple regression with the <code>statsmodels</code> package</h3>
<p>So far we have used Numpy to solve our regression problems. This is fine if we are just solving for the beta coefficients. However, if we want to do more advanced statistical analysis, such as hypothesis testing, confidence intervals, or model comparison, we need to use a more advanced package. One such package is <code>statsmodels</code>. This package is built on top of <code>Numpy</code> and <code>Scipy</code> and provides a more comprehensive set of tools for statistical analysis. Letâ€™s use this package to solve for the beta coefficients of a multiple regression problem.</p>
<p>In the <code>statsmodels</code> package there is a function, <code>OLS</code>, that takes the dependent variable (<span class="math inline">\boldsymbol{y}</span>) and the matrix of independent variables/factors (<span class="math inline">\boldsymbol{X}</span>) and returns a <code>RegressionResults</code> model object. In the lingo of the <code>statsmodels</code> package, the dependent variable is referred to as the â€˜endogenousâ€™ variable, while the independent variable is the â€˜exogenousâ€™ variable. Once the <code>OLS</code> model object is created, you fit the beta coefficients by calling its <code>fit</code> method. This returns a results object. The results object contains a lot of information about the model. The beta coefficients are stored in the <code>params</code> attribute.</p>
<div id="8604f308" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create an OLS model using statsmodels</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>ols_mdl <span class="op">=</span> sm.OLS(y, X)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the beta coefficients</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> ols_mdl.fit()</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># return the beta coefficients</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Statsmodels beta coefficients: '</span>, results.params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Statsmodels beta coefficients:  [1.02546624 2.07215648 2.87082363 3.93774308]</code></pre>
</div>
</div>
<p>Again, we find that the fitted beta coefficients match those we obtained with our home brew versions.</p>
</section>
</section>
<section id="constructing-a-design-matrix" class="level2">
<h2 class="anchored" data-anchor-id="constructing-a-design-matrix">Constructing a design matrix</h2>
<section id="continuous-factor" class="level3">
<h3 class="anchored" data-anchor-id="continuous-factor">Continuous factor</h3>
<p>Now that we can fit a model with a simple function, we will explore how to design our <span class="math inline">\boldsymbol{X}</span> matrix to model different kinds of experimental situations. Recall that the <span class="math inline">\boldsymbol{X}</span> matrix is a matrix of size <span class="math inline">N \times (p+1)</span>, where <span class="math inline">N</span> is the number of observations and <span class="math inline">p</span> is the number of factors. The first column of the matrix is a column of ones, which represents the intercept. The intercept can be thought of as capturing the mean value of the dependent variable when all the factors are zero. The remaining columns are the factors that affect the dependent variable. In the example data we generated above, the factors were continuous variables. That means they were real numbers that could take on any value. For single regression there was only one factor, while for multiple regression we included three factors. Something I find helpful when thinking about regression problems is to visualize the design matrix. This can help you understand how the factors are affecting the dependent variable. Letâ€™s visualize the design matrix for some of the data we generated above.</p>
<div id="634bd736" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_dmat(df, num_rows<span class="op">=</span><span class="dv">10</span>, ax<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> df.values</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    varnames <span class="op">=</span> df.columns</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    max_abs <span class="op">=</span> np.<span class="bu">max</span>(np.<span class="bu">abs</span>(X[:num_rows]))</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    axh <span class="op">=</span> ax.imshow(X[:num_rows], cmap<span class="op">=</span><span class="st">'bwr'</span>, norm<span class="op">=</span>plt.Normalize(vmin<span class="op">=-</span>max_abs, vmax<span class="op">=</span>max_abs))</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Dependent variable'</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Observations'</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="st">'Design matrix for the first 10 observations'</span>)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> varnames <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        ax.set_xticks(np.arange(X.shape[<span class="dv">1</span>]))</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        ax.set_xticklabels(varnames)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        ax.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    ax.figure.colorbar(axh)</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a><span class="co"># turn our design matrix into a pandas dataframe, which will allow us to label the columns</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>dmat <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span>[<span class="st">'Intercept'</span>, <span class="st">'F1'</span>, <span class="st">'F2'</span>, <span class="st">'F3'</span>])</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>plot_dmat(dmat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-17-output-1.png" width="360" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Each row is an observation and each column a factor. The first column is the intercept and all the entries in it have the value of 1. The subsequent columns are the continuously valued independent variables we generated randomly. You can see the range of blue to red is continuous in the last three columns. I find that a color map that sets 0 to white and had all positive values as red and negative values as blue helps see the relationships between the factors.</p>
</section>
<section id="ordinal-categorical" class="level3">
<h3 class="anchored" data-anchor-id="ordinal-categorical">Ordinal categorical</h3>
<p>But your independent variables do not need to be continuous. Instead they could be categorical. Categorical variables take on a limited set of values, denoting either states or values (often integers). For instance, a categorical variable could be the amount of reward payout given in a task, with values ranging from 0 to 3. When a categorical variable is composed of numerically ordered items, such as amount of reward, we refer to it as <em>ordinal</em>. We could simulate a categorical variable for reward payout by adding another column to our current design matrix that randomly chooses from those values for each observation.</p>
<div id="c688b9da" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add reward categorical column representing reward payout</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>reward_cat <span class="op">=</span> rs.choice([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>], size<span class="op">=</span>(<span class="dv">100</span>,<span class="dv">1</span>)) <span class="co"># create a categorical variable</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>dmat[<span class="st">'Rew_Cat'</span>] <span class="op">=</span> reward_cat</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>plot_dmat(dmat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-18-output-1.png" width="360" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>With this plot it is obvious that column 4 is a categorical variable. The values are discrete and there is no continuous gradient between them. Moreover, all of its values are positive, since no blue is visible.</p>
</section>
<section id="binarydummy-categorical" class="level3">
<h3 class="anchored" data-anchor-id="binarydummy-categorical">Binary/dummy categorical</h3>
<p>Specifying a categorical variable as an integer is fine if we think that the relationship between the dependent variable and the categorical independent variable will be monotonic (they increase or decrease together). But this is not always the case. We could imagine the case where two symbols are displayed during reward payout. Either, neither, or both could be displayed on any trial. For that, we need to create <em>dummy variables</em>. A dummy variable is a categorical binary variable that represents the presence or absence of an event. We create a dummy variable for each symbol that is 1 if the symbol is present and 0 if absent. This allows us to capture the effect of each symbol on the dependent variable. Letâ€™s add a dummy variable to our design matrix.</p>
<div id="6115f7b5" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add two dummy variables to the design matrix</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>dummies <span class="op">=</span> rs.choice([<span class="dv">0</span>,<span class="dv">1</span>], size<span class="op">=</span>(<span class="dv">100</span>,<span class="dv">2</span>))</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>dmat[[<span class="st">'Dummy1'</span>, <span class="st">'Dummy2'</span>]] <span class="op">=</span> dummies</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>plot_dmat(dmat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-19-output-1.png" width="392" height="498" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The two new columns on the right side are the dummy variables, whose values are restricted to 0 or 1. Note that on any given trial either the first or second can have the value 1. They are independent of each other.</p>
</section>
<section id="one-hot-categorical" class="level3">
<h3 class="anchored" data-anchor-id="one-hot-categorical">One-hot categorical</h3>
<p>What if we wanted to encode the reward payout as a dummy variable? This would allows us to model it as a categorical variable that has a nonlinear relationship with the dependent variable. We would want a dummy variable for each possible value of the reward payout. For that we would use <em>one-hot encoding</em>. One-hot encoding is a method of converting a categorical variable into a binary matrix. Each column of the matrix represents a possible value of the categorical variable. The column corresponding to the value of the categorical variable is 1, while all other columns are 0. Note that the number of columns we create is one less than the number of conditions. This is to avoid the problem of <em>multicollinearity</em> between the intercept column and our one-hot encoding columns. We will explain this further later. For now, letâ€™s add a one-hot encoding of the reward payout to our design matrix.</p>
<div id="4834cd1d" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># one hot encode the reward payout variable</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>payout_data <span class="op">=</span> dmat[<span class="st">'Rew_Cat'</span>].values</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>payout_onehot <span class="op">=</span> np.zeros((<span class="dv">100</span>, <span class="dv">3</span>))</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    payout_onehot[:,i] <span class="op">=</span> payout_data <span class="op">==</span> i<span class="op">+</span><span class="dv">1</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>dmat[[<span class="st">'Rew_OH1'</span>, <span class="st">'Rew_OH2'</span>, <span class="st">'Rew_OH3'</span>]] <span class="op">=</span> payout_onehot</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>plot_dmat(dmat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-20-output-1.png" width="499" height="500" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here we have added three columns to the design matrix, one for each possible payout (1, 2, and 3), excluding the 0 payout column. No trial can have more than one of these columns with a value of 1. This is because the reward payout can only take on one value at a time. If there is a non-monotonic relationship between the dependent variable and the reward payout, this design matrix will allow us to capture that relationship by giving a unique <span class="math inline">\beta</span> coefficient to each payout value. If the payout is zero, none of the dummy columns for payout will be 1. Instead, the intercept column will reflect that condition, or more precisely the condition where the payout was not 1, 2, or 3. You can think of each of the one-hot encoded dummy variables are representing the change in the dependent variable by presence of that payout value relative to no payout. It is important to keep in mind that you do not have to choose the 0 payout condition. We could have chosen the 1 payout condition as the reference and then the intercept would represent the condition where the payout was not 1. This is a choice you have to make when designing your model. It is often just conceptually easier to go with the 0 condition.</p>
</section>
<section id="interaction-term" class="level3">
<h3 class="anchored" data-anchor-id="interaction-term">Interaction term</h3>
<p>The last kind of variable we consider is an interaction term. This occurs when the effect of one factor on the dependent variable depends on the value of another factor. For instance, the effect of Factor 1 on the dependent variable could depend on the symbols presented. We could model this by adding a columns to the design matrix that are the product of the reward payout and each of the symbols. This would allow us to capture the interaction between Factor 1 and the symbols. Letâ€™s add an interaction term to our design matrix.</p>
<div id="4729c2e8" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create the interaction terms between the categorical reward and the dummy coded symbols</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>int_terms <span class="op">=</span> dmat[<span class="st">'F1'</span>].values[:,<span class="va">None</span>]<span class="op">*</span>dmat[[<span class="st">'Dummy1'</span>, <span class="st">'Dummy2'</span>]].values</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>dmat[[<span class="st">'Int1'</span>, <span class="st">'Int2'</span>]] <span class="op">=</span> int_terms</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>plot_dmat(dmat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-21-output-1.png" width="546" height="478" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The interaction terms here capture how the dependence of Factor 1 changes with the presence of each symbol. For instance, the <span class="math inline">\beta</span> coefficient associated with â€˜Int1â€™ tells the model how much to change the slope of the relationship between Factor 1 and the dependent variable, given that Dummy 1 was true.</p>
</section>
</section>
<section id="evaluating-a-linear-model" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-a-linear-model">Evaluating a linear model</h2>
<section id="design-matrix-issues" class="level3">
<h3 class="anchored" data-anchor-id="design-matrix-issues">Design matrix issues</h3>
<p>Before we run our linear model, we have to ensure that it meets certain criteria. Perhaps most important of these is that the columns of the design matrix are linearly independent. This means that no column can be expressed as a linear combination of the other columns. By that, I mean that no column can be expressed as a sum of the other columns multiplied by some scalar. If the columns are linearly dependent, the matrix <span class="math inline">\boldsymbol{X}^T\boldsymbol{X}</span> will not be invertible and the <span class="math inline">\beta</span> coefficients cannot be solved for. One can imagine a situation where the design matrix has a column that is the sum of two other columns. This would mean that the effect of the two factors would be confounded and the model would not be able to distinguish between them. This is known as <em>multicollinearity</em>.</p>
<p>The design matrix we have created above has a multicollinearity issue. We have specified the reward payout as a categorical variable and as a one-hot encoding. This means that the sum of the one-hot encoding columns is equal to the categorical variable column (minus 1). If reward payout affects the dependent variable, it is ambiguous which of the columns, the categorical or the one-hot encoding, is responsible. To fix this we need to remove either set of columns. If we assume that the relationship between the dependent variable and the reward payout is nonlinear, we should remove the categorical variable column.</p>
<p>The issue of multicollinearity is also why when creating the one-hot encoding columns we drop one of the columns. This is to avoid the problem of multicollinearity between the intercept column and the one-hot encoding columns. If we did not drop one of the columns, the model would not be able to distinguish between the intercept and the one-hot encoding columns. This is because the sum of the one-hot encoding columns is equal to the intercept column.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Effect of multicollinearity on the error landscape
</div>
</div>
<div class="callout-body-container callout-body">
<p>The issue of multicollinearity can seem a bit abstract if you donâ€™t understand how matrix inverses are calculated (e.g.&nbsp;<a href="https://www.cmor-faculty.rice.edu/~zhang/caam335/F14/handouts/gaussian_elimination.pdf">gaussian elimination</a>). To get a geometric or visual intuition for the problem, consider how the error landscape is affected if two factors are highly correlated. In this case, the error landscape will be stretched along the direction of the correlation. This means that the error surface will be very flat in the direction of the correlation and very steep in the orthogonal direction. This can make it difficult for the optimization algorithm to find the minimum of the error surface. This is why multicollinearity can cause the optimization algorithm to fail to converge. We can visualize this by plotting the error surface for a design matrix with two highly correlated factors.</p>
<p>First we will create a couple functions to help us calculate and visualize the error surface.</p>
<div id="197e57b4" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate the loss landscape for a two factor linear model</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_landscape(X, y):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># perform a grid search of the parameters for the regression, just for the two factors</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    intercepts <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">101</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    factors <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">101</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    intercept, factor1, factor2 <span class="op">=</span> np.meshgrid(intercepts, factors, factors)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    MSEs <span class="op">=</span> np.zeros(intercept.shape)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(intercept.shape[<span class="dv">0</span>]):</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(intercept.shape[<span class="dv">1</span>]):</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(intercept.shape[<span class="dv">2</span>]):</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>                beta <span class="op">=</span> np.array([intercept[i,j,k], factor1[i,j,k], factor2[i,j,k]])</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>                MSEs[i,j,k] <span class="op">=</span> MSE(y, X <span class="op">@</span> beta)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> MSEs, intercept, factors</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the error surface for the </span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_mse_surface(factors, MSEs, ax<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    ax.contourf(factors, factors, MSEs[:,<span class="dv">50</span>,:], levels<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    CS <span class="op">=</span> ax.contour(factors, factors, MSEs[:,<span class="dv">50</span>,:], levels<span class="op">=</span><span class="dv">10</span>, colors<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>    ax.clabel(CS, fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Factor 2'</span>)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Factor 1'</span>)</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">'equal'</span>)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>    ax.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'white'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a>    ax.axvline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'white'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next we will create three design matrices. One will have two factors that are strongly positively correlated, the second with those factors lack any correlation, and the third with those factors are strongly negatively correlated.</p>
<div id="43c452c3" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># beta values for our models to examine multicollinearity (intercept, factor1, factor2)</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>beta_mc <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create data generated by a linear model positively correlated factors</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>Xp <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">3</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>Xp[:,<span class="dv">2</span>]<span class="op">=</span> Xp[:,<span class="dv">1</span>] <span class="op">+</span> np.random.randn(<span class="dv">100</span>)<span class="op">/</span><span class="dv">8</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>yp <span class="op">=</span> Xp <span class="op">@</span> beta_mc <span class="op">+</span> np.random.randn(<span class="dv">100</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The correlation between the positively correlated factors is </span><span class="sc">{:0.2f}</span><span class="st">'</span>.<span class="bu">format</span>(np.corrcoef(Xp.T)[<span class="dv">1</span>,<span class="dv">2</span>]))</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co"># create data generated by a linear model uncorrelated factors</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>Xu <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">3</span>)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>Xu[:,<span class="dv">2</span>]<span class="op">=</span> Xu[:,<span class="dv">2</span>] <span class="op">+</span> np.random.randn(<span class="dv">100</span>)<span class="op">/</span><span class="dv">8</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>Xu[:,<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>yu <span class="op">=</span> Xu <span class="op">@</span> beta_mc <span class="op">+</span> np.random.randn(<span class="dv">100</span>)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The correlation between the uncorrelated factors is </span><span class="sc">{:0.2f}</span><span class="st">'</span>.<span class="bu">format</span>(np.corrcoef(Xu.T)[<span class="dv">1</span>,<span class="dv">2</span>]))</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co"># create data generated by a linear model negatively correlated factors</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>Xn <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">3</span>)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>Xn[:,<span class="dv">2</span>]<span class="op">=</span> <span class="op">-</span>Xn[:,<span class="dv">1</span>] <span class="op">+</span> np.random.randn(<span class="dv">100</span>)<span class="op">/</span><span class="dv">8</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>Xn[:,<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>yn <span class="op">=</span> Xn <span class="op">@</span> beta_mc <span class="op">+</span> np.random.randn(<span class="dv">100</span>)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'The correlation between the negatively correlated factors is </span><span class="sc">{:0.2f}</span><span class="st">'</span>.<span class="bu">format</span>(np.corrcoef(Xn.T)[<span class="dv">1</span>,<span class="dv">2</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The correlation between the positively correlated factors is 0.99
The correlation between the uncorrelated factors is 0.04
The correlation between the negatively correlated factors is -0.99</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/drewheadley/anaconda3/envs/regress/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:3045: RuntimeWarning: invalid value encountered in divide
  c /= stddev[:, None]
/Users/drewheadley/anaconda3/envs/regress/lib/python3.10/site-packages/numpy/lib/_function_base_impl.py:3046: RuntimeWarning: invalid value encountered in divide
  c /= stddev[None, :]</code></pre>
</div>
</div>
<p>Now that we have our data, we will calculate the error landscape. In addition, to demonstrate how mulitcolinearity can cause instability in estimation of the beta coefficients, we will recalculate the betas for each design matrix multiple times, leaving out one observation each time.</p>
<div id="a2e8aea9" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get the MSE landscapes for all data</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>MSEsp, intercepts, factors <span class="op">=</span> mse_landscape(Xp, yp)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>MSEsu, _, _ <span class="op">=</span> mse_landscape(Xu, yu)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>MSEsn, _, _ <span class="op">=</span> mse_landscape(Xn, yn)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="co"># get the estimates of the beta coefficients, leaving one observation out each time</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>betasp <span class="op">=</span> np.stack([np.linalg.lstsq(Xp, yp<span class="op">+</span>rs.randn(<span class="dv">100</span>))[<span class="dv">0</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>)])</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>betasu <span class="op">=</span> np.stack([np.linalg.lstsq(Xu, yu<span class="op">+</span>rs.randn(<span class="dv">100</span>))[<span class="dv">0</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>)])</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>betasn <span class="op">=</span> np.stack([np.linalg.lstsq(Xn, yn<span class="op">+</span>rs.randn(<span class="dv">100</span>))[<span class="dv">0</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>)])</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the MSEs</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">3</span>))</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>plot_mse_surface(factors, MSEsp, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>plot_mse_surface(factors, MSEsu, ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>plot_mse_surface(factors, MSEsn, ax<span class="op">=</span>ax[<span class="dv">2</span>])</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the estimates of the beta coefficients</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(betasp[:,<span class="dv">2</span>], betasp[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:orange'</span>, marker<span class="op">=</span><span class="st">'+'</span>)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(betasu[:,<span class="dv">2</span>], betasu[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:orange'</span>, marker<span class="op">=</span><span class="st">'+'</span>)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].scatter(betasn[:,<span class="dv">2</span>], betasn[:,<span class="dv">1</span>], color<span class="op">=</span><span class="st">'tab:orange'</span>, marker<span class="op">=</span><span class="st">'+'</span>)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(beta_mc[<span class="dv">2</span>], beta_mc[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(beta_mc[<span class="dv">2</span>], beta_mc[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].scatter(beta_mc[<span class="dv">2</span>], beta_mc[<span class="dv">1</span>], color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">'Positively correlated factors'</span>)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">'Uncorrelated factors'</span>)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">2</span>].set_title(<span class="st">'Negatively correlated factors'</span>)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-24-output-1.png" width="779" height="251" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For the above graphs, the orange plus signs are the estimated <span class="math inline">\beta</span> coefficients and the red dot is the true <span class="math inline">\beta</span> coefficient.</p>
<p>Notice how the valley of the error surface is streched along a diagonal for the positive and negatively correlated cases. Both are offset from the origin of the graph by the true <span class="math inline">\beta</span> coefficients. In the positive correlation case, the diagonal has a negative slope. As the value of one of the <span class="math inline">\beta</span> for one of the factors increases, the value for the other decreases. This is because as you increase the <span class="math inline">\beta</span> for one factor, the other must be decreased since it has the same values. On the other hand, the negative case has a positive slope. As the value of one of the <span class="math inline">\beta</span> for one of the factors increases, the value for the other also increases. This is because as you increase the beta for one factor, the other must also be increased since they have opposite values. This is why the error surface is stretched along the diagonal.</p>
<p>You can also see that the variety of beta estimates when we added noise to <span class="math inline">\boldsymbol{y}</span> fall along the diagonal. This is due to the error surface being stretched along the diagonal. By comparison, when the factors are uncorrelated the scatter of <span class="math inline">\beta</span> estimates is tightly clustered around the true <span class="math inline">\beta</span> values.</p>
</div>
</div>
<p>Letâ€™s now simulate some data using our design matrix.</p>
<div id="27f4a206" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>mdl_spec <span class="op">=</span> [<span class="st">'Intercept'</span>, <span class="st">'F1'</span>, <span class="st">'F2'</span>, <span class="st">'F3'</span>, </span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>          <span class="st">'Dummy1'</span>, <span class="st">'Dummy2'</span>, </span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>          <span class="st">'Rew_OH1'</span>, <span class="st">'Rew_OH2'</span>, <span class="st">'Rew_OH3'</span>, </span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>          <span class="st">'Int1'</span>, <span class="st">'Int2'</span>]</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>beta_true <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, </span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>                      <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>,</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>                      <span class="dv">1</span>, <span class="dv">3</span>, <span class="op">-</span><span class="dv">1</span>,</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>                      <span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> dmat[mdl_spec].values</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X <span class="op">@</span> beta_true <span class="op">+</span> rs.randn(<span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have our design matrix and simulated <span class="math inline">\boldsymbol{y}</span>, we can fit a linear model to the data. We will use the <code>OLS</code> function in the <code>statsmodels</code> package to do this.</p>
<div id="38c3a1ed" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create linear model and specify the exogenous variable names</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>lm_mdl <span class="op">=</span> sm.OLS(y, X)</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>lm_mdl.exog_names[:] <span class="op">=</span> mdl_spec</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>lm_results <span class="op">=</span> lm_mdl.fit()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As we did above, letâ€™s compare the true <span class="math inline">\beta</span> coefficients with the <span class="math inline">\beta</span> coefficients we estimated with our model.</p>
<div id="f5d51bf1" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print true beta coefficiengs to 2 decimal places</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Factor name: True beta, Estimated beta'</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(beta_true.size):</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{}</span><span class="st">: </span><span class="sc">{:4.2f}</span><span class="st">, </span><span class="sc">{:4.2f}</span><span class="st">'</span>.<span class="bu">format</span>(mdl_spec[i], beta_true[i], lm_results.params[i]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Factor name: True beta, Estimated beta
Intercept: 1.00, 1.14
F1: 0.00, -0.43
F2: 1.00, 1.09
F3: 2.00, 1.89
Dummy1: -1.00, -0.89
Dummy2: 1.00, 1.18
Rew_OH1: 1.00, 0.88
Rew_OH2: 3.00, 2.57
Rew_OH3: -1.00, -1.07
Int1: 0.00, 0.29
Int2: 1.00, 1.24</code></pre>
</div>
</div>
<p>Note that they are all close to the true <span class="math inline">\beta</span> values. Since we had a 100 data points, and the noise we added was relatively small, we were able to recover the true <span class="math inline">\beta</span> coefficients with a high degree of accuracy. This is not always the case. If the noise is large or the number of observations is small, the <span class="math inline">\beta</span> coefficients will be estimated less accurately. For instance, if we reduce the number of observations to 15:</p>
<div id="32430f9e" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># restrict model fitting to just 15 observations</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>lm_mdl_sub <span class="op">=</span> sm.OLS(y[:<span class="dv">15</span>], X[:<span class="dv">15</span>])</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>lm_results_sub <span class="op">=</span> lm_mdl_sub.fit()</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Factor name: True beta, Estimated beta'</span>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(beta_true.size):</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{}</span><span class="st">: </span><span class="sc">{:4.2f}</span><span class="st">, </span><span class="sc">{:4.2f}</span><span class="st">'</span>.<span class="bu">format</span>(mdl_spec[i], beta_true[i], lm_results_sub.params[i]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Factor name: True beta, Estimated beta
Intercept: 1.00, 0.43
F1: 0.00, 1.00
F2: 1.00, 0.70
F3: 2.00, 2.42
Dummy1: -1.00, 0.17
Dummy2: 1.00, 1.20
Rew_OH1: 1.00, 2.20
Rew_OH2: 3.00, 1.58
Rew_OH3: -1.00, -0.31
Int1: 0.00, -0.21
Int2: 1.00, -0.35</code></pre>
</div>
</div>
<p>Now we are off by quite a bit more. To get a more concrete sense of how well our model is fitting the data, and the reliability of our <span class="math inline">\beta</span> coefficients, we should examine the summary table returned by the <code>results</code> object.</p>
</section>
<section id="summary-table-of-the-model" class="level3">
<h3 class="anchored" data-anchor-id="summary-table-of-the-model">Summary table of the model</h3>
<p>After fitting the linear model, we get a <code>results</code> object that has a <code>summary</code> method, which provides a comprehensive summary of the model. This includes the <span class="math inline">\beta</span> coefficients, the standard errors of the coefficients, the t-statistics, the p-values, and the confidence intervals. Here we will return it for the model fitted to all the data and examine each of its major fields in turn.</p>
<div id="7dac7191" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lm_results.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.875
Model:                            OLS   Adj. R-squared:                  0.861
Method:                 Least Squares   F-statistic:                     62.35
Date:                Tue, 04 Mar 2025   Prob (F-statistic):           7.58e-36
Time:                        21:03:39   Log-Likelihood:                -142.24
No. Observations:                 100   AIC:                             306.5
Df Residuals:                      89   BIC:                             335.1
Df Model:                          10                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.1426      0.293      3.901      0.000       0.561       1.725
F1            -0.4338      0.215     -2.017      0.047      -0.861      -0.006
F2             1.0883      0.117      9.307      0.000       0.856       1.321
F3             1.8867      0.124     15.159      0.000       1.639       2.134
Dummy1        -0.8930      0.248     -3.602      0.001      -1.386      -0.400
Dummy2         1.1790      0.236      4.985      0.000       0.709       1.649
Rew_OH1        0.8772      0.341      2.571      0.012       0.199       1.555
Rew_OH2        2.5657      0.320      8.006      0.000       1.929       3.202
Rew_OH3       -1.0690      0.305     -3.504      0.001      -1.675      -0.463
Int1           0.2889      0.226      1.279      0.204      -0.160       0.738
Int2           1.2386      0.220      5.618      0.000       0.801       1.677
==============================================================================
Omnibus:                        2.239   Durbin-Watson:                   2.220
Prob(Omnibus):                  0.326   Jarque-Bera (JB):                2.137
Skew:                           0.280   Prob(JB):                        0.344
Kurtosis:                       2.553   Cond. No.                         6.87
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
<section id="model-parameters-and-overall-fit" class="level4">
<h4 class="anchored" data-anchor-id="model-parameters-and-overall-fit">Model parameters and overall fit</h4>
<p>The top banner of the summary table tells us the model we fitted and its overall fit quality. You can see for â€˜Modelâ€™ we have â€˜OLSâ€™ and the â€˜Methodâ€™ is â€˜Least Squaresâ€™. Also present in that column is the number of observations (100) and degrees of freedom of the model (10, for 10 factors that were not the intercept). Most important when evaluating the overall fit of the model is the â€˜R-squaredâ€™ value. This is a measure of how much of the variance in the dependent variable is explained by the model. The R-squared values from 0, meaning none of the variance is explained, to 1, meaning all the variance is explained. In this case, the R-squared value is 0.883, meaning that 88.3% of the variance in the dependent variable is explained by the model. This is a good fit. Other more specialized versions of fit quality are returned below, such as Aikake Information Criterion (AIC) and Bayesian Information Criterion (BIC). These are used to compare the fit of different models to the data and control for the number of factors in the model.</p>
<p>Below that area is the table of coefficients. This table shows the <span class="math inline">\beta</span> coefficients (â€˜coefâ€™), their standard errors of the coefficients (â€˜std errâ€™), t-statistics (t), p-values (â€˜P&gt;|t|â€™), and confidence intervals (â€˜[0.025 0.975]â€™). The <span class="math inline">\beta</span> coefficients are the same as we calculated above. The standard errors are a measure of how much the <span class="math inline">\beta</span> coefficients vary from the true value. The t-statistics are a measure of how significant the <span class="math inline">\beta</span> coefficients are. The p-values are a measure of the probability that the <span class="math inline">\beta</span> coefficients are different from zero. The confidence intervals are a range of values that the <span class="math inline">\beta</span> coefficients are likely to fall within. Notice that the two factors whose true <span class="math inline">\beta</span> coefficients we set to 0, F1 and Int1, have very small <span class="math inline">\beta</span> coefficients that are not significantly different from 0. This is reflected in the p-values, which are very large. The confidence intervals also include 0. This is a good sign that the model is correctly identifying factors that do not affect the dependent variable.</p>
<p>The bottom field contains statistics that evaluate the statistical properties of the model. The â€˜Omnibusâ€™ test is a test of the normality of the residuals. The â€˜Durbin-Watsonâ€™ test is a test of the independence of the residuals between observations 1 sample apart (also known as autocorrelation). Generally, a value close to 2 suggest the residuals are independent. Ours is 1.759, which makes sense since we generated each observation randomly, so there shouldnâ€™t be any correlations between adjacent observations. The â€˜Jarque-Beraâ€™ test is a test of the skewness and kurtosis of the residuals to determine if they fit what would be expected for a normal distribution. Here itâ€™s p-value (â€˜Prob(JB)â€™) was 0.167, which is not significant. The â€˜Cond. No.â€™ (Condition Number test) is a test of multicollinearity. Explaining its calculation goes a bit beyond this lecture (it is the ratio between the largest and smallest singular values of the <span class="math inline">\boldsymbol{X}^T\boldsymbol{X}</span>). A value greater than 30 is indicative of multicollinearity. In our case, the value is 6.87, which is good.</p>
</section>
</section>
<section id="visualizing-the-model-fit" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-model-fit">Visualizing the model fit</h3>
<p>Another way to evaluate the model is to visualize the fit of the model to the data. This can be done by plotting the predicted values of the model against the observed values. If the model fit is good, the predicted values should be close to the observed values. Moreover, the variability in the predicted values should not depend on the observed values. This is known as <em>homoscedasticity</em>. If the variability in the predicted values depends on the observed values, this is called <em>heteroscedasticity</em>. Letâ€™s plot the predicted values of the model against the observed values.</p>
<div id="bd0fdf6c" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get predicted values from the model</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> lm_results.predict()</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the predicted values against the true values</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>ax.scatter(y, y_pred)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>ax.plot([<span class="op">-</span><span class="dv">8</span>, <span class="dv">8</span>], [<span class="op">-</span><span class="dv">8</span>, <span class="dv">8</span>], color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'True values'</span>)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Predicted values'</span>)</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Predicted vs. true values'</span>)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>ax.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-30-output-1.png" width="587" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Our predicted values fall along the equality line (dotted red), indicating that the predicted and observed values tend to be close to each other. This is a good sign that the model is fitting the data well. Indeed, the â€˜R-squaredâ€™ field in the summary table already told us this. It can be calculated by squaring the correlation coefficient of the the observed <code>y</code> (<span class="math inline">\boldsymbol{y}</span>) with the predicted <code>y_pred</code> (<span class="math inline">\boldsymbol{\hat{y}}</span>).</p>
<div id="d620619d" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>r_squared <span class="op">=</span> np.corrcoef(y, y_pred)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'R-squared: '</span>, r_squared[<span class="dv">0</span>,<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R-squared:  0.8750901027173</code></pre>
</div>
</div>
<p>As you can see, same exact value as reported in the summary table.</p>
<p>Moreover, the variability in the predicted values does not seem to depend on the observed values, indicating the model residuals are homoscedastic. To better visualize this, we can plot the residuals of the model against the predicted values.</p>
<div id="1167abe4" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>resid <span class="op">=</span> y <span class="op">-</span> y_pred</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the residuals against the predicted values</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>ax.scatter(y_pred, resid)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>ax.set_ylim([<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>])</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Predicted values'</span>)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Residuals vs. predicted values'</span>)</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>ax.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-32-output-1.png" width="587" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The residuals do not seem to vary systematically with the predicted values, reinforcing the idea that the residuals are homoscedastic. What might a similar plot look like if the residuals were not homoscedastic? We can simulate this by tweaking the model that generates the data so that one of our variables has a nonlinear relationship with the dependent variable. For this, we will make Factor 2 and 3 have a cubic (<span class="math inline">x^3</span>) relationship with the dependent variable.</p>
<div id="d1b64913" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>X_nonlin <span class="op">=</span> X.copy()</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>X_nonlin[:,<span class="dv">2</span>:<span class="dv">4</span>] <span class="op">=</span> X_nonlin[:,<span class="dv">2</span>:<span class="dv">4</span>]<span class="op">**</span><span class="dv">3</span> <span class="co"># create a non-linear relationship that will be hard to fit</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>y_nonlin <span class="op">=</span> X_nonlin <span class="op">@</span> beta_true <span class="op">+</span> rs.randn(<span class="dv">100</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>lm_mdl_nonlin <span class="op">=</span> sm.OLS(y_nonlin, X) <span class="co"># note use use the original X here</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>lm_results_nonlin <span class="op">=</span> lm_mdl_nonlin.fit()</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>y_pred_nonlin <span class="op">=</span> lm_results_nonlin.predict()</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>y_resid_nonlin <span class="op">=</span> y_nonlin <span class="op">-</span> y_pred_nonlin</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>ax.scatter(y_pred_nonlin, y_resid_nonlin)</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>ax.axhline(<span class="dv">0</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a><span class="co">#ax.set_ylim([-4,4])</span></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'Predicted values'</span>)</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Residuals'</span>)</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Residuals vs. predicted values'</span>)</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a>ax.grid()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="Regression1_files/figure-html/cell-33-output-1.png" width="596" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>You can see the cubic shape of the residuals. Negative predicted values tend to have large negative residuals, while large positive predicted values have large positive residuals. It is also apparent that the residuals systematically shift from being positive to negative as you move from a predicted value of ~-7 to ~7. Both of these phenomena are evidence for heteroscedasticity and a sign that the dependent variable has a nonlinear relationship with the independent variables.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>